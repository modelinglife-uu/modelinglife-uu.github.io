[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modeling Life",
    "section": "",
    "text": "Modeling life\nThis website accompanies the Modeling Life course at Utrecht University. It primarily serves as a central hub for all the practicals (werkcolleges), with the necessarily files, links and other resources for each day.\nEach practical has its own page containing background explanations, code snippets, and questions that are designed for you to learn about the models. Most of these exercises build directly on the lectures, allowing you to explore biological questions—such as how morphogen gradients form, how spatial patterns emerge, or how cells evolve to “stick together”.\nYou can navigate the site using the sidebar or the left. The General Course Info section outlines the general course info (exams, learning goals, etc.). You can also find the Schedule on this website. The individual practicals sections provides detailed instructions for each day. In the second part of the course you will get even more experience doing things yourself by working on a mini-project.\nWe hope you’ll use this website actively. There’s lots to read, simulate, modify, and explore.\nNote: this website is to help you, but is by no means perfect. Please let us know if you see any mistakes, typo’s or other issues. Any constructive feedback on how to make things better and easier for you is always welcome.",
    "crumbs": [
      "Course information",
      "Modeling life"
    ]
  },
  {
    "objectID": "general.html",
    "href": "general.html",
    "title": "General course info",
    "section": "",
    "text": "Contact\nCourse coordinator: Monica Garcia Gomez (m.l.garciagomez@uu.nl). In this website you will find all practical information about the course. You can reach the instructors via email at modelinglife@uu.nl .\nWe are part of the Theoretical Biology group of Utrecht University).",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#contact",
    "href": "general.html#contact",
    "title": "General course info",
    "section": "",
    "text": "Teachers: Kirsten ten Tusscher (K.H.W.J.tenTusscher@uu.nl), Erika Tsingos (e.tsingos@uu.nl),Monica Garcia Gomez (m.l.garciagomez@uu.nl), Bram van Dijk (b.vandijk@uu.nl).",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#course-content",
    "href": "general.html#course-content",
    "title": "General course info",
    "section": "Course content",
    "text": "Course content\nThis course will consist of lectures (HC) covering various modelling approaches to study life at different levels accompanied by computer practicals (WC) where students will get hands-on experience on running computational models of the development or the evolution of animals, plants, and microbial systems.\n\n\n\nRough roadmap of the Modeling life course",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#general-schedule",
    "href": "general.html#general-schedule",
    "title": "General course info",
    "section": "General schedule:",
    "text": "General schedule:\nMonday – Guest lectures (mandatory)\nTuesdays / Thursdays– Lecture and practicals (mandatory)\n\nKey dates:\n\nDecember 18, 2025: Exam\nJan 6, 2026: Miniprojects presentations\nJan 29, 2026: Minisymposium",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#learning-goals",
    "href": "general.html#learning-goals",
    "title": "General course info",
    "section": "Learning Goals",
    "text": "Learning Goals\nThe course aims to provide you with an introductory understanding of computational modeling to study living systems, and their large range of applications.\nAfter this course the student can:\n\nExplain how biological systems can be studied with computational models,\nTranslate a biological system into a working computational model using python,\nUse algorithmic thinking to break down problems into programmable steps,\nIdentify the underlying assumptions and limitations of computational models,\nIdentify the modeling approach & formalism best suited for a research question.",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#communication",
    "href": "general.html#communication",
    "title": "General course info",
    "section": "Communication",
    "text": "Communication\nFeedback and other matters: m.l.garciagomez@uu.nl\nSchedule, slides, practicals: https://modelinglife-uu.github.io/\nCode: https://github.com/moneralee/UU_Modeling-Life-course\nBrightspace: is the channel for all official communications (e.g. final grades).\nModeling Life 2025 in Microsoft Teams. You can sign up as follows:\n\nOpen MS Teams\nIn the menu on the left, select the “Teams” icon\nClick the “Join or create a team” button at the bottom left of the screen (or, depending on the Teams version, at the top right)\nFind the tile that says “Join a team with a code”\nEnter the following code in this field: 6jhv6z5\nSelect “Join team”\nTeams is not used to livestream lectures. However, if you have any questions for the instructors, student assistants, or your fellow students, it’s very helpful to share them on Teams (General channel) so everyone can benefit.",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#grading",
    "href": "general.html#grading",
    "title": "General course info",
    "section": "Grading",
    "text": "Grading\nTo pass this course, a minimum of 5,5 is mandatory. Your grade is calculated by the following components:\nExam: 70%\nMini project: 30%\nYou will find your Final grade on Brigthspace.",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#attendance",
    "href": "general.html#attendance",
    "title": "General course info",
    "section": "Attendance",
    "text": "Attendance\nAttendance is mandatory to all lectures and practicals. Should you be unable to attend, please communicate this by email to: m.l.garciagomez@uu.nl.",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#feedback",
    "href": "general.html#feedback",
    "title": "General course info",
    "section": "Feedback",
    "text": "Feedback\nPlease help us improve the course by providing feedback via Caracal and other means. We want to make this new course as good as possible for you and future students.\nThis website is to help you, but is by no means perfect. Please let us know if you see any mistakes, typo’s or other issues. Any constructive feedback on how to make things better and easier for you is always welcome.",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "WEEK 1\nNov 10, 2025 – Welcome and Intro to Python (HC 13:15-17:00).\nNov 11, 2025 – Pattern formation I: Gradients and segments (HC 10:00-12:45 and WC1 13:15-17:00).\nNov 13, 2025 – Pattern formation II: Turing patterns (HC 10:00-12:45 and WC2 13:15-17:00).",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-2",
    "href": "schedule.html#week-2",
    "title": "Schedule",
    "section": "WEEK 2",
    "text": "WEEK 2\nNov 17, 2025 – Guest lecture: Max Rietkerk, Ecosystem’s spatial patterns (HC 13:15-17:00).\nNov 18, 2025 – Pattern formation III: Clock and Wavefront (HC 10:00-12:45 and WC3 13:15-17:00).\nNov 20, 2025 – Morphogenesis: Cell sorting (HC 10:00-12:45 and WC4 13:15-17:00).",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-3",
    "href": "schedule.html#week-3",
    "title": "Schedule",
    "section": "WEEK 3",
    "text": "WEEK 3\nNov 24, 2025 – Guest lecture: Ina Sonnen, Signal encoding in multicellular systems (HC 13:15-17:00).\nNov 25, 2025 – Cell differentiation: gene regulation in time (HC 10:00-12:45 and WC5 13:15-17:00).\nNov 27, 2025 – Cell differentiation: gene regulation in space (HC 10:00-12:45 and WC6 13:15-17:00).",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-4",
    "href": "schedule.html#week-4",
    "title": "Schedule",
    "section": "WEEK 4",
    "text": "WEEK 4\nDec 1, 2025 – Guest lectures: Vivek Bhardwaj and Kaisa Kajala, What is a cell type? genomic and functional perspectives (HC 13:15-17:00).\nDec 2, 2025 – Environment and Development (HC 10:00-12:45 and WC7 13:15-17:00).\nDec 4, 2025 – Evolving populations I: Sticky cells (HC 10:00-12:45 and WC8 13:15-17:00).",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-5",
    "href": "schedule.html#week-5",
    "title": "Schedule",
    "section": "WEEK 5",
    "text": "WEEK 5\nDec 8, 2025 – Guest lecture: Rutger Hermsen (HC 13:15-17:00).\nDec 9, 2025 - Evolving populations II: Genotype-phenotype map (HC 10:00-12:45 and WC9 13:15-17:00).\nDec 11, 2025 - Evolving populations III: Microbial communities (HC 10:00-12:45 and WC10 13:15-17:00).",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-6-self-study-and-exam",
    "href": "schedule.html#week-6-self-study-and-exam",
    "title": "Schedule",
    "section": "WEEK 6 (self-study and exam)",
    "text": "WEEK 6 (self-study and exam)\nDec 18, 2025 –Exam",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-7-9-mini-projects",
    "href": "schedule.html#week-7-9-mini-projects",
    "title": "Schedule",
    "section": "WEEK 7-9 (mini projects)",
    "text": "WEEK 7-9 (mini projects)\nJan 6, 2026 - Miniprojects presentation and making teams\nFollowing weeks you will have classrooms to work on your miniprojects (13:15-17:00). Also, you should schedule meetings with your supervisor to discuss progress of your miniproject.",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-10-mini-symposium",
    "href": "schedule.html#week-10-mini-symposium",
    "title": "Schedule",
    "section": "WEEK 10 (mini symposium)",
    "text": "WEEK 10 (mini symposium)\nJan 29, 2026 - Miniprojects final presentation",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "intro_to_python.html",
    "href": "intro_to_python.html",
    "title": "0) Introduction to Python",
    "section": "",
    "text": "Installing Spyder and Anaconda\nTo best way to install Spyder is to do so via Anaconda, which is a free and open-source distribution of Python for scientific computing. You can download Anaconda from this page (note: use the download link on the left, and not the one called “miniconda” which does not include Spyder). After downloading, follow the installation instructions.\nOnce done, you now have:",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#installing-spyder-and-anaconda",
    "href": "intro_to_python.html#installing-spyder-and-anaconda",
    "title": "0) Introduction to Python",
    "section": "",
    "text": "Python\nSpyder (our main editor)\nMost scientific packages like numpy, matplotlib, and pandas\nConda and pip (for if you want to install other packages)",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#launching-spyder",
    "href": "intro_to_python.html#launching-spyder",
    "title": "0) Introduction to Python",
    "section": "Launching Spyder",
    "text": "Launching Spyder\nYou can now launch Spyder via the ‘Anaconda Navigator’ application. In MacOS you find this under ‘Applications’ &gt; ‘Anaconda-Navigator’ (it doesn’t always show up in the Spotlight search immediately). In Windows, you can search for ‘Anaconda Navigator’ in the Start Menu. Once the Anaconda Navigator is open, you can launch Spyder by clicking on the ‘Launch’ button under the Spyder icon.",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#disabling-inline-plots",
    "href": "intro_to_python.html#disabling-inline-plots",
    "title": "0) Introduction to Python",
    "section": "Disabling inline plots",
    "text": "Disabling inline plots\nBy default, Spyder shows figures inside the “Plots” pane, but in this course we typically use plots that update dynamically (animations), which don’t work well in that mode. So we need to change that.\nHere’s how:\n\nClick the ‘preferences’ icon in the top panel (a wrench icon 🔧).\nGo to ‘IPython Console’ → Graphics → Backend\nChoose QT, QT5, or QT6 (whichever is available)\nClick Apply and OK\nRestart Spyder may or may not be necessary depending on your operating system.\n\nNow your plots will open in a separate window and can animate properly!",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#testing-your-setup",
    "href": "intro_to_python.html#testing-your-setup",
    "title": "0) Introduction to Python",
    "section": "Testing your setup!",
    "text": "Testing your setup!\nThroughout this course you will either work with scripts you have been handed out, or scripts that you can copy/paste from this website. Let’s test your Spyder setup with the following script:\nimport random, matplotlib.pyplot as plt\n\nplt.ion()\nfig, ax = plt.subplots()\nax.set_ylim(-2, 2)\nax.set_xlim(0, 100)\nax.set_title(\"Random Wiggle Test\")\nline, = ax.plot([], [], color='seagreen', lw=2)\n\ny = [0]\nfor i in range(100):\n    y.append(y[-1] + random.uniform(-0.1, 0.1))  # random wiggle\n    line.set_data(range(len(y)), y)\n    ax.set_xlim(0, len(y))\n    plt.pause(0.03)\n\nax.set_title(\"Animation works! (you can close this window)\")\nplt.ioff()\nplt.show()\nPaste this into a new script in Spyder and hit the ‘play’ icon (or press F5). Does the animation show up in a seperate window and is it animated? Good, you’re ready to go!",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#variables-and-data-types",
    "href": "intro_to_python.html#variables-and-data-types",
    "title": "0) Introduction to Python",
    "section": "Variables and Data Types",
    "text": "Variables and Data Types\nIn Python, variables are used to store data. You can create a variable by assigning a value to it using the = operator. For example:\nx = 10\ny = \"Hello, World!\"\nPython has many built-in data types, including:\n\nIntegers (int): Whole numbers, e.g., 10, -5\nFloating-point numbers (float): Decimal numbers, e.g., 3.14, -0.001\nStrings (str): Text, e.g., \"Hello\", 'Python'\nBooleans (bool): True or False values, e.g., True, False\nLists (list): Ordered collections of items, e.g., [1, 2, 3], ['a', 'b', 'c']\nDictionaries (dict): Key-value pairs, e.g., {'name': 'Alice', 'age': 25}\nAnd many more",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#control-structures",
    "href": "intro_to_python.html#control-structures",
    "title": "0) Introduction to Python",
    "section": "Control Structures",
    "text": "Control Structures\nControl structures allow you to control the flow of your program. Common control structures in Python include:\n\nConditional statements (if, elif, else):\n\nif x &gt; 0:\n    print(\"x is positive\")\nelif x == 0:\n    print(\"x is zero\")\nelse:\n    print(\"x is negative\")\n\nLoops (for, while):\n\nfor i in range(5):\n    print(i)\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#functions",
    "href": "intro_to_python.html#functions",
    "title": "0) Introduction to Python",
    "section": "Functions",
    "text": "Functions\nFunctions are reusable blocks of code that perform a specific task. You can define a function using the def keyword:\ndef greet(name):\n    return f\"Hello, {name}!\"\n    \nprint(greet(\"Alice\"))",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#libraries",
    "href": "intro_to_python.html#libraries",
    "title": "0) Introduction to Python",
    "section": "Libraries",
    "text": "Libraries\nPython has a rich ecosystem of libraries that extend its functionality. Most scientific packages are shipped with Anaconda, so you don’t need to worry about these. These include:\n\nNumPy: For numerical computing\nPandas: For data manipulation and analysis\nMatplotlib: For data visualization\nSciPy: For scientific computing\n\nBut if you still wish to install other packages, you can use pip or conda in the terminal. For example, to install the html5 library using pip, you would run:\npip install html5lib\nFrom the command line. You can also run this bit of code within the Spyder console by adding an ! in front of the command, which tells the console to run it as a shell command:\n!pip install html5lib\nBut you probably won’t have to install a lot of packages during this course. (nearly) All scripts we use only rely on the base packages.",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#studylens-practice",
    "href": "intro_to_python.html#studylens-practice",
    "title": "0) Introduction to Python",
    "section": "StudyLens practice",
    "text": "StudyLens practice\nIf you’ve installed Python and read through the basics of Python above, it’s time to dive into the StudyLens exercises. For this, login to StudyLens and use the username and password that was shared with you on Brightspace.",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "pattern_intro_text.html",
    "href": "pattern_intro_text.html",
    "title": "1  Pattern formation (intro)",
    "section": "",
    "text": "The field of pattern formation studies the mechanisms that underly the formation of spatial patterns in biological systems. Patterns may arise at any biological level of organization, from single cells polarizing to decide in which direction to move, grow or divide, to the formation of body axes, different cell types and shapes that set apart complex multi-cellular organisms from amorphous blobs, to entire ecosystems patterning where e.g. plants do and do not grow. An important concept in pattern formation is so called “symmetry breaking”, which refers to the destruction of an originally homogeneous or non-patterned state, to a non-homogeneous patterned state (see figure below).\n\n\n\n\n\nFocusing on multi-cellular development, pattern formation addresses how within an organism in which all cells (except for germ cells that have undergone meiosis and immune cells applying VJ recombination) share the same genetic material “symmetry is broken” resulting in usage of a different subset of genes and functions by different cells. Symmetry breaking is needed for the creation of body axes, domains with different functions as well as repeating elements.\nA limited range of mechanisms leading to symmetry breaking exist, which have been used time and again by evolution to create patterns in animals, plants, fungi and multicellular algae. Major mechanisms are morphogen gradients, where graded distribution of a signal provides distinct input to cells parallel to the gradient allowing both regionalization and segmentation; Turing patterns, where initial noise combined with diffusion induced destabilization lead to repetitive patterns, and clock-and-wavefront patterning where autonomous oscillations combined with growth and a memorization mechanism provide an alternative means for segmenting a tissue. We will explore these 3 mechanisms in the practicals. In addition, in plants self-organized patterning of auxin transporters underly phyllotaxis (the positioning of new leaf organs at the shoot apex) and leaf venation. Note that this list of symmetry breaking mechanisms is not exhaustive and additional mechanisms such as lateral inhibition and planar cell polarity exist.\n\n\n\n\n\n\nLinks throughout part I-V\n\n\n\nAlthough this course is divided into 5 clearly distinct topics, there is also substantial overlap. We challenge you to see how all sections use similar concepts, and to think about how different types of models may even be combined. To help you along with this integrated view of modelling biology, we will below discuss some of the links with future topics. It may be a good idea to go back to this text later in the course, and reflect if you indeed see all the links.\n\n\nOften, initial signals like gradients and clocks are transient, implying that the patterns they induce require additional mechanisms of memorization. Critical for understanding this memorization process is the concept of multistability, where two or more alternative stable states of the system exist and the initial signals bring the system from the original to a new patterned state. This concept will be further explored in Part III of the course which focuses on Differentiation. Of course, to form a multicellular organism with a functional shape, simply telling a blob of cells where the head or tail needs to be or which cells will become finger bones and which cells will apoptose to carve out the tissue between the fingers is insufficient, and actual shape changing processes need to occur. This we will discuss in Part II.\nParticularly for animal development, simply breaking symmetry and stably memorizing formed patterns is not enough, scaling of the pattern with body plan size and robustness against noise from gene expression, cell division, cell signalling and other processes is essential for fitness. In contrast, in plants developmental plasticity, the potential to adjust developmental patterning to environmental conditions, plays a key role. This latter concept will be discussed in Part IV of the course on Environment.\nThe repeated usage of a limited number of possible symmetry breaking mechanisms also raises interesting evolutionary questions (Part V). Are there indeed only a limited number of options, or did evolution select for specific mechanisms that are more robust or more evolvable? Or are some mechanisms simply easier to find? This way of evolutionary thinking will be further discussed in Part V.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pattern formation (intro)</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html",
    "href": "pattern_practical_1.html",
    "title": "2  Gradients and segments",
    "section": "",
    "text": "2.1 Morphogen Gradients and Patterning\nIn this practical, we a going to look at how an organism can form segments along its body axis . The mathematical model that we will implement and study is an implementation of the so-called French flag conceptual model first proposed by Lewis Wolpert (Wolpert 1969). It assumes the spatially graded expression of a morphogen “M” that influences the expression of some downstream genes A, B and C. Their expression is often visualized by red, white and blue and the arising pattern resembles the French flag, hence the name (see the power of visualization).\nOne of the most well-studied organisms when it comes to body axis segmentation (although its segmentation mechanism is evolutionary derived and a-typical!) is the development of the fruit fly Drosophila melanogaster. Supporting the ideas of Wolpert, it was found that through tethering maternal Bicoid mRNA to one side of the embryo, Bicoid protein can form a gradient extending along the anterior-posterior axis, with so called gap genes as a first tier in the segmentation hierarchy differentially responding to different Bicoid protein levels (Driever and Nüsslein-Volhard 1988). Later it was found that often at least two opposing morphogen gradients drive downstream gene expression and genes typically respond to multiple inputs.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html#mathematical-modeling---integrating-multiple-signals",
    "href": "pattern_practical_1.html#mathematical-modeling---integrating-multiple-signals",
    "title": "2  Gradients and segments",
    "section": "2.2 Mathematical modeling - integrating multiple signals",
    "text": "2.2 Mathematical modeling - integrating multiple signals\nPromotors/enhancers driving gene expression frequently make use of so called OR and AND gates to integrate inputs from different transcription factors. An OR gate can be implemented mathematically with a sum of the effect of the transcription factors, while an AND can be implemented mathematically with a product. Some examples:\n\n\\(\\frac{dX}{dt} = a(\\text{tf1}) + b(\\text{tf2})\\): Gene X is induced by transcription factor 1 and 2 in an OR fashion, either \\(a(\\text{tf1})\\) or \\(b(\\text{tf2})\\) needs to be high to give high transcription of X.\n\\(\\frac{dY}{dt} = a(\\text{tf1})\\cdot b(\\text{tf2})\\): Gene Y is induced by transcription factor 1 and 2 in an AND fashion, both \\(a(\\text{tf1})\\) and \\(b(\\text{tf2})\\), which are being multiplied, need to be high to give high transcription of X.\n\nNote that the shape of \\(a(\\text{tf1})\\) and \\(b(\\text{tf2})\\) (increasing or decreasing function of the transcription factor) determines whether tf1 and tf2 are repressing or activating.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html#python-code",
    "href": "pattern_practical_1.html#python-code",
    "title": "2  Gradients and segments",
    "section": "2.3 Python code",
    "text": "2.3 Python code\n\n\n\n\n\n\nStarting code for this practical\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nLx = 40.0  # Length of the domain in x in microm\nLy = 10.0  # Length of the domain in y in microm\nT = 200  # Total time in seconds\ndx = 0.5  # Grid spacing in x\ndt = 0.1  # Time step\nnx = int(Lx/dx)+2  # Number of grid points in x + padding grid points\nny = int(Ly/dx)+2  # Number of grid points in y + padding grid points\n# Padding grid points to account for boundary conditions\nnt = int(T/dt)  # Number of time steps\nD = 0.4  # Diffusion coefficient in mm^2/s\ndecayM =0.01 # Decay rate in 1/s\n\n\n# Parameters for A, B, C\n... # TODO create parameters for A, B, C as needed in Q5\n\n# Stability criterion\nif D * dt / dx**2 &gt; 0.5:\n    raise ValueError(\"Stability criterion not met\")\n\n# A, B and C are required for later exercises.\nA = np.zeros((nx, ny))\nB = np.zeros((nx, ny))\nC = np.zeros((nx, ny))\n\n# Initial condition\nu = np.zeros((nx, ny))\nu[0, :] = 100\n\n# Reaction-diffusion equation\ndef reaction_diffusion_step(u, D, dt, dx, decay):\n    un = u.copy()\n    u[1:-1, 1:-1] = un[1:-1, 1:-1] +  D *dt / dx**2 * (un[2:, 1:-1] + un[:-2, 1:-1] + \\\n                    un[1:-1, 2:]  + un[1:-1, :-2] - 4 * un[1:-1, 1:-1]) - \\\n                    decay * un[1:-1, 1:-1] * dt\n    ## for loop version to understand the equation\n    # for i in range(1, nx-1):\n    #     for j in range(1, ny-1):\n    #         u[i, j] = (un[i, j] +\n    #                    D * dt / dx**2 * (un[i+1, j] + un[i-1, j] - 2 * un[i, j] +\n    #                    un[i, j+1] + un[i, j-1] - 4 * un[i, j]) - decay * un[i, j] * dt)\n    #boundary conditions\n    u[-1, :] = (u[-2, :]/u[-3, :])*u[-2, :]  if sum(u[-3, :]) != 0 else np.zeros(ny)\n    #to understand this line:\n    #if sum(u[-3, :]) != 0:\n    #    u[-1, :] = (u[-2, :]/u[-3, :])*u[-2, :]#extrapolate from third to last row\n    #else:\n    #    u[-1, :] = np.zeros(ny) #if already zero in third to last row, also zero in last row\n    u[:, 0] = u[:, 1]\n    u[:, -1] = u[:, -2]\n\n    return u\n\ndef reaction_diffusion_gradient(t, u, D, dx, decay, switch_time = None, noise = False):\n    '''\n    Function to create a gradient in the u array that could decay after a certain time.\n    t: current time step\n    u: array to create the gradient in\n    D: diffusion coefficient\n    dx: grid spacing\n    decay: decay rate\n    switch_time: time step after which the gradient decays. If no switch is desired, set to None\n    noise: whether to add noise to the gradient\n    '''\n    # TODO for student: write code for the noise and the switch.\n    added_noise = np.zeros_like(u)  # Initialize noise array\n    if noise:\n        ...  # TODO: add noise generation code here for Q10\n    \n    if switch_time is None or t &lt;= switch_time:\n        # define a exponential decay gradient over the array in the x direction with numpy array operations using the index\n        for i in range(u.shape[0]):\n            u[i, :] = np.maximum(100 * np.exp(-i*dx/np.sqrt(D/decay))+added_noise[i, :], 0)\n        return u\n    if t &gt; switch_time:\n        ...# TODO Q7: implement a gradient that decays over time, otherwise return the original u array\n        return u\n    # In all other cases, return the original u array        \n    return u\n\ndef hill(x, Km, pow):\n    \"\"\"Hill function for the reaction kinetics.\"\"\"\n    return (x**pow) / (Km**pow + x**pow) \n\ndef ihill(y, Km, pow):\n    \"\"\"Inverse Hill function for the reaction kinetics.\"\"\"\n    return( (Km**pow) / (y **pow  + Km**pow))\n\n# TODO for student: write update functions for A, B, C as needed in Q5\n\n\n# initilize figure and axes for plotting\n# TODO for student: Add a new axis for the ABC flag visualization as suggested in Q5\nfig, (ax_M, ax_lines) = plt.subplots(2, figsize=(10, 8), gridspec_kw={'height_ratios': [3, 1]})  # Make the first graph 3 times the height of the second\n\n# Time-stepping simulation loop\nfor n in range(nt):\n    # Update all variables\n    u = reaction_diffusion_step(u, D, dt, dx, decayM)\n    # TODO for student: use precomputed gradient, update A, B, C as needed in Q5\n    \n    if n == 0:  # Initial plot\n        imshow_M_plot = ax_M.imshow(u.T, cmap='viridis', origin='lower', aspect='auto')\n        ax_M.set_title(f\"Time: {n*dt}\")\n        ax_M.set_xlabel('x direction')\n        ax_M.set_ylabel('y direction')\n        ax_M.set_xticks([])\n        ax_M.set_yticks([])\n\n        # Plot the concentration at a specific y index (e.g., y=2)    \n        line_plot = ax_lines.plot([x*dx for x in range(nx)], u[:, 2], label='M', color='green')\n        # TODO: Add lines for A, B, C as needed in Q5\n\n        \n        ax_lines.legend(loc='upper right')\n        ax_lines.set_ylim(0, 100)\n        ax_lines.tick_params(axis='y')\n        ax_lines.set_xlim(0, dx*nx)\n        ax_lines.set_xlabel('x')\n        ax_lines.set_ylabel('Concentration at y=2')\n        ax_lines.tick_params(axis='x')\n\n    if n % 20 == 0:  # Update plot every so many time steps\n        #update the imshow M plot with the new data\n        imshow_M_plot.set_data(u.T)\n        ax_M.set_title(f\"Time: {n*dt}\")\n            \n        # Update the line plots with new data\n        line_plot[0].set_ydata(u[:, 2])\n        # TODO: Update A, B, C line plots as needed in Q5\n\n    plt.pause(0.001)  # Pause to refresh the plot\n\n# And keep the last plot open\n# plt.show()\n\n# Or close the plot window when done\nplt.close(fig)",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html#questions",
    "href": "pattern_practical_1.html#questions",
    "title": "2  Gradients and segments",
    "section": "2.4 Questions",
    "text": "2.4 Questions\n\nExercise 2.1 (Algorithmic thinking) Have a look at the reaction-diffusion equation for the morphogen and the implementation of it in the file morphogengradient_to_segments.py in the reaction_diffusion_step function, as well as at the initialization of the array u. How is this different from the gradient formation modeling we discussed during the lecture? What is done at the terminal boundary in the length direction and why does this make sense? (hint: outcommented we provided code doing essentially the same but not using numpy arrays and hence written in a less compact matter to help you understand what is happening)\n\n\nAnswer An important difference relative to the lecture is that instead of implementing a production term at the x=0 position, we instead fixed the concentration of u at x=0 at 100 and have nowhere a production term. This implies that we assume that production is very high and different D and d values have negligible effects on the level at x=0. The advantage of this approach is that we can easily study what different D and d does, as it only affects the shape of the gradient but not the maximum.   Instead of using if/else to check for boundary conditions, we only compute the Laplacian in the inner, non-boundary points. For the x=0 boundary we already set a constant value. For y=0 and y=n−1 we copy the values from y=1 and y=n−2 as the patterning is identical in that direction. For x=n−1 we use a special approach: if n−3=0 we assume it is zero; otherwise, we calculate the ratio of x at n−2 and n−3 to approximate the decay and multiply this with the value at x=n−2 to estimate x at n−1. Simply copying x at n−2 would add matter to the system and prevent the gradient from reaching a steady state.\n\n\nExercise 2.2 (Important concept) Play with the morphogen diffusion rate and the morphogen decay rate and describe what happens. What happens in terms of dynamics and steady state if you change both, but the ratio stays the same? Hint: it might help to draw a horizontal line at a certain height to ease comparison.\n\n\nDiffusion length concept =√(D/d), it is the ratio of these factors that determines the length scale lambda over which the morphogen concentration decays an e-fold. So with the same ratio, you get the same steady state gradient. However, it will still approach the steady state faster.\n\n\nExercise 2.3 (Biology & mathematical thinking) Next, we are going to introduce the genes A, B and C in the model. We want these genes to be expressed dependent on M, and in the head, trunk and tail respectively, so A on the left, B in the middle and C on the right. Think of what conditions in terms of M should lead to expression of A/B/C. What Hill functions (normal/inverse/any combination) corresponds to those conditions? Write down (pen and paper, not in code) full equations for the genes, do we need any other terms than just Hill functions, would we need specific parameter conditions?\n\n\nAnswer  \\(dA/dt\\) = normal hill (M) – decay term \\(dB/dt\\) = normal hill (M) * inverse hill(M) – decay term, \\(dC/dt\\) = inverse hill (M) -decay term. Furthermore, \\(kA=kB1&lt;kB2&lt;kC\\) to ensure the switches happen at the right places.\n\n\nExercise 2.4 (Biology & algorithmic/mathematical thinking) From hereon we assume that the morphogen gradient reaches steady state very quickly, and no longer use the numerical implementation of the diffusion equation and instead work with a superimposed morphogen profile defined by reaction_diffusion_gradient to save time. (Hint: to not call the function any more use # in front of where it is called) Change the simulation loop such that it computes the morphogen gradient once. What type of function is the superimposed morphogen profile and how does this relate to question 2.\n\n\nAnswer The imposed morphogen profile is an exponential gradient, the – indicates it is a declining function, the \\(i\\cdot dx\\) is to convert position in the spatial grid to actual distance, and the /sqrt(D/d) ensures that if \\(i\\cdot dx=lambda=\\sqrt(D/d)\\) we have \\(e^{-1}=1/e\\) and hence an e-fold decline as is the case for a steady state diffusion gradient.\n\n\nExercise 2.5 (Biology & algorithmic/mathematical thinking)  \n\nNow create functions to update A, B and C according to your equations from the previous question. You may use the predefined hill and ihill functions that are provided in the code. For simplicity, you may keep most of the parameters the same across genes, but some have to be different to ensure the right location of the genes (see your reasoning to the previous question). (Hint: using array properties to update A/B/C, like in the reaction_diffusion_step function, makes your code run a lot faster than using for-loops)\nAlso make sure that the levels of A, B and C are updated in the simulation loop. (Hint: using array properties to update A/B/C, like in the reaction_diffusion_step function, makes your code run a lot faster than using for-loops)\nNext, ensure A, B and C are visualized in the bottom plot axis (copy and adapt the code for the visualization of M). You can add an extra third axis to the plot to visualize the (French) flag pattern, by getting which of the three genes is maximal at each location with np.argmax(np.array([A, B, C]), axis=0)and turning that into an array of RGB colors of choice.\nDo you get the expected “French flag” pattern? If not, think of why not and improve your equations from previous question, parameters or your code.\n\n\n\nAnswer Look at code 02_answer_morphogengradient_superimposed_quickABC.py. You will find that gene C needs to be induced by low and repressed by high M. And as explained above: kA=kB1&lt;kB2&lt;kC.\n\n\nExercise 2.6 (Biology) At some point in development, the morphogen gradient will disappear, for example in the case of the Drosophila Bicoid gradient because the maternal mRNA is degraded. Predict what will happen to the expression of A,B and C (and hence the French flag pattern) if the morphogen gradient disappears over time and assume A, B, and C are regulated by our equations (first try think about this without actually simulating this).\n\n\nAnswer As M decays A and B will no longer be activated and C no longer repressed, hence the takeover by C. The French flag pattern disappears.\n\n\nExercise 2.7 (Algorithmic/mathematical thinking) Now write the code in reaction_diffusion_gradient that updates the morphogen concentration, such that after the time point A, B and C have gotten close to equilibrium, the morphogen gradient gradually disappears. Adapt the simulation loop where necessary and run your code: was your prediction on A/B/C from the previous question correct? Why/why not? Hint: you might need to increase the duration of your simulation, especially if it takes a long time for A, B and C to reach equilibrium (or you can change the parameters to speed things up by using same production/degradation rate ratio yet higher absolute values of the individual parameters).\n\n\nAnswer It should always result in C taking over. See answer to 6.\n\n\nExercise 2.8 (Biological & mathematical thinking) A, B and C cannot remain in a stable pattern if they are only influenced by M. How can we stabilize the pattern in absence of M? Test your ideas by creating new update functions for A, B and C and let these new ‘rules’ kick in at the same time when M starts to decline. Again, you may find Hill functions useful and perhaps also the before/after switch time structure used in reaction_diffusion_gradient for M. (Hint: think about how the genes should affect each other, and assume that in this new phase, when genes have already been initialized in absence of repression the genes will be expressed). Can you maintain the expression domains of A, B and C and does their shape change?\n\n\nAnswer See 03_answer_morphogengradient_superimposed_quickABC_phase2.py As M decays A and B will no longer be activated and C no longer repressed, hence the takeover by C. They will find mutual repression with only your neighbor gene is not enough, you need it with both 2 other genes. So A needs to be repressed by B and C, B by A and C, and C by A and B. We can maintain A at the beginning, B in the middle and C at the end, but the shape of the expression domains becomes less gradual and more sharply defined.\n\n\nExercise 2.9 (Biology & algorithmic/mathematical thinking) A sudden switch from phase 1 (stable M gradient) to phase 2 (decaying M gradient) resulting in the genes following different differential equations is biologically implausible. In reality, genes have complex promotors and enhancers integrating different inputs that arise in different developmental stages. Try to come up with one integrated expression for each gene, incorporating simultaneous input from the morphogen gradient and the other genes. Create new update functions for A/B/C and test your ideas. Can you get a stable pattern before and after the decline in M? A couple of things you could consider: a. Think of how positively and negatively M regulated expression behave once the gradient starts declining: what is the best way to combine (AND/OR) that with the regulation by the genes? a. Consider splitting up the M regulated expression of middle gene B into a positive and negatively regulated morphogen part before integrating the other genes inputs. a. It is also possible to give certain genes a bit of a constant boost to prevent their takeover by other genes due to timing issues\n\n\nAnswer This is not at all easy (or as the professor would say “highly non trivial”). It requires that for genes (or part of their input) being positively dependent on M that there is an OR gate (sum) (so they stay on once M is absent as long as the mutual repressors are absent) while for genes (or part of their input) being negatively dependent on M that there is an AND gate (so they do not expand once M is absent but require both M absent and mutual repressors absent). See the answer code: 05_&lt;longname&gt;_quickABC_phase2_combined.py\n\n\nExercise 2.10 (Biology & algorithmic/mathematical thinking) Write code to get noise in the morphogen gradient, both in its steady state and decaying phase. During which phase do the expression domains of A, B and C suffer more from the noise. Explain why? How could we make the system more robust in a manner that is also likely occurring in nature?\n\n\nAnswer See the answer code: 05_&lt;longname&gt;_quickABC_phase2_combined_noise.py The noise has more effect in the initial phase when mostly morphogen is driving the expression domains, as gene expression becomes established and morphogen levels decay the mutual repression ensures stability against the noise Diffusion of A, B and C, in the lateral direction this evens out differences (not implemented right now)",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html#extra-questions",
    "href": "pattern_practical_1.html#extra-questions",
    "title": "2  Gradients and segments",
    "section": "2.5 Extra questions",
    "text": "2.5 Extra questions\nIf you’re done early or a master student, you can make these extra questions. These questions need not be made in the order they are provided, you can choose what you would like to investigate.\n\nExercise 2.11 (Biological thinking) Play with the size of the domain to see what happens to the gene expression domains. What would this mean for an organism?\n\n\nAnswer You may find that if the domain expands the C domain expands and that if the domain shrinks the C domain disappears. This would mean having either a very large or loosing your “butt” part.\n\n\nExercise 2.12 (Biological & algorithmic thinking) Various mechanisms have been proposed to ensure that the domains of the morphogen controlled genes scale with the size of the domain. One proposed mechanism suggests the existence of an also diffusible “expander” molecule which expression is repressed by the morphogen but which itself either represses the degradation of the morphogen or enhances its diffusion (see e.g. https://www.pnas.org/doi/full/10.1073/pnas.0912734107 and https://onlinelibrary.wiley.com/doi/10.1111/dgd.12337).\nFor the expander we can write:\n\\[\n∂E/∂t=p K^h/(K^h+M^h )-dE+D_E ∆E\n\\]\nAssume E reduces degradation of M (easier to implement than enhancement of diffusion) and study the effect of scaling. (Hint, vary the size of the domain in the length direction but plot domains as a function of relative instead of absolute domain size to compare domain sizes).\n\n\nAnswer We have not tried this out. Let us know what you find!\n\n\nExercise 2.13 (Algorithmic thinking) In b, we can also implement other boundary conditions, especially for the right boundary. What happens to the profile if we make a no-flux boundary by copying the value at n-2 to n-1? And what if we set it to a sink (force concentration to zero)?\n\n\nAnswer Copying n-2 to n-1 adds material, so now concentration at end keeps rising and gradient fails to reach equilibrium. Putting it to sink not tried, may speed up stabilization of gradient\n\n\nExercise 2.14 (Algorithmic thinking) From question 4 onwards, how do things change if we do not assume a quasi-steady-state for the morphogen gradient (i.e. keep the morphogen dynamics instead of replacing it by the superimposed exponential)? How would you implement a disappearing gradient and how does it shape change the outcomes of the flag?\n\n\nAnswer Instead of only initializing position x=0 at 100, you now need to include it in the update of the diffusion and use a decaying exponential behind the value of 100 beyond a certain point. Not tried myself, likely somewhat different shape change than simply decreasing an exponential along the entire axis proportionally, may affect sizes and dynamics of domains.\n\n\nExercise 2.15 (Algorithmic thinking) Implement your solution to make the system more robust for noise from question 10. Did it work?\n\n\nAnswer This requires also having A, B and C diffuse and is expected to smooth out differences.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html#relevant-literature-further-reading",
    "href": "pattern_practical_1.html#relevant-literature-further-reading",
    "title": "2  Gradients and segments",
    "section": "2.6 Relevant literature / further reading",
    "text": "2.6 Relevant literature / further reading\nDagmar Iber group, scaling from non-steady state dynamics and uniform growth: Fried and Iber (2014) https://www.nature.com/articles/ncomms6077\nCheung et al. (2011) https://pmc.ncbi.nlm.nih.gov/articles/PMC3109599/\nHe et al. (2015) https://www.nature.com/articles/ncomms7679\nBicoid gradient: larger eggs get more Bicoid mRNA so higher production rate, when it scales with volume this helps scale the gradient: Inomata (2017) https://onlinelibrary.wiley.com/doi/10.1111/dgd.12337\nExpansion repression model. Players: Chordin, Bmp, Sizzled. Chordin represses Bmp which induces Sizzled (which has low decay rate), yet Sizzled reduces Chordin decay. Chordin is morphogen. Sizzled is expander (by reducing decay of morphogen) Ben-Zvi and Barkai (2010) https://www.pnas.org/doi/abs/10.1073/pnas.0912734107\n\n\n\n\nBen-Zvi, Danny, and Naama Barkai. 2010. “Scaling of Morphogen Gradients by an Expansion-Repression Integral Feedback Control.” Proceedings of the National Academy of Sciences 107 (15): 6924–29.\n\n\nCheung, David, Cecelia Miles, Martin Kreitman, and Jun Ma. 2011. “Scaling of the Bicoid Morphogen Gradient by a Volume-Dependent Production Rate.” Development 138 (13): 2741–49.\n\n\nDriever, Wolfgang, and Christiane Nüsslein-Volhard. 1988. “The Bicoid Protein Determines Position in the Drosophila Embryo in a Concentration-Dependent Manner.” Cell 54 (1): 95–104.\n\n\nFried, Patrick, and Dagmar Iber. 2014. “Dynamic Scaling of Morphogen Gradients on Growing Domains.” Nature Communications 5 (1): 5077.\n\n\nHe, Feng, Chuanxian Wei, Honggang Wu, David Cheung, Renjie Jiao, and Jun Ma. 2015. “Fundamental Origins and Limits for Scaling a Maternal Morphogen Gradient.” Nature Communications 6 (1): 6679.\n\n\nInomata, Hidehiko. 2017. “Scaling of Pattern Formations and Morphogen Gradients.” Development, Growth & Differentiation 59 (1): 41–51.\n\n\nWolpert, Lewis. 1969. “Positional Information and the Spatial Pattern of Cellular Differentiation.” Journal of Theoretical Biology 25 (1): 1–47.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html",
    "href": "pattern_practical_2.html",
    "title": "3  Turing digit patterns",
    "section": "",
    "text": "A foundational work in Theoretical Biology is Alan Turing’s paper “The Chemical Basis of Morphogenesis” [1]. In this work, Turing mathematically demonstrates a plausible mechanism whereby a reaction-diffusion system can lead to the formation of a spatially-patterned solution in steady state. Mathematically and conceptually, Turing’s key innovation was considering diffusion-driven instability. That is: An unpatterned steady state solution becomes unstable in the presence of diffusion, leading to the formation of a spatially patterned state.\nTuring-type patterns emerge when a system exhibits short-ranged activation and long-ranged inhibition. The most well-known realization of a Turing reaction-diffusion system is the combination of a slow-diffusing activator and fast-diffusing inhibitor. Many more examples exist which can be reduced to a mathematically equivalent form. In this practical, we will explore a 3-component Turing system developed by Raspopovic and coauthors to explain the patterning of fingers on the paws of mice [2].\nIn this practical you will work with a total of 6 different python codes. Please note that each code simply differs by an extension or modification from the previous code, so that in the end you will have a long script that performs all the steps in one go.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#section",
    "href": "pattern_practical_2.html#section",
    "title": "3  Turing digit patterns",
    "section": "",
    "text": "Turing, A. M. “The Chemical Basis of Morphogenesis.” Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences 237.641 (1952): 37–72.\n\nRaspopovic, Jelena, et al. “Digit patterning is controlled by a Bmp-Sox9-Wnt Turing network modulated by morphogen gradients.” Science 345.6196 (2014): 566-570.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#equivalence-of-turing-models",
    "href": "pattern_practical_2.html#equivalence-of-turing-models",
    "title": "3  Turing digit patterns",
    "section": "3.1 Equivalence of Turing models",
    "text": "3.1 Equivalence of Turing models\nThe model proposed by Raspopovic et al. for generating Turing patterns underlying limb bone modeling consists of 3 instead of 2 interacting variables. We will refer to this model as “BSW” model, as it represents interactions between Bmp, Sox9, and Wnt.\n\nExercise 3.1 (Conceptual thinking) Study the schemes below. The arrows represent activating or inhibiting interactions, while the wavy lines represent diffusion of components. Simplify the BSW model scheme to a 2-component system and explain why the relevant Turing constraints apply to the BSW model. In your answer, think about the logic of the interactions in the system conceptually; we do not expect you to answer by writing down equations.\n\n\n\n\n\n\n\n\nFigure 3.1: Schemes and numerical solutions of two prototypical 2-component Turing models\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Scheme of the BSW model\n\n\n\n\n\n\n\nAnswer “fuse” Sox9 and Wnt nodes to get Substrate-Depletion model a) Sox9 inhibits Wnt which inhibits Sox9 -&gt; summarize as self-activation b) Wnt self-inhibition arrow -&gt; part of self-activation in reduced scheme c) Wnt slow diffusion -&gt; Sox9 slow diffusion Turing instability requires short-range activation and long-range inhibition.  In the substrate-depletion model, the growth of the slow-diffusing activator A is constrained by the local loss of fast-diffusing substrate S. Around an A-enriched domain a halo of S will form, which will induce nearby A domains to grow, generating the alternating spatial pattern. In the BSW model, Sox9 indirectly locally self-activates by inhibiting its slow-diffusing inhibitor Wnt, but the growth of Sox9 domains is constrained by depletion of the fast-diffusing Bmp (playing the role of substrate in this model). Around Sox9-enriched domains a halo of Bmp will form and induce the generation of more Sox9 domains in the surroundings.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#patterning-in-a-fixed-domain-size",
    "href": "pattern_practical_2.html#patterning-in-a-fixed-domain-size",
    "title": "3  Turing digit patterns",
    "section": "3.2 Patterning in a fixed domain size",
    "text": "3.2 Patterning in a fixed domain size\nThe code in the script 01__digits_squarehomogeneoustissue.py simulates the model in a 2D square tissue of constant size. This setup can be considered as a small region of a large Petri dish in which mesenchymal cells have been plated with the correct chemicals to undergo bone formation (Supplementary Figure S3 in the paper).\n\nExercise 3.2 (Biology) Play with the reaction parameters and diffusion constants to study their effects on the wavelength of the patterns. Note down the following:\n\nWhich parameter changes lead to a pattern with thicker/thinner stripes?\nWhich parameter changes lead to a pattern with spots?\n\nNote: To make things easier on yourself, only change one parameter at a time. Use a narrow range around the reference value to stay in the numerically stable regime of the forward Euler solver. If the solution becomes blank or wildly oscillating, you’re likely in the numerically unstable regime and you should try a smaller change of values. When increasing diffusion constants, you may need to reduce the time stepping dt.\n(only Master students) Based on the BSW interaction scheme, give a short interpretation of why the following parameter alterations change the spatial pattern:\n\nParameters of Sox9-BMP interaction k2 and k4\nParameters of Sox9-Wnt interaction k3 and k7\nFold change difference in BMP diffusion compared to Wnt diffusion\n\nHint: Use the maximum and minimum values of the variables to guide your thinking.\n\n\nFor simplicity I will only show Sox9 patterns:\n\n\n\n\nIn general, all three components follow similar trends in terms of concentrations as any impact on one component will affect the others via feedbacks. Increasing or decreasing the parameters k2 and k4 has the same effect on the pattern.  With increased Sox9 activation (high k2) a lower amount of BMP is sufficient to get Sox9 to increase which in turn inhibits both Wnt and Bmp. As a result, the “peaks” are reached sooner in space resulting in shorter lower amplitude wavelengths. Similarly, when Sox9 inhibition of BMP is high (high k4) then BMP peaks are lower, which means there is less Sox9 activation leading to smaller amplitude peaks and hence shorter wavelength. Conversely, if Sox9 activation by BMP is weak (low k2), then it takes more BMP to build up to a peak, resulting in longer higher amplitude wavelengths. When Sox9 inhibition of BMP is weak (low k4), then it takes larger Sox9 domains to deplete BMP, resulting in longer higher amplitude wavelengths. If Sox9 activation (k2) or BMP inhibition (k4) are extremely strong, Sox9 locally depletes BMP faster than it can diffuse, which in turn leads to a reduction in Sox9; this could explain local oscillations and travelling waves, though this result should be verified with a better numerical scheme. Changing BMP self-inhibition (k5) has the opposite effect as Sox9 activation (k2) and BMP inhibition (k4). BMP self-inhibition restricts the size of BMP domains. Stronger self-inhibition (high k5) means small pockets of BMP are destabilized and Sox9 regions grow until the point that they completely deplete the local BMP, resulting in larger higher amplitude peaks. Vice-versa, reducing BMP self-inhibition (low k5) allows BMP peaks to more easily form, leading to shorter pattern wavelengths. Unlike the other two parameters, this reduction only happens up to a point, as the other feedback interactions take over.\n\n\n\nIncreasing or decreasing the parameters k3 and k7 has the same effect on the pattern. When Wnt inhibition of Sox9 is increased (high k3) or Sox9 inhibition of Wnt is increased (high k7) the cross-inhibition of both variables is stronger, meaning they will tend to create larger exclusion zones which translates as longer higher amplitude wavelengths. When Wnt inhibition of Sox9 is decreased (low k3) or Sox9 inhibition of Wnt is decreased (low k7) smaller regions can coexist, reducing the wavelength and the amplitude. Even lower cross-inhibition means the regions overlap, and small time delays could lead to oscillations, although this should be verified with a numerically better scheme. Wnt self-inhibition (k9) limits the growth of Wnt domains. When self-inhibition is strong (high k9) Wnt domains shrink and therefore the wavelength is shorter with lower amplitude. Even stronger self-inhibition leads to overlapping domains and similar oscillations as observed for low k3 and low k7. Vice-versa, a decrease of Wnt self-inhibition (low k9) allows larger patches of Wnt to form, which increases the wavelength and the amplitude.\n\n\n\nThe equation for Sox9 includes a “saturation term” in the form of a negative cubic power of Sox9, which means that effectively there is Sox9 self-inhibition in the model. The parameter delta multiplies a quadratic Sox9-dependent term in the equation for Sox9, which by default is set to 0. A positive delta effectively means that Sox9 has a self-activation term that counteracts the self-inhibition by the cubic saturation term, whereas a negative delta means the Sox9 self-inhibition is stronger. Introducing a stronger Sox9 self-activation (delta &gt; 0) increases the amplitude of Sox9 domains. Due to this local strong increase in Sox9, there is also a local strong depletion of BMP which prevents stripes from elongating, resulting in a spot pattern. Introducing a stronger Sox9 self-inhibition (delta &lt; 0) reduces the amplitude of Sox9 domains. There is thus less BMP depletion which enables stripes to elongate and fuse more easily. Wnt and Sox9 are cross-inhibitory, and thus their domains will compete. Lower Wnt diffusion means Wnt domains form locally, leading to a reduced wavelength. Vice-versa a higher Wnt diffusion means that Wnt spreads away from Sox9 domains enabling them to expand ultimately resulting in longer wavelengths. Note that since BMP diffusion scales with Wnt diffusion, both parameters were changed at the same time. The ratio of diffusion speeds defines the size of the Turing space. As the diffusion of BMP is set equal to that of Wnt, the homogenous solution becomes stable. Reducing the diffusion of BMP further leads to travelling waves or oscillations. This is because a slow (or stationary) BMP is in a local negative feedback loop with Sox9, which results in oscillatory behavior. The establishment of Wnt domains around the oscillating Sox9 regions is also oscillatory. Increasing BMP diffusion allows to more effectively “feed” Sox9 domains, which leads to a subtle increase in the wavelength and amplitude.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#the-homogenous-steady-state-in-turing-models",
    "href": "pattern_practical_2.html#the-homogenous-steady-state-in-turing-models",
    "title": "3  Turing digit patterns",
    "section": "3.3 The homogenous steady state in Turing models",
    "text": "3.3 The homogenous steady state in Turing models\nIn a system exhibiting Turing instability, the spatially unpatterned steady state is unstable in the presence of diffusion.\n\nExercise 3.3 (Conceptual thinking) Modify the initial condition in the script 01__digits_squarehomogeneoustissue.py to be exactly in this state (Sox9, BMP, Wnt should be all zero everywhere) and run the simulation. Explain the simulation result.\n\n\nAnswer The homogenous steady state continues to exist, and if starting in that state the system will not evolve to a different state. A small perturbation is needed to nudge the system out of the steady state.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#patterning-in-a-growing-domain-size",
    "href": "pattern_practical_2.html#patterning-in-a-growing-domain-size",
    "title": "3  Turing digit patterns",
    "section": "3.4 Patterning in a growing domain size",
    "text": "3.4 Patterning in a growing domain size\nBesides parameter values, the pattern that emerges from a Turing system is also strongly influenced by initial conditions and the size and shape of the domain where reactions and diffusion happen.\nIn the script 02__digits_growingsquare_PD.py we added tissue growth in the proximo-distal (body to limb) direction. Growth is controlled by a growth rate vi.\n\nExercise 3.4 (Biology)  \n\nStudy the code to understand what it does.\n\nIncrease the horizontal growth rate vi in small increments (try: 0.01, 0.05, 0.1, 0.2, 0.5) and study what happens to the pattern. Explain why you think this happens.\n\nNote: Increase the totaltime parameter to 5000 when using growth rate of 0.01.\n\n\nAnswer At low growth rates, the pattern forms stripes parallel to the growth direction. As the growth speed increases, the alignment shifts to stripes perpendicular to the growth direction. When growth is slow, the “new domain” added by growth is too small to fit a Turing wavelength. As a result, the existing pattern gets extended into the new parts. When growth is fast, the “new domain” added by growth is sufficiently large to fit a Turing wavelength, which becomes the selected mode. Additionally for faster growth, the diffusion/growth ratio is now lower making it harder to simple extend the existing pattern from diffusion of the old cells that previously made up the boundary.\n\n\n\n\nSimulations from this practical with domain growth\n\n\n\n\n\nExperimental work using a light-gated chemical reaction that forms Turing patterns when illuminated. The illuminated area was shifted over time to produce a growing domain, work by Míguez et at., 2006",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#patterning-along-two-growth-axes",
    "href": "pattern_practical_2.html#patterning-along-two-growth-axes",
    "title": "3  Turing digit patterns",
    "section": "3.5 Patterning along two growth axes",
    "text": "3.5 Patterning along two growth axes\nIn reality, the limb bud grows in both the proximo-distal and anterior-posterior direction. In the script 03__digits_growingsquare.py this is implemented with growth rates vi for proximo-distal growth and vj for anterior-posterior growth.\n\nExercise 3.5 (Biology) Play with the relative size of these growth rates and study what happens. Compare the order of appearance of stripes to the experimental data in the paper (see Figure 3F in the Raspopovic paper).\n\n\nAnswer As in the previous question, the rate of growth influences whether stripes grow parallel to or perpendicular to the growth boundary. Also as before, slow horizontal growth promotes horizontal stripes (vi&lt;=0.1), and when this becomes faster stripes reorient to become vertical. However now, if we add vertical growth and it is not small enough relative to the horizontal growth, above and below the horizontal stripes we get vertical stripes and these domains are larger the faster vertical growth is relative to horizontal growth. So to get ok-ish patterns we need horizontal growth not too fast, and vertical growth to be slower. These results are consistent with horizontal growth of the “paddle” being faster than vertical growth. Also, we see that middle horizontal stripes appear earlier than the outer ones (in those situation where not all is vertical), similar to what we see in the paper in Fig 3F. Still interestingly, if you put vj=0.2 and vi=0.1 you promote initially vertical and later on horizontal stripes, this is an effect of in the vertical direction hitting the boundaries of the domain first and then still slowly continuing growth in the horizontal dimension at a not too high rate that was supportive of extending horizontal stripes.\n\n\n\n\nSimulations from this practical with domain growth",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#making-a-virtual-tissue-grow",
    "href": "pattern_practical_2.html#making-a-virtual-tissue-grow",
    "title": "3  Turing digit patterns",
    "section": "3.6 Making a virtual tissue grow",
    "text": "3.6 Making a virtual tissue grow\nThink about how tissue growth is implemented in the script 03__digits_growingsquare.py.\n\nExercise 3.6 (Algorithmic and conceptual thinking) What is the default value of the newly added tissue when it grows? Is the concentration elsewhere in the tissue changed?\nDo you think this approach is reasonable to model biological growth of a tissue? Explain why/why not.\nHint: Think about it from the perspective of a growing cell containing a number of molecules “X”.\nIf the cell increases in volume and does not produce/degrade X, what happens to the concentration of X?\nWhat would be the concentration of X in the two daughter cells if the cell divides?\nHow would the situation change if X is constantly produced and degraded?\nIn the paper, Raspopovic et al. create a “tissue growth map” (see Figure 3A), which they use to map concentration values from one timepoint to the next using interpolation-based transformations.\nInspired by this approach, the script 04__digits_growingsquare.py implements a different growth function that uses bilinear interpolation to expand the tissue. Play around with the growth rate parameters and compare the results to what you found earlier. Do the patterns differ? If yes, how do they differ?\nHint: Try the supplementary script supp_g__bilinear_interpolation.py to understand how interpolation-based growth works.\nFor master students: yet another way of implementing tissue growth would be to take cell division and inheritance of maternal state by the two daughter cells literal and implement this by new boundary cells copying the state of their direct neighbors. Implement this alternative growth and see how this affects your results.\n\n\nAnswer The default value of newly added tissue is 0, because the entire matrix is initialized with 0s, and only a small rectangle corresponding to the initial tissue domain is filled with random values and then updated based on the equations. When growth happens, the tissue domain extends to cover new parts of the matrix that were not included in the initialization. Extending the edges into 0-valued territory would be as if new, empty cells appear at the edges of the tissue, which is not a reasonable approach to model limb bud growth.If a cell increases in volume without producing/degrading molecules “X”, then the concentration of X will decrease as the cell increases in volume. Daughter cells would have on average half the concentration of X as the mother cell. If “X” is constantly produced and degraded (and the cell is a well-mixed environment) then the concentration of X could be in quasi-steady state during growth. A growing cell would maintain the same concentration of X over time. When the cell divides, both daughter cells would inherit the same concentration of X as the mother cell had. Assuming the latter scenario is more likely, then growth of a tissue should work by “stretching” the values of the growing parts. (Some tissues actually do grow only at the edges, but even there we would expect the newly added “territory” to be made from previous cells and therefore still inherit their concentrations.)\n\n\nThe different growth appears to affect pattern formation, e.g for vj=0.01 the pattern is still predominantly horizontal for 0.15, which was less so before. In constrast for vj=0.05 and vi=0.15 pattern is mostly vertical whereas in other growth regime it was still partly horizontal. Seems that transition from horizontal to vertical stripes is thus modulated. So for low vj it is easier to get horizontal stripes.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#spatial-modulation-of-parameters-k4-and-k7",
    "href": "pattern_practical_2.html#spatial-modulation-of-parameters-k4-and-k7",
    "title": "3  Turing digit patterns",
    "section": "3.7 Spatial modulation of parameters k4 and k7",
    "text": "3.7 Spatial modulation of parameters k4 and k7\nLet us now ignore growth and its role on orienting stripes for a while and explore once again the influence of parameters on the patterns. In reality, digits are patterned further apart at the distal end than at the proximal end, where they need to converge on a hand and wrist. This implies that the wavelength of the Turing pattern should not be constant. In a previous question you probably found that the k7 and k4 parameters impact the wavelength of the Turing pattern. The authors speculate that the FGF and Hox gene gradients observed in the limb bud exert an effect on the Sox9-BMP-Wnt patterning module through these parameters.\n\nExercise 3.7 (Biology) The script 05__digits_squarek4k7gradient.py allows you to implement exponential gradients of k4 and k7 across the tissue. You can set the minimum and maximum values, axis (x=horizontal, y=vertical) and direction (0=decreasing, 1=increasing). Look in the supplement of the Raspopovic paper, page 48 Fig 2C, left for the estimated change of k4 and k7 across the tissue and try to reproduce (not exactly but similarly) Figure 2C, right side, by playing with the parameters affecting the k4 and k7 gradients. Describe along which axis, in which direction and to what extent k4 and k7 are changing. Estimate the change in Turing pattern wavelength this results in.\n\n\nAnswer See the script 05__digits_squarek4k7gradient_answer.py for a solution. The output may look sometimes like this:",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#hoxd13-and-fgf",
    "href": "pattern_practical_2.html#hoxd13-and-fgf",
    "title": "3  Turing digit patterns",
    "section": "3.8 Hoxd13 and FGF",
    "text": "3.8 Hoxd13 and FGF\nAs mentioned above, the biological factors thought to underly the variation in the k4 and k7 parameters are Hoxd13 and FGF. Hoxd13 is highly expressed in the distal domain of the paddle and not expressed elsewhere, and this expression domain is growing as the paddle grows. FGF is expressed from the distal edge of the paddle and spreads through diffusion. For an illustration see Fig S8A of the supplement of the Raspopovic paper (page 10). Experimental data furthermore suggest that Hoxd13 and FGF together affect k4 and k7, which the authors decided to model using the following equations:\n\\[\nk4^* = k4 - k_{HF_{bmp}} * fgf(i,j) * hox(i,j)\n\\]\n\\[\nk7^* = k7 + k_{HF_{wnt}} * fgf(i,j) * hox(i,j)\n\\]\n\nExercise 3.8 (Algorithmic and biological thinking) The script 06__digits_squarehoxdfgf.py implements how k4 and k7 are a function of local FGF and Hox values, but does not yet include Hox and FGF spatial patterns and how these develop over time. Adjust the code to incorporate the observed Hoxd13 and FGF patterns and study how the digit patterns compare to what you found under question 7. Note that arrays for hox and fgf are included, and decay and diffusion values for fgf are provided already in the code. Also note that hox and fgf each have a maximum value of 1.\n\n\nAnswer See the script 06_digits_squarehoxdfgf_answer.py for a solution. The output may look sometimes like this:\n\n\nThe Hox domain starts as a narrow vertical band and expands backward. The FGF gradient develops from the rightward boundary. Since a certain amount of Hox and FGF is needed to result in k4 and k7 values permissive of Turing patterns the stripes develop from right to left and because of the wavelength changes induced by different Hox and FGF values become closer together towards the left. Note that here a nice example run is shown but patterns are variable.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#shape-makes-the-tissue",
    "href": "pattern_practical_2.html#shape-makes-the-tissue",
    "title": "3  Turing digit patterns",
    "section": "3.9 Shape makes the tissue",
    "text": "3.9 Shape makes the tissue\nHaving studied the effects of growth and genetic factors modulating the Turing pattern now it is time for the final step. So far, we’ve approximated the limb bud very crudely with a rectangle. As you probably noticed in your parameter exploration, the tissue boundary can bias stripe orientation. Therefore it is interesting to study the interplay between the shape of the “paddle” or limb bud in which the digits develop and how it grows with the temporal development of the hox and fgf patterns. In the script 07__digits_growingpaddle.py we implemented two different tissue geometries: An ellipse and a “paddle” that imitates the shape of the limb bud. Note that to implement Hox and FGF domains on a complex growing shape, quite a bit of complex code involving masks describing the tissue domain and angles to control the Hox and FGF domains was needed. At first, we are going to ignore these technical details.\n\nExercise 3.9 (Biology) Switching between geometries is easily done by changing the value of the parameter geometry on line 78 of the script. Start with the “ellipse” geometry. What do you observe with regards to the FGF gradient? What consequences does this have? Why would this be “a smart thing to do”?\nPlay around with vi/vj and Lx0 to investigate the effect of ellipse shape and development over time on the number, shape and robustness of digits that form.\nNow switch to the paddle geometry. Observe the patterns that emerge and compare these to the simulated and experimentally observed patterns in Figure 3E and 3F of the Raspopovic article. According to you what does and what doesn’t the model explain well?\nThe model explains the formation and positioning of the 4 digits, and how the distance between these digits increases along the limb bud. However, the model clearly does not simulate/explain the formation of the wrist bones that is observed experimentally. Interestingly the experimental data suggest that while digits may form from the “right to the left”, the “leftmost” bones of the wrist are there the earliest, suggesting there locally different conditions apply allowing earlier Turing patterning but with a very large wavelength happening there.\nTo implement non-square geometries, the script uses “masks”. Study how these masks are created in the functions create_ellipse_mask and create_tissue_mask.\nHint: Try the supplementary script supp_i__geometries.py.\nNote: For computational convenience, we use the “Class” data structure to implement common functions shared by the masks. This is a somewhat more advanced programming concept, so it’s ok if you don’t understand what it does yet. For the extra curious and motivated, feel free to check out the supplementary script supp_i__class_data_structure.py.\n\n\nAnswer Fgf gradient forms from bottom to top, as a consequence bottom fingers form first, often one of the bottom fingers forms and than splits. While in general Turing mechanisms give patterns they are not reproducible in terms of the positioning of the stripes. By starting the highest point of the FGF gradient locally it is determine where the first digits can arise, generating control over the positioning of the digits. Vi 0.1 goes okay, 0.2 stretched and splitting of digits  Vj too low too little room so less than 5 digits, vi too high more than 5 fingers, higher rates result in less reproducible patterns Below a nice pattern showing 5 fingers, but also showing near splitting of one of the fingers at the end The model explains the formation and positioning of the 4 digits, and how the distance between these digits increases along the limb bud. However, the model clearly does not simulate/explain the formation of the wrist bones that is observed experimentally. Interestingly the experimental data suggest that while digits may form from the “right to the left”, the “leftmost” bones of the wrist are there the earliest, suggesting there locally different conditions apply allowing earlier Turing patterning but with a very large wavelength happening there.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#master-students-exercise",
    "href": "pattern_practical_2.html#master-students-exercise",
    "title": "3  Turing digit patterns",
    "section": "3.10 Master students exercise",
    "text": "3.10 Master students exercise\n\nExercise 3.10 (Poly and oligodactyly) Above we played with ellipse size and growth rates, however in addition to mutations affecting tissue size and growth, also mutations in genes affecting the Hox and FGF morphogens may occur. Play with parameters that determine Hox and FGF expression zones on the paddle geometry. Which parameter changes lead to the formation of supernumerary fingers (polydactyly)? Which parameters lead to too few fingers (oligodactyly)?\nHint: The following supplementary scripts will be helpful to understand how the FGF and Hox domains were coded:\n\nsupp_i__ellipse_slice.py visualizes the usefulness of masks based on the example of selecting a slice of an ellipse;\nsupp_i__smooth_function.py visualizes the function smooth_function();\nsupp_i__morphological_operations.py visualizes morphological operations (binary dilate/erode and Gaussian blur) which are used for mask operations, calculating the Laplacian on the irregular domain, and to select pixels to generate the FGF and Hox pattern;\nsupp_i__maskDifference.py visualizes the difference of masks, which is how the function single_step_growth() checks if the tissue domain is growing or shrinking.\n\n\n\nAnswer Reference values:\n\n# Reaction-diffusion parameters for FGF\nDfgf = 0.001            # diffusion coefficient\nfgf_decay_rate = 0.0003 # degradation parameter\n\n# Opening angle of fgf source at the start and end\nfgf_angle_start_1 = -0.3*np.pi\nfgf_angle_start_2 = -0.5*np.pi\nfgf_angle_end_1   =  0.4*np.pi\nfgf_angle_end_2   = -0.6*np.pi    \nfgf_angle_rate    = 0.0002      # Exponential rate of change of angle\n\n# Opening angle of Hoxd13 expression domain at the start and end\nhox_angle_start_1 = -0.1*np.pi\nhox_angle_start_2 = -0.3*np.pi\nhox_angle_end_1   =  0.6*np.pi\nhox_angle_end_2   = -0.6*np.pi\nhox_angle_rate    = 0.001       # Exponential rate of change of angle\n\nhox_minR   = 0.3    # Minimum radius for Hoxd13 permissive zone at the end\nhox_maxR   = 0.85   # Maximum radius for Hoxd13 permissive zone (also minimum at the start)\nhox_R_rate = 0.001  # Rate of change of the Hoxd13 minimum radius over time\n\nDecreasing FGF diffusion increases the number of fingers and creates a fin-like appearance. However note that it also creates a shorter finger forming domain. Increasing FGF diffusion decreases the number of fingers.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html",
    "href": "pattern_practical_3.html",
    "title": "4  Clock-and-wavefront patterning",
    "section": "",
    "text": "4.1 Goal of the tutorial:\nIn this tutorial you will look at gradient formation and patterning using different mechanisms than you have seen previously. We will model the so-called clock-and-wavefront pattern, which stems from oscillations in gene expression and gene products and results in a regularly striped segmentation pattern in a growing tissue. Note that as compared to the Drosophila French Flag type case, this is thought to be the evolutionary ancestral model of segmenting animal body axes.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html#the-model-system",
    "href": "pattern_practical_3.html#the-model-system",
    "title": "4  Clock-and-wavefront patterning",
    "section": "4.2 The model system",
    "text": "4.2 The model system\nThe clock-and-wavefront model is an important model in describing somitogenesis. In this process early in embryo development, the somites, a precursor tissue for the vertebrae and other tissues later in development, are formed from the pre-somitic mesoderm. This mesodorm extends on the posterior end by growth, and the somites bud off periodically at the anterior end in the order of a couple of weeks (in humans).\nKey players in this model system are the protein FGF (fibroblast growth factor), and many genes and gene products that have an oscillatory pattern that will determine cell fate. However, for this tutorial, we simplify this to a single pair of gene mRNA and product, denoted with \\(m\\) and \\(p\\) for short.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html#programming-with-classes",
    "href": "pattern_practical_3.html#programming-with-classes",
    "title": "4  Clock-and-wavefront patterning",
    "section": "4.3 Programming with classes",
    "text": "4.3 Programming with classes\nBecause we are going to make a more complex model with a tissue existing of multiple cells, and each cell having its own concentrations of FGF, \\(m\\) and \\(p\\), we are going to use Classes in our code. You have been using Classes already: the data types such as int, str and bool have their own class, and the str class has many methods (=functions working on that class) defined, such as \"hello world\".upper(), but it is also possible to create custom classes. With classes, you can easily make objects, which is part of the object-oriented programming paradigm.\nToday, we are going to use classes for the different levels of our model: 1) tissue, 2) cell, 3) \\(m\\) & \\(p\\) clock and 4) the plotting. By using classes, we can separate things that happen on a tissue/cellular/clock scale, and seperate the model from the visualization of it. You will first work with 1 & 2 & 4, then 3 on its own and then combine all four yourself into one model.\nImportant concepts when working with classes are\n\nClass versus instance\nDefining the __init__ method and other methods\nClass attributes and using the keyword self\n\nThis tutorial should be doable without an extensive knowledge of classes as there are plenty of examples to copy-paste from, but feel free to read up on these concepts here at this online tutorial",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html#questions",
    "href": "pattern_practical_3.html#questions",
    "title": "4  Clock-and-wavefront patterning",
    "section": "4.4 Questions",
    "text": "4.4 Questions\n\nExercise 4.1 (Biology - An alternative gradient forming mechanism) In practical 1 we saw how gradients can be created through local production, diffusion and decay. However, other mechanisms for gradient formation are possible, such as cell lineage transport. Here we work with a model for the FGF gradient (fgfgradientfromgrowth.py) where only the rightmost/posterior cell produces FGF and grows, and in which cells upon division inherit this FGF from their mother cell. First study the code to see how it uses Classes and see what happens. Next, play with the model by varying the decay rate of FGF and the division rate of the cells. How does this affect the gradient? What happens if divisions are only allowed during the first half of the simulation (put divisiontime to 0.5 instead of 1).\n\n\nAnswer If growth rate goes up cell volumes increase faster, causing more dilution of FGF and hence a lower maximum of the gradient as due to the stable protein quite some time is needed to compensate for this decrease by more production, at the same time the gradient is less steep as cell division follow up faster and hence less time has passed and less day has taken place.\nIf decay rate goes up, maximum goes down but the gradient become steeper and shorter as for same time between divisions more decay takes place. If divisions stop halfway the simulation FGF in last cell goes up (no longer any loss due to growth-induced dilution) and FGF in all other cells goes to zero (no fresh influx from newly divided cells still having FGF).\n\n\nExercise 4.2 (Conceptual thinking) When would this type of gradient formation be more applicable than the earlier studied production, diffusion, decay type of gradient formation? Compare how this model is built-up to a production/diffusion/decay gradient formation model.\n\n\nAnswer Earlier gradient model: gradient length increases and slope decreases with faster diffusion and slower degradation. Here gradient driven by combination of growth/division and degradation. This new mechanism can only work if growth and patterning are occurring simultaneously, not if growth precedes patterning..\n\n\nExercise 4.3 (Mathematics) Let us now move to the other half of the clock and wavefront model, the clock part (clock.py), in which we implemented one of the earliest models for the somitogenesis oscillator from Lewis (2003) which models a gene that codes for a mRNA (\\(m\\)) that encodes a protein (\\(p\\)) that acts as a repressive transcription factor of this same gene. In class we discussed how for oscillations negative feedbacks, delays and non-linearity are important. Examine the code to find the differential equations governing this model and determine the negative feedback, delay and non-linearities in them.\n\n\nAnswer Non-linearity is in saturation function with power n, negative feedback is because affects own expression negatively, delay is here modeled explicitly through a special delay type differential equation\n\n\nExercise 4.4 (Biology) Play with the parameters of the model. How does the delay (\\(\\tau\\)) affect oscillations?\n\n\nAnswer Larger value of \\(\\tau\\)=delay results in a longer period (lower frequency) and higher amplitude. Remember how in class we discussed how removing introns reduced delays and affected oscillator period.\n\n\nExercise 4.5 (Algorithmic thinking) In the file rolling_clock.py, there is a different implementation of the clock. Compare the two files and find out how they differ. What benefits for studying the model does clock.py have over rolling_clock.py and vice versa? Ignore the added functions __copy__ and set_tau in this comparison. Some differences become clearer when you run the code too.\n\n\nAnswer Working with delay functions implies that you need the value of a variable not on the current time but on time-\\(\\tau\\). In the previous model we simply memorized the entire history of both variables. Here instead we only store the variable state for the length of the time delay \\(\\tau\\). This saves memory and hence runtime. This will become especially important when simulating a larger number of individual cells each having their own clocks. In the plotting we only plotted stored values, hence you only see what happened between current_time-\\(\\tau\\) and current_time instead of what happened from start of simulation till current time. Of course if you want to dynamically change \\(\\tau\\) you need to make sure you store values over an interval corresponding to the largest possible value of \\(\\tau\\) and need to know this beforehand.\n\n\nExercise 4.6 Next, we will combine the clock and FGF wavefront into a single model.\nRead and interpret existing code: Read the code of clockplusfgf.py and try to understand the assumptions from this model implementation by answering the following questions:\n\nEach cell has its own clock. What clock states is the tissue initialized with? And what clock states do newly divided cells get? 2. The FGF wavefront affects \\(\\tau\\) : what function is used for that? What are your expectations for the effect of the FGF wavefront on the cells’ clocks? 3. Does growth affect the clock state? 4. What is your opinion on the assumptions discussed in the three questions?\n\n\n\nAnswer\n\nGoing to code rolling_clock.py we see that clocks are initialized with the init function of the Class Clock, which gets as arguments \\(m0\\) and \\(p0\\), this initialization function is called when in Class Cell, in its respective __init__ function self.clock=Clock(..) is called. It uses default values of \\(m=0\\) and \\(p=0\\) to pass to the Clock. (In turn, the initialization function of Class Cell is called from the initialize_regular_tissue function of Class Tissue when the first cell is created). Newly divided cells inherit clock state and and FGF value from mother cell.\nThe function is: \\[\\tau_\\text{cell}=\\tau_\\text{model}(1+0.5\\frac{100-\\text{FGF}\\_\\text{cell}}{100}),\\] So if localfgf=max=100, \\(\\tau{local}\\)=\\(\\tau{global}\\), and if localfgf=min=0 \\(\\tau{local}\\)=1.5*\\(\\tau{global}\\) So for lower FGF \\(\\tau\\) increases slowing down the oscillations.\nNot directly, growth of cells, leading to volume increase and potential dilution effects is not affecting m or p levels, however since it is affecting FGF levels it is indirectly affecting the clock.\nInitialization is somewhat arbitrary, inheritance from mother cells is logical although in reality it is of course not perfect but noisy, FGF effect is also reasonable to get the observed slowing and waves, but it would be more logical for clock state like fgf to also be diluted if cells grow their volume\n\n\n\nExercise 4.7 (Biology) Describe how the model behaves and why. Do we get stable somites?\n\n\nAnswer First there is a startup period where we have synchronized oscillations in the first formed cells that all still have high FGF. Then as the tissue grows and the FGF gradient forms we obtain waves of oscillations moving from right to left with increasing amplitudes. The right to left movement arises because oscillations are faster on the right than on the left, this difference in oscillation frequency arises from the impact of FGF on oscillator frequency. However, the oscillations do not become halted and transformed into a stable spatial pattern because there is no memory mechanism in place.\n\n\nExercise 4.8 (Biology & Programming) What is still missing is a means to transform the temporal oscillations in the posterior of the tissue into a spatial pattern in the anterior. In the French Flag morphogen gradient lecture we discussed that memory mechanisms are important to stabilize spatial patterns once the start up signal that broke the symmetry and initialized patterning has gone. It turns out that such a memorization/stabilization mechanism is also essential to convert oscillations to stripes. Add a memorization mechanism to the model to achieve this. Hint: Make use of an extra ‘memory’ molecule \\(M\\) in each cell and perform its updating inside the function run_clocks. To see what you are doing, visualize the spatial pattern of \\(M\\)over time following the same procedure as for visualizing thefgf` and \\(p\\) values. Think about where, when and what should be memorized to design how \\(M\\) is regulated and what \\(M\\) itself affects and take a stepwise approach.\n\nAdd an extra ‘memory’ molecule \\(M\\) as a new attribute to the Class Cell, which value is inherited from the mother cell upon division. To create memory we need bistability, which can be easily achieved by having the memory molecule having a non-linear saturating positive feedback on itself. However, doing only this it will depend on the initial value of \\(M\\) -below or above a threshold- whether \\(M\\) will autoactivate or not. Additionally, if it happens it will occur across the entire tissue.\nThus, we need to make \\(M\\) dependent on the clock. To achieve this, we can start with low values of \\(M\\) that prevent autoactivation, and then have \\(M\\) activated by either \\(p\\) (part of the clock) or \\(M\\) itself using the following function: \\[\\frac{dM}{dt}=c\\max\\left(\\frac{p^4}{h_p^4+p^4}, \\frac{M^4}{h_M^4+M^4}\\right)-\\delta M.\\]\nHowever, now we simply always get activation of \\(M\\) everywhere.\nSince oscillations occur in the PSM and stripe formation occurs only more anteriorly we should constrain memorization to occur only below a certain FGF value. As a consequence, high levels of \\(M\\) now only arise anteriorly, but there is no pattern yet and oscillations keep occurring. This is because cells cycle through high and low p values so all cells at some point have high \\(p\\) values and can induce \\(M\\).\nSo in addition to have memorization occur only below a certain FGF value it should also occur above another, lower FGF value, constraining it to occur in a limited temporal window that enables cells passing through there with a high \\(p\\) state to induce a high \\(M\\) state while cells pasing through with a low \\(p\\) state to not induce a high \\(M\\) state. You should get somite patterns that are stable, but not necessarily regular. Still \\(m\\) and \\(p\\) oscillations continue.\nAs a final step, beyond the FGF window where we memorize \\(M\\) we no longer update either \\(M\\) or the clock.\n\n\n\nAnswer See code 05_answers_clockplusfgf_memory.py\n\n\nExercise 4.9 (Biology) Both zebrafish and mice are common model organisms, so we know a lot about the biological parameters of their somitogenesis. See the following list:\n\n\n\n\n\n\n\n\n\n\nParameter\nZebrafish\nReference\nMouse\nReference\n\n\n\n\nDuration of somitogenesis\n18 h\n(a)\n5 days\n(b)\n\n\nNumber of somites\n~30\n(a)\n65\n(c)\n\n\nSomite size\n50 \\(\\mu\\) / 30 \\(\\mu\\)\n(d) (a)\n120 \\(\\mu\\)\n(c)\n\n\nCells per somite\n~5 cell in diameter\n(a)\n5-11 (estimated from total cell size in 3D, ranging 1 order of magnitude)\n(c)\n\n\nClock period\n25 min /30 min\n(d) / (e) / (f)\n2-3 h\n(e)\n\n\n\nFrom these parameters, you can derive a number of desired model inputs/outcomes:\n\nThe total size of the tissue at the end of somitogenesis\nThe size of a cell\nSpeed of division\n\nUse the model and try to find suitable model parameters to recreate the development of both zebrafish and mice: is this model able to describe both of these processes? I.e., is this model able to deal with the scale differences between zebrafish and mice?\nA couple of notes:\n\nYou don’t have to exactly recreate the biological parameter with 100% precision, except for the number of segments/somites, although it can be a fun challenge to get a complete match.\nYou might want to adapt your plotting timestep to have sufficient but not too many plot updates in one simulation.\nYou can test the clock parameters separately with the clock.py/rolling_clock.py script. Don’t forget that FGF has an effect on \\(\\tau\\) !\n\n\n\nAnswer See codes 06_answers_clockplusfgf_memory_mouse.py and 06_answers_clockplusfgf_memory_zebrafish.py.\nZebrafish:\nTotaltime: 18 hours for duration of somitogenesis + 10 hours startup phase.\nCell width= 30-50 \\(\\mu\\) sized somite/5 cells per somite = 6-10 so 8\nGrowthrate: 30 somites x 5 cells x 8\\(\\mu\\) / 18 hours = \\(1200\\)\\(/18*3600\\)s\nFor clock frequency:\n\\(\\tau\\) times 0.25\nFac = 6 (multiplies alpha, beta, mu, v)\nThis gives 32/31 somites with on average 4.9 cells, so perfect.\nMouse:\nSimply doing the computations would give you:\nTotaltime: 5 days for duration of somitogenesis + 15 hours startup phase\nCell width = 120\\(\\mu\\) sized somite/5-11 cells = \\(120/8=15\\)\nGrowthrate: 65 somites x 8 cells x 15\\(\\mu\\) / 5 days = \\(7800/5* 24 *3600\\)s\nFor clock frequency:\n\\(\\tau\\) times 1.4-1.6 is sufficient (Fac=1 (multiplies alpha, beta, mu, v))\nSomehow this does not fully work:\nToo few cells (not \\(65*8=520\\)) too short tissue (not \\(65*8*15=7800\\)) and only 24 segments. First, remember \\(\\tau_{local}=(1+0.5 *(100-localfgf)/100))*\\tau_{global}\\). So \\(\\tau_{local}\\) max 1.5 times \\(\\tau_{global}\\) so the also increasing \\(\\tau_{global}\\) 1.4-1.6 times 1.5 makes it really slow. Indeed \\(\\tau\\) times 1.2 gives 31 segments, \\(\\tau\\) times 1 gives &gt; 36 segments, \\(\\tau\\) times 0.7 gives 49 segments\nGood to realize that the numbers in the table do not completely add up:\nIf it takes 5 days =\\(5*24=120\\) hours, and the period is 2-3 hours this would result in 40-60 segments but never 65. For a duration of 5.5 days and \\(\\tau\\) times 0.6 we get to 62 somites of size 9, so quite close.\n\n\nExercise 4.10 (Questions for master students) The model currently has a number of assumptions that we can question. Feel free to explore any of these further open questions and study how it effects the outcome of the model:\n\nWhat if the relationship between FGF and \\(\\tau\\) is shaped differently? For instance, if the clock runs faster on the left/anterior than on the right/posterior?\nWhat if the clock state of a daughter cell is started fresh rather than being a copy from the mother cell?\nWhat if the clock \\(m\\) and \\(p\\) are diluted by growth?\nWhat if \\(\\tau\\) is unaffected by FGF: can we still get somites fixed in place?\n\n\n\nAnswer These are open questions and do not have a single correct answer. Feel free to discuss with us if you find something interesting or have an interesting answer.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html#relevant-literature",
    "href": "pattern_practical_3.html#relevant-literature",
    "title": "4  Clock-and-wavefront patterning",
    "section": "4.5 Relevant literature",
    "text": "4.5 Relevant literature\nIf you want to know more about the model system and previous models, have a look at the following (after the tutorial):\nLewis (2003) Autoinhibition with transcriptional delay: a simple mechanism for the zebrafish somitogenesis oscillator.\nHester (2012) A Multi-cell, Multi-scale Model of Vertebrate Segmentation and Somite Formation.\nHerrgen et al. (2010) Intercellular coupling regulates the period of the segmentation clock.\nSoroldoni et al. (2014) Genetic oscillations. A Doppler effect in embryonic pattern formation.\nSonnen et al. (2018) Modulation of Phase Shift between Wnt and Notch Signaling Oscillations Controls Mesoderm Segmentation.\nBulusu et al. (2017) Spatiotemporal Analysis of a Glycolytic Activity Gradient Linked to Mouse Embryo Mesoderm Development.\nOostrom et al. (2025) Coupling of cell proliferation to the segmentation clock ensures robust somite scaling.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html#references-for-biological-parameters",
    "href": "pattern_practical_3.html#references-for-biological-parameters",
    "title": "4  Clock-and-wavefront patterning",
    "section": "4.6 References for biological parameters:",
    "text": "4.6 References for biological parameters:\n\nStickney, Barresi, and Devoto (2000)\nSaga (2012)\nTam (1981)\nIshimatsu et al. (2018)\nCarraco, Martins-Jesus, and Andrade (2022)\nTomka, Iber, and Boareto (2018)\n\n\n\n\n\nBulusu, Vinay, Nicole Prior, Marteinn T Snaebjornsson, Andreas Kuehne, Katharina F Sonnen, Jana Kress, Frank Stein, Carsten Schultz, Uwe Sauer, and Alexander Aulehla. 2017. “Spatiotemporal Analysis of a Glycolytic Activity Gradient Linked to Mouse Embryo Mesoderm Development.” Developmental Cell 40 (4): 331–41.\n\n\nCarraco, Gil, Ana P Martins-Jesus, and Raquel P Andrade. 2022. “The Vertebrate Embryo Clock: Common Players Dancing to a Different Beat.” Frontiers in Cell and Developmental Biology 10: 944016.\n\n\nHerrgen, Leah, Saúl Ares, Luis G Morelli, Christian Schröter, Frank Jülicher, and Andrew C Oates. 2010. “Intercellular Coupling Regulates the Period of the Segmentation Clock.” Current Biology 20 (14): 1244–53.\n\n\nHester, Susan D. 2012. “Multi-Scale Cell-Based Computational Models of Vertebrate Segmentation and Somitogenesis Illuminate Coordination of Developmental Mechanisms Across Scales.” PhD thesis, Indiana University.\n\n\nIshimatsu, Kana, Tom W Hiscock, Zach M Collins, Dini Wahyu Kartika Sari, Kenny Lischer, David L Richmond, Yasumasa Bessho, Takaaki Matsui, and Sean G Megason. 2018. “Size-Reduced Embryos Reveal a Gradient Scaling-Based Mechanism for Zebrafish Somite Formation.” Development 145 (11): dev161257.\n\n\nLewis, Julian. 2003. “Autoinhibition with Transcriptional Delay: A Simple Mechanism for the Zebrafish Somitogenesis Oscillator.” Current Biology 13 (16): 1398–408.\n\n\nOostrom, Marek J van, Yuting I Li, Wilke HM Meijer, Tomas EJC Noordzij, Charis Fountas, Erika Timmers, Jeroen Korving, Wouter M Thomas, Benjamin D Simons, and Katharina F Sonnen. 2025. “Scaling of Mouse Somitogenesis by Coupling of Cell Cycle to Segmentation Clock Oscillations.” bioRxiv, 2025–01.\n\n\nSaga, Yumiko. 2012. “The Mechanism of Somite Formation in Mice.” Current Opinion in Genetics & Development 22 (4): 331–38.\n\n\nSonnen, Katharina F, Volker M Lauschke, Julia Uraji, Henning J Falk, Yvonne Petersen, Maja C Funk, Mathias Beaupeux, Paul François, Christoph A Merten, and Alexander Aulehla. 2018. “Modulation of Phase Shift Between Wnt and Notch Signaling Oscillations Controls Mesoderm Segmentation.” Cell 172 (5): 1079–90.\n\n\nSoroldoni, Daniele, David J Jörg, Luis G Morelli, David L Richmond, Johannes Schindelin, Frank Jülicher, and Andrew C Oates. 2014. “A Doppler Effect in Embryonic Pattern Formation.” Science 345 (6193): 222–25.\n\n\nStickney, Heather L, Michael JF Barresi, and Stephen H Devoto. 2000. “Somite Development in Zebrafish.” Developmental Dynamics: An Official Publication of the American Association of Anatomists 219 (3): 287–303.\n\n\nTam, PPL. 1981. “The Control of Somitogenesis in Mouse Embryos.” Development 65 (Supplement): 103–28.\n\n\nTomka, Tomas, Dagmar Iber, and Marcelo Boareto. 2018. “Travelling Waves in Somitogenesis: Collective Cellular Properties Emerge from Time-Delayed Juxtacrine Oscillation Coupling.” Progress in Biophysics and Molecular Biology 137: 76–87.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "morpho_intro_text.html",
    "href": "morpho_intro_text.html",
    "title": "5  What is morphogenesis?",
    "section": "",
    "text": "TODO\n\nWrite a brief intro to the morphogenesis practicals",
    "crumbs": [
      "II) Morphogenesis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What is morphogenesis?</span>"
    ]
  },
  {
    "objectID": "differentiation_intro_text.html",
    "href": "differentiation_intro_text.html",
    "title": "7  Differentiation introduction",
    "section": "",
    "text": "TODO\n\nDifferentiation is fun.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Differentiation introduction</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_1.html",
    "href": "differentiation_practical_1.html",
    "title": "8  Gene regulation in time",
    "section": "",
    "text": "8.1 Introduction\nIn this practical you will get hands on experience into making gene regulatory networks models. First you will practice how to encode regulatory interactions into ordinary differential equations and logical rules. Next you will study how many interactions produce self-sustained activity configurations (attractors) in a Boolean network model. Lastly, you will learn how to predict the genes that cause cell fate transitions (attractor changes).\nToday we will use the following python file: circuitsfunctions.py. In this file you will find the code of the functions we use throughout the practical, have a quick look at it. Look at the name of the functions, inputs and outputs.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gene regulation in time</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_1.html#part-i---encoding-regulatory-interactions",
    "href": "differentiation_practical_1.html#part-i---encoding-regulatory-interactions",
    "title": "8  Gene regulation in time",
    "section": "8.2 Part I - Encoding regulatory interactions",
    "text": "8.2 Part I - Encoding regulatory interactions\nOpen Boolean_practical.py and familiarize yourself with the functions ODEgeneRegulation() and logicalRule().\nBoth functions model an AND gate where nodeA and nodeB positively regulate nodeC. This could represent that A and B are transcription factors that form a protein complex, and that is via this complex that they can regulate the expression of C. While ODEgeneRegulation() encodes this regulation with an ordinary differential equation that requires several parameters, logicalRule() only needs the logical operator relating the input genes.\ndef ODEgeneRegulation(a,t,parameters): \n    prod=parameters['prod']\n    decay=parameters['decay']\n    Ksat=parameters['Ksat']\n    nodeA=parameters['nodeA']\n    nodeB=parameters['nodeB']\n    n=parameters['n']\n    outputC=a[0]\n    doutputC=prod*nodeA**n/(Ksat**n+nodeA**n)*nodeB**n/(Ksat**n+nodeB**n)-decay*outputC  \n    return(doutputC)\n\ndef logicalRule(nodeA,nodeB):\n    return(nodeA and nodeB)\n\nExercise 8.1 (Mathematical thinking) What is the minimum information you need to encode a regulatory interaction with either function? In what cases would you prefer to use an ODE or a logical rule for a model?\n\nLet’s run the model to simulate what happens if nodeA and nodeB are both active/expressed. Do this for the ODE and the Boolean logic code model using the following lines inside the main().\n# ODE model - see the parameters used in circuitsfunctions.py\n# ODErun has three arguments: model, geneA, geneB\nA=10\nB=10\nODErun(ODEgeneRegulation,A,B) \n\n# Boolean model\n# look at the terminal for the result:\nA=1\nB=1\nprint(\"the boolean operation of nodeA \",A,\" AND nodeB\",B,\" is:\", logicalRule(A,B))\nNow, let’s explore the output of each function using different values of nodeA and nodeB. We are going to use the code below to plot the output of the ODE and the Boolean model in a 2D heatmap.\n# First, Boolean network simulation - AND gate\nexplorationvalue=2 # a Boolean model assumes there is only 2 possible states: 0 or 1\nbool_output = np.zeros((explorationvalue, explorationvalue))\nfor nodeA in range(0, explorationvalue):\n    for nodeB in range(0, explorationvalue):\n        bool_output[nodeA, nodeB] = nodeA and nodeB #AND # CHANGE THIS\n\n# Now, ODE model simulation\nexplorationvalue = 11 # an ODE model allows us to explore more values than a boolean model\node_output = np.zeros((explorationvalue, explorationvalue))\nfor nodeA in range(0, explorationvalue):\n    for nodeB in range(0, explorationvalue):\n        parameters = {'prod': 0.01, 'decay': 0.001,'Ksat': 4, 'n': 2,'nodeA':nodeA,'nodeB':nodeB} # prod, decay, Ksat, n, and initial values for A and B\n        cells = odeint(ODEgeneRegulation, 0, np.arange(0, 1000.1 , 0.1), args=(parameters,)) #np.shape\n        ode_output[nodeA, nodeB] = cells[-1, 0]\n\n# use this code to see your ode and boolean results side by side\nODEBooleanPlot(ode_output, bool_output)\n\nExercise 8.2 (Biology) What differences do you see in each case? (differences in what?!) Discuss x, y, and z (question is a little short otherwise)\n\nNow let’s compare how ODE and Boolean logic represent different logical gates. You can use the same code in the previous box. Remember to modify the logical operator in the Boolean section, and to modify the doutputC equation in ODEgeneRegulation:\n\neither nodeA or nodeB can activate outputC (OR gate)\nthat nodeA represses outputC (NOT gate)\nthat both nodeA and nodeB repress outputC (NOT nodeA & NOT nodeB )\nthat nodeA represses outputC but nodeB activates it (NOT nodeA, yes nodeB)\nthat nodeA or nodeB can activate outputC, but not both at the same time (XOR gate)\nany other biological scenario yon can think of!\n\n\ntip: include an extra input node, and make a 3-node logical gate.\n\n\nExercise 8.3 (Algorithmic thinking) Q3 Is there a logical gate that can be better represented with an ODE than with Boolean logic? Think of examples of biological regulatory interactions that can be described with each logical gate?",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gene regulation in time</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_1.html#part-ii-gene-regulatory-network",
    "href": "differentiation_practical_1.html#part-ii-gene-regulatory-network",
    "title": "8  Gene regulation in time",
    "section": "8.3 Part II – Gene regulatory network",
    "text": "8.3 Part II – Gene regulatory network\nNow let’s move to a network model made of many individual regulatory interactions. This model includes regulatory interactions experimentally determined in the cells of plant roots, here they encoded as logical rules. The model consists of 18 nodes representing transcription factors, hormones, peptides, and multitude regulatory interactions among them. Look at the rootNetwork() function in the circuitFunctions.py file. See how the activity of each node is determined by the state of its regulators using and combining the logical operators AND, OR and NOT. Find the model here: https://www.nature.com/articles/s41598-020-60251-8\nLet’s define a random initial condition for each of these 18 nodes, and the timesteps to solve the system using the logical functions. To find the state of the nodes in the network the next timestep, we need to give the state of our nodes to the rootNetwork() function, and then save the output of this function. We will save in matrix the initial condition, and how these 18 nodes change in the timesteps defined. At the end the function plotBooleanTimecourse() will show us the network changes in the simulated timesteps. * tip: 100 timesteps is enough to reach the attractors. For the initial condition you can also use that of one of the attractors reported in the paper and check what happens when you update them.\ntimesteps=20\nnodes=18\nmatrix = np.zeros((timesteps+1, nodes), dtype=int) \nmatrix[0,:] = np.random.randint(0, 2, size=nodes) #Random initial condition\nfor i in range(timesteps):\n    parameters= {'CK': matrix[i,0], 'ARR1': matrix[i,1], 'SHY2': matrix[i,2], 'AUXIAAR': matrix[i,3], 'ARFR': matrix[i,4], 'ARF10': matrix[i,5], 'ARF5': matrix[i,6], 'XAL1': matrix[i,7], 'PLT': matrix[i,8], 'AUX': matrix[i,9], 'SCR': matrix[i,10], 'SHR': matrix[i,11], 'MIR165': matrix[i,12], 'PHB': matrix[i,13], 'JKD': matrix[i,14], 'MGP': matrix[i,15], 'WOX5': matrix[i,16], 'CLE40': matrix[i,17]}\n    matrix[i+1, :] = rootNetwork(parameters)\n\nplotBooleanTimecourse(matrix,timesteps)\nInstead of a random initial condition try this one:\nmatrix[0,:]=[0,1,1,0,1,1,0,0,1,0,1,1,1,1,1,0,1,0]\nUse the asynchronous updating scheme by changing the function to rootNetworkAsynchronous() and see what happens to this initial condition.\n\nExercise 8.4 (Algorithmic thinking) Q4 What happens now and why? What does an asyncrhonous update do?\n\nThis network has 18 nodes, and then 218 = 262,144 possible states. We can either solve each of these conditions, or instead explore just a subset of them. Using the previous code, add for loop to solve 100 random initial conditions, and then save the final activity configurations (attractors) in the attractors matrix. *tip: 100 initial conditons is a good number to start with. # The attractors matrix should have the shape (initial_conditions, nodes) so that you can plot the results using the function plotBooleanAttractors.\nICs=100\nattractors = np.zeros((ICs, len(parameters))) # new\n\n# your code\n\nplotBooleanAttractors(attractors) # it takes as argument your matrix of attractors\nYou probably found many different attractors. First, let’s use a multidimensional reduction technique to see if each of initial conditions form groups of similar cells.\nUMAPBoolean(attractors)\n\nExercise 8.5 (Biology / abstract thinking) Q5 Why do you see clearly defined groups and not a continuous distribution of attractors?\n\nNow let’s analyse the attractors you found. First, let’s group similar attractors:\nattractors_sorted = np.array(sorted(attractors.tolist()))\nplotBooleanAttractors(attractors_sorted) \n\nExercise 8.6 (Biology / abstract thinking) Q6 Do some attractors occur more frequently than others. Do they all have the same basin size of attraction? Why is this?\n\n\nExercise 8.7 (Biology) Some nodes are active (1) in most attractors, others in half, and others in very few. In how many of them is the node SHR active? What does this suggest about their regulation?\n\nNow let’s remove the repeated rows (duplicate attractor states) to see unique attractors:\n_, unique_indices = np.unique(attractors, axis=0, return_index=True)\nattractors_unique = attractors[np.sort(unique_indices)]\nplotBooleanAttractors(attractors_unique) \n\nExercise 8.8 (Biology / abstract thinking) How many unique attractors did you find? Compare them with the ones reported in the paper. Are they all fixed points?",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gene regulation in time</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_1.html#part-iii-cell-differentiation-jumping-from-one-attractor-to-another",
    "href": "differentiation_practical_1.html#part-iii-cell-differentiation-jumping-from-one-attractor-to-another",
    "title": "8  Gene regulation in time",
    "section": "8.4 Part III – Cell differentiation – jumping from one attractor to another",
    "text": "8.4 Part III – Cell differentiation – jumping from one attractor to another\nDepending on what we want to answer a continuous or discrete model may be more appropriate for a model. To study the role of many genes in cell differentiation, a Boolean model might be better particularly if we lack details of the parameters underlying each reaction. If we want to study how cells jump from one state to another, a continuous model might be more appropriate.\nHere we will see how we can convert the Boolean model to a continuous one and use it to predict which regulators are able of causing a change in the state of the system (changes in cell fate!)\nCompare the code of the functions rootNetwork() and rootNetworkODE(). Notice how in rootNetworkODE() the logical rules are represented with min and max functions, and then used in a sigmoidal function to create a continuous ODE model. * AND operator is a min function, OR operator is a max function, and NOT operator is 1-x. Use the code below to run a random initial condition for the 18 nodes (IC) and see how the system behaves.\ntimerunning=10.1 \ntimes = np.arange(0, timerunning, 0.1)\n\nIC = np.random.randint(0, 2, size=18).tolist() #random initial condition\nparameters = {'decayrate': 1, 'h': 50} \ncells = odeint(rootNetworkODE, IC, times, args=(parameters,)) \n\nplotODEroot(cells,times)\n\nExercise 8.9 (Modeling choices) Do the attractors match those recovered with the Boolean network?\n\nNow let’s use the model to study cell differentiation. Let’s start in the following initial condition (IC vector), and find a change in a node that produce a jump to another attractor (end). For this you can simply flip the activity of a gene in the initial condition (from 0 -&gt;1 or 1-&gt;0) and see the final attractor matches the desired end state.\n# We start here: \nIC=[0,0,0,0,1,0,1,1,1,1,1,1,1,0,1,0,1,0]\n# We want to end here. \nend=[1,1,0,0,1,1,1,1,1,1,0,0,1,0,0,0,0,1]\n# node order\n# CK, ARR1, SHY2, AUXIAAR, ARFR, ARF10, ARF5, XAL1, PLT, AUX, SCR, SHR, MIR165, PHB, JKD, MGP, WOX5, CLE40\n\n# your code \n\nplotODErootTransition(cells,times) # use this function to plot your result\n\nExercise 8.10 (Algorithmic thinking / biology) What regulator causes the transition between these attractors? How many nodes change their activity between the initial and final state? what could be the biological meaning of this switch? how would you test this experimentally?\n\n\nExercise 8.11 (Biology) Finally, use the model to predict the rest of the attractor transitions because of single node changes. Are all attractor transitions possible, or are there preferred differentiation routes?",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gene regulation in time</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html",
    "href": "differentiation_practical_2.html",
    "title": "9  Gene regulation in space",
    "section": "",
    "text": "9.1 TODO FOR",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#todo-for",
    "href": "differentiation_practical_2.html#todo-for",
    "title": "9  Gene regulation in space",
    "section": "",
    "text": "(Monica?), check labels for questions (‘mathematical thinking’, ‘biology’, etc)\n(Monica?), figure out how/where to share the files and answers",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#section",
    "href": "differentiation_practical_2.html#section",
    "title": "9  Gene regulation in space",
    "section": "",
    "text": "How wonderful that we have met with a paradox.  Now we have some hope of making progress – Niehls Bohr",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#introduction",
    "href": "differentiation_practical_2.html#introduction",
    "title": "9  Gene regulation in space",
    "section": "9.2 Introduction",
    "text": "9.2 Introduction\nIn this practical you will study the effects of auxin, a plant hormone, on gene regulation in the root. Auxin is a hormone virtually involved in all developmental processes in plants, from embryo patterning and floral organ determination to leaf disposition and root growth. In each context, auxin triggers specific responses, which can be explained by the underlying regulatory networks—namely by the redundancy of molecular players in the auxin signalling pathway (the auxin response factors, ARFs), and how they are expressed in different tissues.\nIn the root, researchers have found an auxin–WOX5 paradox: some experiments show that auxin promotes the expression of WOX5 (a root stem cell transcription factor), while others found WOX5 repression. This opposite auxin regulation is mediated by different ARFs—ARF5 and ARF10—posing the possibility that accounting for both ARF5 and ARF10 regulatory networks can reconcile the results. To this end, you will use an in silico root model to address this paradox and propose an explanation.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#biological-background",
    "href": "differentiation_practical_2.html#biological-background",
    "title": "9  Gene regulation in space",
    "section": "9.3 Biological background",
    "text": "9.3 Biological background\nPlant roots grow thanks to the presence of stem cells housed in a niche at the root tip. A subset of these cells expresses WOX5, maintaining the surrounding cells as stem cells. WOX5 is regulated by several factors, and network models help us understand how.\nThe Boolean network used last week describes the gene and hormonal activity configurations of the cells in the root apex. In this model WOX5 is regulated positively by ARF5 and negatively by ARF10 and CLE40. Auxin regulates stem cell activity in the roots, and multiple links exist between auxin and the regulators in the Boolean network. Both ARF5 and ARF10 are auxin response factors activated by auxin. Moreover, root transcription factors control ARF5 and ARF10 expression such that they are not expressed in every cell of the root apex. This regulation is included in the Boolean model we studied last week. Whether this is relevant to understanding the auxin–WOX5 paradox is unclear. Here, each cell of the in silico root model carries a copy of the Boolean network, which allows us to test hypotheses and find an explanation.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#the-model",
    "href": "differentiation_practical_2.html#the-model",
    "title": "9  Gene regulation in space",
    "section": "9.4 The model",
    "text": "9.4 The model\nThe model simulates a root apex, showing the stem cell niche at the apex (lime green cells in the cell graphics; purple cells in the WOX5 visualization). The different tissues of the root are spatially organized. Each cell has a different set of expressed genes, which we account for by initializing each cell’s Boolean network in the QC, Vascular, Endodermis, and Columella attractors. (Cortex and epidermis are not modeled.) Multiple grids store different cell properties: cell-type, auxin levels, and expression of each gene.\nYou can visualize ARF10, ARF5, MGP, and WOX5 expression using the provided plotting function.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#files",
    "href": "differentiation_practical_2.html#files",
    "title": "9  Gene regulation in space",
    "section": "9.5 Files",
    "text": "9.5 Files\nRootfunctions.py – contains model functions. You do not need to modify them but should understand what they do.\nRoot-model-Auxin.py – code to simulate the in silico root.\nauxin_grid.npy – initial auxin levels in each cell.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#questions",
    "href": "differentiation_practical_2.html#questions",
    "title": "9  Gene regulation in space",
    "section": "9.6 Questions",
    "text": "9.6 Questions\n\nExercise 9.1 Algorithmic thinking\nFamiliarize yourself with the model. Examine Rootfunctions.py to see how each process is modeled and how they are coupled.\n\n\nExercise 9.2 Biology\nWhat happens if you update the network every timestep? Does that make biological sense? Where in the code can you make this modification?\n\n\nExercise 9.3 Biology & algorithmic thinking\nSimulate an auxin treatment by changing the AuxinTreatment parameter. Try values of 10 and 750. This increases auxin levels in all cells by the given amount.\nHow does auxin treatment affect gene expression in the root? (You can visualize different genes by changing the grid provided to plotGrids().)\nDoes the model output correspond to the transcriptional reporter of WOX5 shown in Figure 1? Do you see activation or repression as reported experimentally?\n\n\nExercise 9.4 Biology & algorithmic/mathematical thinking\nThe model does not yet reproduce the experiments. To account for quantitative effects of auxin, implement two hypotheses based on the literature:\nAuxin represses MGP expression.\nExtremely high auxin levels (unrealistic in vivo) repress WOX5 expression.\nModify the equations for MGP and WOX5 in rootNetwork accordingly.\nFor MGP, retain its logical rule but multiply its production term by a saturation function of auxin (Km = 45):\ndMGP = (45/(45+auxininput))((-np.exp(0.5  h) + np.exp(-h * w_MGP)) / ((1 - np.exp(0.5 * h)) * (1 + np.exp(-h * (w_MGP - 0.5))))) - (lambda_ * MGP)\nSimilarly, add a negative regulation by auxin to the WOX5 equation (Km = 1000).\nRun control and auxin treatments with (i) updated MGP, (ii) updated WOX5, and (iii) both updates.\nDo you now see the differences reported by experimentalists?\n\n\nExercise 9.5 Biological interpretation\nYou should observe that WOX5 expression gradually expands into the endodermis and disappears from QC cells when both equations are updated. This matches experimental microscopy. To explain repression:\nWhy do different parts of the root respond differently to auxin? Why does induction occur mainly in upper cells and repression in basal cells?\n\n\nExercise 9.6 Algorithmic thinking\nExperimental repression of WOX5 was detected by RT-PCR at the very apex. Reproduce this in silico by saving the average WOX5 levels of cells at the apex (y &lt; 40) at the end of the simulation for all three treatments. Plot them.\nWhat is the root apex response to auxin treatments? Considering that the root contains mixed cell types, which cells best represent your plotted values?\n\n\nExercise 9.7 Biology & algorithmic thinking\nRepeat the analysis for all cells, for QC cells, and for endodermis cells. Compare WOX5 regulation across tissues.\nDo you see a difference between your observations and the simulated RT-PCR plot? Why?\n\n\nExercise 9.8 Biology & algorithmic thinking\nChanges in WOX5 expression depend on tissue context and auxin dosage. Simulate a range of auxin treatments from 0 to 750 (choose your step size).\nCan you now explain the auxin–WOX5 paradox? What mechanism do you propose?\n\n\nExercise 9.9 Biology & conceptual thinking\nPerform an in silico intervention to alter WOX5 dosage responsiveness. For example, enforce continuous induction or block its induction in the endodermis. How would you test these model predictions experimentally?\n\n\nExercise 9.10 Biological thinking\nAll models are wrong, but some are useful. The model should offer mechanistic insight into paradoxical auxin responses. Yet the original experiments used different auxin analogs: NAA (transported independently of PINs), IAA (PIN-transported), and NPA (blocks PINs, increasing auxin in the apex). How does this information change your conclusions? What extensions to the model would account for these differences?",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "evdev_intro_text.html",
    "href": "evdev_intro_text.html",
    "title": "10  Environmental inputs",
    "section": "",
    "text": "So far we have largely discussed developmental patterning processes as if they are either occurring under constant environmental conditions or as if environmental conditions have no effect. A minor exception to this is the discussion on robustness of patterning under morphogen gradients, where differences in input of maternal resources, which in turn may depend on food conditions, may impact embryo size and hence scaling is required.\nIn contrast to our discussion so far, environmental conditions may have a major impact on developmental processes. This ranges from the effect of temperature on the body sizes of fruitflies and butterflies, and on the gender of hatching sea turtles, nutritional effects on ant size and worker function to the complete reshaping of organs, organ positioning and body plans in plants. Indeed, while for animals symmetry and scaling are essential for mobility, for plants adaptation to the conditions they find themselves in is of key importance, making developmental plasticity more pervasive in plant development. In the practical we are going to investigate temperature induced leaf hyponasty, in which plants develop leaves with longer petioles (stems) and smaller blades (leaf surface itself) that are positioned in a more upright angle, investigating the adaptive value of this developmental plasticity.\nApart from the question whether plasticity of a developmental process is adaptive, a major question is how this plasticity can be united with robustness. That is, how certain aspects of development can be adjusted, while other aspects can be maintained (i.e. the plant still makes a leaf, that has still a top, bottom, stomata veins, etc but other things are allowed to vary). Additionally, a question is how the networks that drive developmental patterning and e.g. cell fate decisions (Part III) are integrated with networks that sense, process and combine various environmental signals to decide how these developmental processes are to be adjusted. This will be discussed in the lecture.\nIn addition to developmental processes, also physiology, behavior and evolution of course depend on environmental conditions. Examples of the latter two will be discussed in Part V.",
    "crumbs": [
      "IV) Environment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Environmental inputs</span>"
    ]
  },
  {
    "objectID": "evdev_practical.html",
    "href": "evdev_practical.html",
    "title": "11  Modeling Thermomorphogenesis in Plants",
    "section": "",
    "text": "11.1 The model\nIn code 00_Environment_and_development.py we start with a simple two ODE model where leaf area (\\(L\\)) grows with rate \\(G\\), which depends on carbon concentration (\\(C_c\\)) and leaf area. Carbon (\\(C\\)) is produced by photosynthesis (\\(P\\)) dependent on leaf area, leaf angle (\\(\\alpha\\)), and temperature (\\(T\\)). Carbon is consumed by growth and respiration (\\(R\\)), dependent on temperature and leaf area again:\n\\(\\frac{dC}{dt}=P(L,T,\\alpha)-R(L,T)-G(C_c,L)\\)\n\\(\\frac{dL}{dt}=G(C_c,L)\\)",
    "crumbs": [
      "IV) Environment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling Thermomorphogenesis in Plants</span>"
    ]
  },
  {
    "objectID": "evdev_practical.html#the-model",
    "href": "evdev_practical.html#the-model",
    "title": "11  Modeling Thermomorphogenesis in Plants",
    "section": "",
    "text": "Photosynthesis\nPhotosynthesis is a complex process that can be modeled in many different ways with varying complexity. Here, in the function Photosynthesis_per_m2() we included a semi-detailed photosynthesis model based on the assumption that the protein RubisCo is the limiting step in photosynthesis. This model has 6 parameters and 2 environmental variables, of which all parameters are temperature sensitive. This temperature sensitivity is based on the Arrhenius equation, which describes the temperature dependency of chemical reaction rates.\n\n\nMaintenance Respiration\nMaintenance respiration is a term used in biological systems to describe all energy/carbon consuming processes that a plant must do to survive. Similar to photosynthesis, the rates of these reactions increase with temperature, and as such, carbon costs greatly increase. We model this in the function Respiration_per_m2() using the Q10 equation, that describes how much a rate increases for 10 degrees of temperature increase.",
    "crumbs": [
      "IV) Environment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling Thermomorphogenesis in Plants</span>"
    ]
  },
  {
    "objectID": "evdev_practical.html#exercises",
    "href": "evdev_practical.html#exercises",
    "title": "11  Modeling Thermomorphogenesis in Plants",
    "section": "11.2 Exercises",
    "text": "11.2 Exercises\n\nExercise 11.1 (Algorithmic thinking and Biology - Temperature Effects on Photosynthesis, Respiration and Growth) Let us first investigate how photosynthesis and respiration depend on temperature.\n\nPlot photosynthesis and respiration rate as a function of temperature, using the functions defined above. Explain: What happens to net carbon gain at high temperature? Why is high temperature problematic for growth in this model?\nRun the code and study the output for leaf area and carbon. Why does leaf area increase exponentially while carbon levels saturate? What carbon level are you actually plotting?\nNow run simulations of the ODE model for low (15°C), medium (25°C), and high (35°C)\ntemperatures. Which plant performs best? Which plant worst? Why is this?\n\n\n\nAnswer\n\nSee answer code ‘01_Environment_and_development_answer.py’ You need to do this plotting outside of the plant growth simulation loop by calling the photosynthesis and respiration functions for a range of temperatures (10-40 degrees).\n\\(dLA/dt=Gmax*(C*/(C*+K))*LA\\)\n\nThis gives \\(LA(t)=LA(0)*exp(Gmax*(C/(C+K))*t)\\)\nIn contrast:\n\\(dC/dt=photosynthesis-respiration-growth\\)\n\\(dC/dt=p*LA-d*LA- Gmax*(C*/(C*+K))*LA\\)\nSolving \\(dC/dt=0\\) we get \\(LA=0\\) (no plant) or\n\\(p-d-Gmax*(C*/(C*+K))=0\\)\n\\(p-d=Gmax*(C*/(C*+K))\\)\n\\((p-d)*C*+(p-d)*K=Gmax*C*\\)\n\\((p-d)*K=(Gmax-p+d)*C*\\)\n\\(C*=(p-d)*K/(Gmax-p+d)\\)\n\\(C*=C/LA\\)\nSo carbon concentration equilibrates, not total carbon\n\nSee answer code 01_Environment_and_development_answer.py for running and plotting the 3 different temperatures. At 15 degrees respiration is low but photosynthesis is far below optimal, so at 25 degrees you loose more to respiration but that is overcompensated by the increase in photosynthesis, at 35 degrees photosynthesis is beyond its optimum while respiration is now very high; as a consequence 25 degrees gives optimum growth and 35 the worst.\n\n\n\nExercise 11.2 (Mathematical & Biological thinking - Hyponasty – Leaf angle increase) Until now we have investigated the effect of temperature on plants that do not respond to their environment, but plants do respond to their surroundings.Plants display a variety of responses to elevated temperature, and one of these is so-called leaf hyponasty in which leaves are positioned in a more upward orientation that is often accompanied by longer leaf stems (petioles) and smaller leaves (blades). Let us extend the model to first simply include the orientation aspect of this hyponastic response to high temperature.\n\nThe photosynthesis function already includes a term for ‘effective leaf area (LA_eff)’, but the function does currently return the total leaf area. Rewrite this function to return effective leaf area as a function of leaf elevation angle, assuming that light comes from above. Plot the effective leaf area as a function of angle. How does hyponasty affect photosynthesis? What other factors could affect effective leaf area?\nNow investigate the effect of the increased leaf angle on plant growth. Compare medium (25°C) and high temperature (35°C) for normal (20°) and increased (40°) angle.\n\n\n\nAnswer\n\nSee answer code ‘02_Environment_and_development_answer.py’ Under the assumption that light comes from above it holds that effective leaf area=the area projected on the horizontal surface which can be found by taking the cosine of the angle of the leaf. So the larger the angle the smaller the effective leaf area. This results in an effective reduction of leaf area and thus photosynthesis. Shading, chloroplast density, leaf size could all affect effective leaf area.\nAs can be expected from the above, the temperature increase combined with the angle increase now both reduce photosynthesis and hence carbon and leaf area increase, so it is basically adding insult to injury rather than helping.\n\n\n\nExercise 11.3 (Biology - Hyponasty– Cooling Benefit) Leaf hyponasty is shown to lower leaf temperature with a few degrees by improving heat dissipation and decreasing the area in which direct sunlight hits the leaves.\n\nInvestigate the effect of the leaf cooling. How much cooling is needed for hyponasty to have a net positive effect? Simply assume a certain cooling effect and hence apply a lower temperature than the environmental one.\nHyponasty is shown to lower leaf temperature by only 1-2°C (see the left picture at the beginning of this document), what is the effect of this amount of cooling on plant growth? Would you argue that this hyponastic response is adaptive in the current model (i.e. assuming no other factors play a role)?\nReanalyze the curves describing how photosynthesis rate depends on temperature and how effective leaf area depends on angle and explain your earlier results.\n\n\n\nAnswer\n\nSee answer code ‘03_Environment_and_development_answer.py’ If the 40 degree angle results in slightly more than 3 degrees of cooling, causing internal leaf temperature to be less then 32 degrees instead of the external 35 degrees, final leaf area is larger than for 35 degrees and an angle of 20 degrees. Note that if under high temperature leaf anlge would increase to only 35 degrees, a slightly less than 3 degrees of cooling would already have a positive effect and this positive effect would start already at 32.5 degrees celcius.\nSee answer code 03_Environment_and_development_answer.py No for only 1-2 degrees of cooling final leaf area is smaller under a hyponastic response than if leaf angle were kept at 20 degrees under 35 degrees celcius temperature.\nPhotosynthesis is normalized to 100% at 25 degrees and is 90% at 35 degrees. Lowering leaf temperature with 1-2 degrees by hyponasty recovers this to 95 % while lowering it to 2-3 degrees recovers it to 100%. Still since there is only a 10% loss of photosynthesis to high temperature there is also only a max 10% gain by cooling the leaves. At the same time a hyponastic angle of 40% decreases effective leaf area to 80 percent whereas it is 95% for 20 degrees. So the loss is 15%, which is more than the potential gain.\n\n\n\nExercise 11.4 (Biology -Is hyponasty adaptive?) The cooling down of the leaves through hyponasty is thought to be an important adaptive trait. Interestingly, from the simulations we’ve done so far this adaptive advantage is not very clear. It might therefore very well be that we miss important processes in the current model. In this last question of the practical you will extend the model with additional processes also relevant in plants to see if this may help explain the adaptiveness of hyponasty. This is also an open question in current research, so there is no clear answer and you might actually come up with novel ideas! To help guide your thinking, we have come up with some additional plant processes you can implement and investigate to see how these impact adaptiveness of leaf hyponasty under high temperature. Pick one to work on and investigate if this would render hyponasty more clearly adaptive. If you have extra time, see if you can combinethem. If you have other ideas to work on, this is also great!\n\nHypothesis 1 - Day-Night Rhythm \n\nWe found that during the day, while hyponastic cooling brings photosynthesis closer to its optimum temperature and reduces respiration, this is insufficient to overcompensate the costs of reduced effective leaf area. During the night, cooler leaf temperatures still reduce maintenance respiration, while photosynthesis halts and hence reduction of effective leaf area plays no role, suggesting at night time hyponasty has only advantages. To investigate whether this results in a net adaptive effect of leaf hyponasty incorporate the following processes in your model: - Temperature: Simulate the cooling effect of nighttime. - Photosynthesis: Photosynthesis does not occur without sunlight, so simulate the change of light over the day and its effects. - Respiration: Maintenance respiration persists in absence of photosynthesis. - Hyponasty changes: Leaf angles change over the course of a day, reaching a minimum early during the day and being relatively high during the night. - Growth rate: Counterintuitively in plants often most growth happens during the night, when water loss is minimal and hence turgor pressure is high.\nHint: use sinus functions to describe the daily rhythms and their relative phases. Make sure that light is really zero at night.\na1. Analyze how the absence of photosynthesis at night affects carbon balance. Do you think what you see happening is reasonable? How could you repair this?\na2. Examine the impact of nighttime temperatures on maintenance respiration rates.\na3. Explore how changes in leaf angle influence overall plant growth.\na4. Now combine the two\n\nHypothesis 2 – Stomata\n\nStomata play a critical role in regulating gas exchange and water loss in plants. As leaf temperature increases, stomata open to enhance transpiration, which cools the leaf through evaporative cooling. This mechanism is particularly effective in well-watered plants, where increased stomatal opening can significantly reduce leaf temperature up to around a maximum of 9-10°C. However, at very high temperatures, stomata may close to prevent excessive water loss, which can lead to overheating. Additionally, stomatal opening increases while stomatal closing decreases photosynthesis rates by affecting gas exchange efficiency. Thus, incorporating this dependence of stomatal aperture on temperature may enhance the detrimental effects of high temperature, offering more opportunity for leaf cooling effects of hyponasty to matter.\nTo model this, implement the following changes: - Stomatal opening affects photosynthesis: Include in the photosynthesis function a multiplication factor for stomatal aperture. - Stomatal opening depends on temperatures: Include a function that describes how stomatal aperture first increases with temperature, reaches a maximum at around 25 degrees and then declines for higher temperatures. Aperture should be approximately half the maximum value for temperatures of 10 degrees and 35 degrees. - Stomatal opening increases transpiration: Simulate the cooling effect of transpiration on leaf temperature.\nAnalyze how stomatal opening affects leaf cooling and photosynthesis under moderate temperature conditions. Investigate the trade-offs between cooling benefits and photosynthesis efficiency at very high temperatures. Examine how stomatal closure impacts plant growth and carbon balance under heat stress.\n\n\nAnswer\na1. Carbon levels at night go below zero because while growth depends on carbon levels, respiration continues independent of carbon levels in the current model. However, you can not burn energy you do not have so it is best to make respiration also depend on carbon levels, this is constistent with experimental observations: Seki et al. (2017)\na2. The less temperatures drop during the night, the larger the reduction in final leaf area, yet also the closer the final leaf area achieved with hyponasty becomes to that without. As night time temperature is higher, respiration is higher and leaf cooling becomes more relevant. So we increase the night time advantage of hyponasty.\na3. For leaf angle we can play with how much the minimum and maximum angles are and when during the day these are reached. If leaf angle is allowed to drop from 40 to 14 degrees instead of 20 degrees and the lowest point is reached early in the morning, average day time angle is lower and hence photosynthesis loss is lower and hence break even is reached easier. If maximum leaf angle is lowered to 35 degrees it also helps. So here we basically decrease the day time costs of hyponasty.\na4. If we make leaf angle during the day low and keep night time temperature high we are able to make hyponasty adaptive. The results are a bit shaky in that in the final leaf area versus temperature curve the non hyponasty and hyponasty curves intersect multiple times but beyond 32 degrees with hyponasty is always on top but with small difference.\n\nSee answer code 04_Environment_and_development_Stomata_1.py This is a first code with a Gaussian function for how stomatal aperture depends on temperature that results in stomatal opening being maximal at around 25 degrees. Now slightly less than 2 degrees cooling (on top of the stomatal aperture induced cooling) suffices to make leaf hyponasty advantageous. This is because temperature increase harms photosynthesis now way more (leaf area under no hyponasty decreases to 32 instead of 43% while with hyponasty it decreases to 20 instead of 25%, 43-25=18% 32-20=12%). But this approach is a bit odd: we let stomatal aperture affect effective leaf temperature, and separately (and before this) substract the cooling effect of hyponasty.\n\nSee answer code 04_Environment_and_development_Stomata_2.py Instead of doing a manual extra cooling because of leaf hyponastic angle here we made a function for leaf temperature that takes into account both the stomatal aperture (max cooling effect of 10 degrees for full aperture at 35 degrees) and the leaf angle (max cooling effect of 2 degrees for a 40 degree angle). Now final leaf area decreases to 34% for no hyponasty and to 25% for hyponasty so 34-25=9%. (and looking at the lower temperatures done by further manual substraction is non-sensical now). So, although difference becomes less and less, still hyponasty costs more than it delivers. Also intriguing that if we put both effects into leaf temperature directly we can not get it to work while if we put in the 2 degrees cooling from the angle manually and then the stomatal affect it could work. This is because with the 2 degrees manually, stomatal opening for 33 degrees applies which is 70% whereas without this, stomatal opening for 35 degrees applies which is 60%, so the actual cooling achieved was higher.\nSee 04_Environment_and_development_Stomata_3.py However in reality stomatal aperture does not depend on external temperature but on leaf temperature, so let’s make leaf temperature a variable that affects stomatal aperture and is affected by it. However, we also reduced the stomatal effects on temperature a bit to avoid weird effects: if internal leaf temperature drops too much instead of external temperature 25 being optimal and 35 far below it, we get internal temperatures 17.5 far from optimum and 25 close to optimum, so now under 35 degrees the plants would be doing better. If we make the stomata effect on leaf cooling less strong (factor 0.175 instead of 0.29) internal temperatures are 29 and 20, things are again worse for external temperature of 35, and now for hyponasty situation is only slightly worse and if external temperature is 37.5 with hyponasty plants even perform better. We can further enhance this by boosting angle effect on temperature a bit (factor 0.4 instead of 0.3). If we now additionally assume that stomatal aperture as a function of temperature is usually plotted for external not internal temperature and therefore assume that an external temperature optimum of 24 is approximately an internal leaf temperature of 22 we get a break even for hyponasty at slightly below 35 degrees and plants with hyponasty do actually a bit better at 35 degrees.\nAs optima are around 25 and stomata always provide some cooling actual growth optimum for angle of 20 degrees is around 28.5 degrees and things only really start to go down at 35 degrees, so there is where the big costs are and where hyponasty earns you more by cooling than it costs in terms of effective leaf area. This explains why if you increase further effect of angle at lowering temperature (eg factor 0.8 instead of 0.4), plants do much better at 36/37 degrees than without hyponasty (i.e. at 36/37 degrees difference gets way bigger), but temperature at which plants with hyponasty start to do better hardly shifts. If optimum temperature of stomatal aperture is lowered than temperature at which plant with hyponasty do better becomes lower.\nAlso note it is a bit of a flywheel effect, angle brings more cooling, more cooling brings more stomatal aperture, which brings still more cooling.",
    "crumbs": [
      "IV) Environment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling Thermomorphogenesis in Plants</span>"
    ]
  },
  {
    "objectID": "evdev_practical.html#references",
    "href": "evdev_practical.html#references",
    "title": "11  Modeling Thermomorphogenesis in Plants",
    "section": "11.3 References",
    "text": "11.3 References\nLook at this: hyponasty angle quite high during night, dips early in the day Praat et al. (2024)\nNormal and shade avoidance hyponasty is still large in the dark, and dips around dawn for long day conditions: Michaud et al. (2017)\nFor short day conditions it even peaks during the dark and is lower during day: Oskam et al. (2023) and Dornbusch et al. (2012)\n\n\n\n\nDornbusch, Tino, Séverine Lorrain, Dmitry Kuznetsov, Arnaud Fortier, Robin Liechti, Ioannis Xenarios, and Christian Fankhauser. 2012. “Measuring the Diurnal Pattern of Leaf Hyponasty and Growth in Arabidopsis–a Novel Phenotyping Approach Using Laser Scanning.” Functional Plant Biology 39 (11): 860–69.\n\n\nMichaud, Olivier, Anne-Sophie Fiorucci, Ioannis Xenarios, and Christian Fankhauser. 2017. “Local Auxin Production Underlies a Spatially Restricted Neighbor-Detection Response in Arabidopsis.” Proceedings of the National Academy of Sciences 114 (28): 7444–49.\n\n\nOskam, Lisa, Basten L Snoek, Chrysoula K Pantazopoulou, Hans van Veen, Sanne EA Matton, Rens Dijkhuizen, and Ronald Pierik. 2023. “A Low-Cost and Open-Source Imaging Platform Reveals Spatiotemporal Insight into Arabidopsis Leaf Elongation and Movement.” BioRxiv, 2023–08.\n\n\nPraat, Myrthe, Zhang Jiang, Joe Earle, Sjef Smeekens, and Martijn van Zanten. 2024. “Using a Thermal Gradient Table to Study Plant Temperature Signalling and Response Across a Temperature Spectrum.” Plant Methods 20 (1): 114.\n\n\nSeki, Motohide, Takayuki Ohara, Timothy J Hearn, Alexander Frank, Viviane CH Da Silva, Camila Caldana, Alex AR Webb, and Akiko Satake. 2017. “Adjustment of the Arabidopsis Circadian Oscillator by Sugar Signalling Dictates the Regulation of Starch Metabolism.” Scientific Reports 7 (1): 8305.",
    "crumbs": [
      "IV) Environment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling Thermomorphogenesis in Plants</span>"
    ]
  },
  {
    "objectID": "evo_intro_text.html",
    "href": "evo_intro_text.html",
    "title": "12  Introduction to evolution",
    "section": "",
    "text": "12.1 Evolution: Life’s most clever algorithm\nEvolution is the process by which populations change over generations through variation, inheritance, and differential survival. This idea, famously championed by Darwin and Wallace, explains the diversity of life on Earth. It describes how species adapt to their environments, how new species arise, and how complex traits evolve. Today, the concept of evolution has expanded beyond biology, it’s recognised as a powerful algorithm that drives adaptation in systems ranging from bacteria (genes) to ideas (memes), from DNA (nucleotides) to computer code (bits).\nIn this part of the course, we’ll bring these ingredients to life by writing our own simulations and watching evolution unfold on the screen. And while our digital creatures aren’t made of flesh and blood, the evolutionary battles they fight, the strategies they discover, and the adaptations they evolve are as real, and often as surprising, as anything found in nature itself.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to evolution</span>"
    ]
  },
  {
    "objectID": "evo_intro_text.html#three-ingredients",
    "href": "evo_intro_text.html#three-ingredients",
    "title": "12  Introduction to evolution",
    "section": "12.2 Three ingredients",
    "text": "12.2 Three ingredients\nAs briefly mentioned above, we just need three ingredients to have evolution by means of natural selection:\n\nvariation (differences between individuals),\ninheritance (the passing on of traits),\nselection (some variants performing better than others).\n\nThe last ingredient is self-evident. Evolution by means of natural selection requires selection. It is especially the first two that are a little more tricky to really understand, as they are not always as obvious as they seem.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to evolution</span>"
    ]
  },
  {
    "objectID": "evo_intro_text.html#balancing-change-and-stability",
    "href": "evo_intro_text.html#balancing-change-and-stability",
    "title": "12  Introduction to evolution",
    "section": "12.3 Balancing change and stability",
    "text": "12.3 Balancing change and stability\nTo evolve, a system needs enough variation – if everyone is the same, there’s nothing for selection to act on. But this variation can’t just be noise; it needs to be passed on. That means inheritance can’t be perfect – there must be room for change, such as through mutations – but it also can’t be too sloppy. If traits aren’t reliably transmitted to the next generation, then even the best adaptations will vanish before they can take hold. Evolution lives in the sweet spot: not too rigid, not too chaotic, just enough memory and just enough change. To make this a little more tangible, let us make our very first simulation.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to evolution</span>"
    ]
  },
  {
    "objectID": "evo_intro_text.html#a-simple-evolutionary-algorithm",
    "href": "evo_intro_text.html#a-simple-evolutionary-algorithm",
    "title": "12  Introduction to evolution",
    "section": "12.4 A simple evolutionary algorithm",
    "text": "12.4 A simple evolutionary algorithm\nOne simple way to simulate evolution is with a Moran process, a classic model from population genetics. Imagine a population of 100 individuals, each with a single gene that determines its fitness. This gene can have all values from 0 to 1 (let’s call this value \\(\\phi\\)). At each time step, one individual is chosen to reproduce with a probability proportional to \\(\\phi\\), producing 1 offspring. This offspring inherits their parents gene (so the same \\(\\phi\\)), but with a probability \\(\\mu\\), the value changes by a small amount (a mutation). The population size will now be 101, which could be interesting if we want to study population growth. However, in a Moran process we keep it simple: one random individual is removed by the new offspring, so the population size is constant while still allowing fitter individuals to spread over time.\nHere’s a minimal Python example:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(5)\n\nN = 100 # Population size \nfitnesses = np.full(N, 0.05)\nmu = 0.01\n# Updated parameters\nsteps = 50000\navg_fitness = []\n\n# Moran process with mutation (logging every 10 steps)\nfor step in range(steps):\n    probs = fitnesses / fitnesses.sum()\n    parent = np.random.choice(N, p=probs)\n    dead = np.random.choice(N)\n\n    # Copy with mutation\n    new_fit = fitnesses[parent]\n    if np.random.rand() &lt; mu:\n        new_fit = np.clip(new_fit + np.random.normal(0, 0.1), 0, 1)\n            \n    fitnesses[dead] = new_fit\n\n    # Save average fitness every 10 steps\n    if step % 10 == 0:\n        avg_fitness.append(fitnesses.mean())\n\n# Plotting\nplt.plot(np.arange(0, steps, 10), avg_fitness)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Average fitness\")\nplt.title(\"Evolution of Fitness in a Moran Process\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nExercise 12.1 (Moran process simulation) \nStudy the Python code for the evolutionary algorithm given above. Answer the following questions:\n\nHow “well adapted” is the initial population?\nHow are mutations implemented in the code? Can you think of other ways?\nCan the parent be replaced by its own offspring? Why/why not?\nTry to decreasing/increase value of \\(\\mu\\) (mutation rate). Which values makes evolution go faster? Which values make evolution more precise?\n\n\n\nAnswer  a. Looking at the output or at the code, we see that the initial population has a “fitness” of 0.05.  b. Mutations are implemented by adding a normally distributed random number (mean 0, std 0.1) to the parent’s fitness value with a probability of mu. This comes down to sampling from a normal distribution with the mean set to the parent’s value. Other ways could include using a uniform distribution for mutations, making every jump equally likely.  c. Yes, the parent can be replaced by its own offspring if the random individual chosen to die happens to be the parent. This is possible because the death selection is random and independent of fitness.  d. Increasing mutation rates makes it easier to find fitter mutants when the population is not (yet) adapted, so evolution goes faster. However, it is also less “precise”, as the average of the population will fluctuate well below the optimal fitness of 1.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to evolution</span>"
    ]
  },
  {
    "objectID": "evo_intro_text.html#what-this-part-of-the-course-is-about",
    "href": "evo_intro_text.html#what-this-part-of-the-course-is-about",
    "title": "12  Introduction to evolution",
    "section": "12.5 What this part of the course is about",
    "text": "12.5 What this part of the course is about\nThe above simulation is fun, but not really… biologically relevant. While some simplifications are necessary to make models feasible, we will investigate a few evolutionary models that are somewhat more interesting. We will discuss how to model spatial structure and local competition, how genotypes (where mutations happen) get translated into phenotypes (where selection happens), and how the environment can change over time and lead to niche construction and interactions.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to evolution</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html",
    "href": "evo_practical_1.html",
    "title": "13  Sticking together",
    "section": "",
    "text": "Sticking together\nIn this practical, you will practice building your own model of collective behaviour, based on the one you saw at the end of the lecture:\nThe example above is a implemented in Javascript, a programming language that is widely used for web development. It is easy to share with others, interactive, and surprisingly fast. But, it’s not the most “professional” programming language. Plus, at this stage of the course there is no point in learning yet another programming language, as you are here to learn about modelling biology. So we will stick to Python.\nFirst, let’s discuss how we can let individuals walk around in space.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-steering",
    "href": "evo_practical_1.html#sec-steering",
    "title": "13  Sticking together",
    "section": "13.1 Steering",
    "text": "13.1 Steering\nWe can represent a moving individual in space as a point with a position and a velocity. The position is represented by two coordinates, \\(x\\) and \\(y\\), and the velocity is represented by two components, \\(v_x\\) and \\(v_y\\). All movement that this individual can do, will be a matter of repeatedly updating its position based on their velocity:\n\n\n\n  \n  \n  Vector Visualisation\n  \n\n\n  \n\n  \n    ← \n    → \n    ↑ \n    ↓ \n    ⟲ \n    ⟳ \n  \n\n  \n\n\n\n\n To model such a vector in python, we can simply define a base point with an x- and y-coordinate, and a velocity vector with an x- and y-component. The position of the individual can then be updated by adding the velocity to the position. Combining that with a function that draws an arrow in Python, we get the following code:\n\n\n\n\n\n\nCODE FOR “moving vector in Python”\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Enable interactive mode for matplotlib\nplt.ion()\n\n# Setup figure and axis for plotting the arrow\nfig, ax = plt.subplots(figsize=(8, 4))\nax.set_xlim(0, 600)  # x-axis limits\nax.set_ylim(0, 250)  # y-axis limits\nax.set_aspect('equal')  # Keep aspect ratio square\nax.set_facecolor('#f0f0f0')  # Background color\nax.set_title(\"A moving vector with an arrowhead\")  # Title\n\n# Initial position and velocity\nx, y = 250.0, 180.0      # Position coordinates\nvx, vy = 5.0, 10.5        # Velocity components\n\n\ndef draw_arrow(x, y, vx, vy):\n    \"\"\"\n    Draws an arrow at position (x, y) with velocity (vx, vy).\n    \"\"\"\n    ax.clear()\n    ax.set_xlim(0, 600)\n    ax.set_ylim(0, 250)\n    ax.set_aspect('equal')\n    ax.set_facecolor('#f0f0f0')\n    ax.set_title(\"A moving vector with an arrowhead\")\n\n    # Normalize velocity for drawing the arrow\n    \n    dx = vx*5\n    dy = vy*5\n\n    # Arrow shaft\n    end_x = x + dx\n    end_y = y + dy\n\n    # Arrowhead calculation\n    angle = np.arctan2(dy, dx)\n    angle_offset = np.pi / 7\n    hx1_x = end_x - np.cos(angle - angle_offset)\n    hx1_y = end_y - np.sin(angle - angle_offset)\n    hx2_x = end_x - np.cos(angle + angle_offset)\n    hx2_y = end_y - np.sin(angle + angle_offset)\n\n    # Draw shaft\n    ax.quiver(x, y, dx, dy, angles='xy', scale_units='xy', scale=1, color='#007acc', width=0.005)\n    # Draw base point\n    ax.plot(x, y, 'o', color='#333')\n\n    # Labels\n    ax.text(x+10, y+10, f\"x = {x:.2f}\")\n    ax.text(x+10, y-5, f\"y = {y:.2f}\")\n    ax.text(end_x + 10, end_y - 20, f\"vₓ = {vx:.2f}\")\n    ax.text(end_x + 10, end_y, f\"vᵧ = {vy:.2f}\")\n\n    plt.draw()\n    plt.pause(0.03)\n\n# Animation loop: update position by velocity\nfor i in range(500):\n    x += vx*0.1  # Update x position\n    y += vy*0.1  # Update y position\n\n    # Wrap around edges\n    x %= 600\n    y %= 250\n    \n    draw_arrow(x, y, vx, vy)\n\nplt.ioff()\n\n\n\n\nExercise 13.1 (Playing with steering arrows - Mathematical thinking) \nCopy-paste the code above, study it for a few minutes, and run it.\n\nWhat can you do to make the arrow accelerate?\n\nTo rotate a vector, we can use the following trigonometrical equations, where \\(\\theta\\) is the angle of rotation:\n\\[\n\\begin{aligned}\nx_{new} = x \\cdot cos(\\theta) - y \\cdot sin(\\theta) \\newline\ny_{new} = x \\cdot sin(\\theta) + y \\cdot cos(\\theta)\n\\end{aligned}\n\\] b. Use the equation above to rotate the velocity vector in the code by a small angle every timestep. What happens? c. Modelling 1 individual is not very exciting. Think about what the code above would look like if you had more than 1 individual. Discuss this with other students and/or Bram.\n\n\nAnswers a. For this, you can simply multiply the velocity components every timestep by a value &gt; 1. For example:\n# Accellerate the velocity\nvx *= 1.01\nvy *= 1.01\nNote that this gets out of hand quite quickly. b. To do this, we need to store the new values in a temporary variable, and then assign them to the original variables. This is necessary because while we first modify vx, we still want to use the ‘old’ value of vx to calculate the new vy. The following code rotates the velocity vector by 0.05 radians:\n# Rotate the velocity vector by a small angle \nvxnew = vx * np.cos(0.05) - vy * np.sin(0.05)\nvynew = vx * np.sin(0.05) + vy * np.cos(0.05)\nvx, vy = vxnew, vynew\n\n\nIf you run this code, you will see that the dot will go in circles.\n\n\n\nWhile the code above stores x, y, vx, and vy as a ‘global variables’, this is not good if we have many individuals. Instead, we can store the state of each individual in a list, dictionary, or a class. For example, we make a class ‘cell’ and store 100 of these ‘cells’ in a large list:\n\nclass Cell:\n    def __init__(self, x, y, vx, vy):\n        self.x = x\n        self.y = y\n        self.vx = vx\n        self.vy = vy\n# Create 100 cells\ncells = [Cell(np.random.rand(), np.random.rand(), 0.0, 0.0) for _ in range(100)]",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#moving-cells",
    "href": "evo_practical_1.html#moving-cells",
    "title": "13  Sticking together",
    "section": "Moving “cells”",
    "text": "Moving “cells”\nIn this practical, you will practice with modelling individuals in space by modifying a Python code based on the foraging cells shown at the beginning. To accommodate for many cells, we will define a new Cell class, embedded in a Simulation class.1\nFirst, read the code yourself (you can ignore the Visualisation class), and see if you can get it running on your own laptop.\n\n\n\n\n\n\nSTARTING CODE FOR “moving cells”\n\n\n\n\n\n###\n# PRACTICAL 1 | \"Every cell for themselves?\"\n# This is the starting code. Follow the instructions in the practical to complete the code. \n# If you get stuck, you can look at the final code in `foraging_for_resources_final.py`, or ask\n# Bram. \n#\n# The structure of this code is as follows:\n# 1. Imports and parameters\n# 2. Simulation class\n# 3. Cell class\n# 4. Visualisation class (you do not need to change this)\n#\n###\n\n# 1. IMPORTS AND PARAMETERS\n# Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider\n\n# Parameters for simulation\nWORLD_SIZE = 200    # Width / height of the world (size of grid and possible coordinates for cells)\nMAX_VELOCITY = 0.3  # Maximum velocity magnitude\nMAX_FORCE = 0.3     # Maximum force magnitude\nRANDOM_MOVEMENT  = 0.01 # Random movement factor to add some noise to the cell's movement\n\n# Parameters for display\nDRAW_ARROW = True  # Draw the arrows showing the velocity direction of the cells\nINIT_CELLS = 20 # Initial number of cells in the simulation\nDISPLAY_INTERVAL = 1 # Frequency with which the plot is updated (e.g., every 10 timesteps can speed things up)\n\n# 1. MAIN LOOP (using functions and classes defined below)\ndef main():\n    \"\"\"Main function to set up and run the simulation.\"\"\"\n    # NOTE: The `Visualisation` class is responsible for managing the visualization \n    # of the simulation, including creating plots, updating them, and handling \n    # user interactions like the slider. As this has nothing to do with modeling\n    # per se, understanding this code is not necessary, but it can be fun to look\n    # at if you are interested. \n    \n    num_cells = INIT_CELLS\n    sim = Simulation(num_cells) \n\n    plt.ion()\n    vis = Visualisation(sim)\n\n    def update_cells(val):\n        sim.initialise_cells(int(vis.slider.val))\n        vis.redraw_plot(sim)\n        \n    # Connect the slider to the update function\n    vis.slider.on_changed(update_cells)\n\n    # Run simulation\n    for t in range(1, 10000):\n        \n        sim.simulate_step()\n        \n        if(t % DISPLAY_INTERVAL == 0):\n            # As long as only cells move, update only positions and timestamp\n            vis.update_plot(sim) \n            vis.ax.set_title(f\"Timestep: {t}\")\n            vis.fig.canvas.draw_idle()\n            plt.pause(10e-20)        \n        if(sim.redraw):\n            # When more has changes (e.g. number of cells or target position), redraw the plot\n            vis.redraw_plot(sim) \n            sim.redraw = False # Make sure it doesn't keep redrawing if not necessary\n        \n\n    # Keep the final plot open\n    plt.ioff()\n    # plt.show()\n\n\n\n# 2. SIMULATION CLASS\nclass Simulation:\n    \"\"\"Manages the grid, cells, target, and simulation logic.\"\"\"\n    def __init__(self, num_cells):\n        # Initialise a grid for the simulation\n        self.grid = np.zeros((WORLD_SIZE, WORLD_SIZE))  # Initialise an empty grid\n        self.fill_grid(self.grid, 0, 0, 0, 0)           # Fill grid with values (currently just 1s)\n        # Initialise a population of cells\n        self.cells = []\n        self.initialise_cells(num_cells)\n        # Place a 'target' in the middle\n        self.target_position = [WORLD_SIZE/2, WORLD_SIZE/2] \n        # A flag to only rebuild the plot when necessary (e.g. when the number of cells changes)\n        self.redraw = False\n\n    def simulate_step(self):\n        \"\"\"Simulate one timestep of the simulation.\"\"\"\n        for cell in self.cells:\n            # Actions taken by each cell. Most of them are still undefined, so you can implement them yourself.\n            self.move_towards_dot(cell)  \n            if self.check_target_reached(cell):\n                print(f\"Target reached!\")\n                self.reproduce_cell(cell)\n                self.redraw = True\n            \n            #self.avoid_collision(cell)\n            #self.stick_to_close(cell)\n            #self.find_peak(cell)\n\n            # Apply forces and update position\n            cell.apply_forces()\n            cell.update_position()\n\n            # Limit velocity to the maximum allowed\n            cell.vx = np.clip(cell.vx, -MAX_VELOCITY, MAX_VELOCITY)\n            cell.vy = np.clip(cell.vy, -MAX_VELOCITY, MAX_VELOCITY)\n\n    def initialise_cells(self, num_cells):\n        \"\"\"Initialise the cells with random positions and velocities.\"\"\"\n        self.cells = []\n        for _ in range(num_cells):\n            x = np.random.uniform(0, WORLD_SIZE)\n            y = np.random.uniform(0, WORLD_SIZE)\n            vx = np.random.uniform(-1, 1)\n            vy = np.random.uniform(-1, 1)\n            self.cells.append(Cell(x, y, vx, vy))\n\n    def fill_grid(self, grid, mean_x, mean_y, std_dev, noise=0):\n        \"\"\"\n        Write a function that takes the 2D grid and fills it with values representing \n        a Gaussian (normal) distribution centered at (mean_x, mean_y). See\n        if you can use the 'noise' argument to randomise the gaussian distribution a bit.\n        \n        Hint: e^{-x^2} yields a bell curve centered around 0. \n        \n        \"\"\"\n        for i in range(WORLD_SIZE):\n            for j in range(WORLD_SIZE):\n                x = i / (WORLD_SIZE - 1)\n                y = j / (WORLD_SIZE - 1)\n                grid[i, j] = 1 # This is 1 in the example, but should be a Gaussian distribution\n\n        # Normalize the grid to keep the total resource concentration the same\n        self.grid = grid\n    \n    def find_peak(self, cell):\n        \"\"\"Make the cell move towards the peak of the resource gradient with a random walk.\"\"\"\n        # Convert cell position to grid indices, as well as the previous position\n        grid_x = int(cell.x) % WORLD_SIZE\n        grid_y = int(cell.y) % WORLD_SIZE\n        next_x = (int(cell.x + 30*cell.vx) + WORLD_SIZE) % WORLD_SIZE \n        next_y = (int(cell.y + 30*cell.vy) + WORLD_SIZE) % WORLD_SIZE \n         \n    \n    def avoid_collision(self, cell):\n        \"\"\"Implement a simple collision avoidance mechanism. You can do so by\n        checking if this individual overlaps with another individual, and if so,\n        applying a repulsion force to the individual apposing the overlapping\n        direction.\"\"\"\n        for other_cell in self.cells:\n            if other_cell is not cell:\n                # Calculate the distance between the two cells\n                dx = cell.x - other_cell.x\n                dy = cell.y - other_cell.y\n                distance = np.sqrt(dx**2 + dy**2)\n                \n                    \n    def stick_to_close(self, cell):\n        \"\"\"Implement an attraction to cells that are nearby (but not overlapping)\"\"\"\n        for other_cell in self.cells:\n            if other_cell is not cell:\n                # Calculate the distance between the two cells\n                dx = cell.x - other_cell.x\n                dy = cell.y - other_cell.y\n                distance = np.sqrt(dx**2 + dy**2)\n\n    \n    def move_towards_dot(self, cell):\n        \"\"\"\n        Write your own function that applies forces in the direction of the dot.\n        Try to think of a way to apply the same force to every cell irrespective\n        of the distance to the dot, such that the cells move towards the dot at \n        the same speed. \n        \n        To get you started, the function already calculates dx and dy, which are\n        the distances to the target position in the x and y direction, respectively.\n        \"\"\"\n        # Calculate dx and dy\n        dx = self.target_position[0] - cell.x\n        dy = self.target_position[1] - cell.y\n        \n    \n    def check_target_reached(self, cell):\n        \"\"\"\n        Write your own function that checks if this cell has reached the target position.\n        You can do this by calculating the distance between the cell and the target.\n        If the distance is smaller than a certain threshold (e.g., 3 units), return True.\n        Otherwise, return False.\n        \"\"\"\n        \n        return(False)  # Dummy 'return' value. \n    \n    def reproduce_cell(self, cell):\n        \"\"\"\n        Write your own function that reproduces this cell. Think\n        about what it should inherit, and what it should *not* inherit. \n        \n        To keep the number of cell constant, you can first throw away a random cell.\n        \"\"\"\n        # Reproduce: Create a new cell with the same properties as the current cell\n        return(False) # Dummy 'return' value.\n\n        \n        \n# 3. CELL CLASS\nclass Cell:\n    \"\"\"Represents an individual cell in the simulation.\"\"\"\n    def __init__(self, x, y, vx, vy):\n        self.x = x\n        self.y = y\n        self.vx = vx\n        self.vy = vy\n        self.ax = 0\n        self.ay = 0\n        self.stickiness = 0.01 # Initial stickiness, can be adjusted later\n        \n    def update_position(self):\n        \"\"\"Update the cell's position based on its velocity.\"\"\"\n        self.x = (self.x + self.vx ) % WORLD_SIZE  # Wrap around the world\n        self.y = (self.y + self.vy ) % WORLD_SIZE  # Wrap around the world\n\n    def apply_forces(self):\n        \"\"\"Apply a force to the cell, updating its velocity.\"\"\"\n        self.ax = np.clip(self.ax, -MAX_FORCE, MAX_FORCE)\n        self.ay = np.clip(self.ay, -MAX_FORCE, MAX_FORCE)\n        self.vx += self.ax + RANDOM_MOVEMENT * np.random.uniform(-1, 1)\n        self.vy += self.ay + RANDOM_MOVEMENT * np.random.uniform(-1, 1)\n        # Apply drag to slow down the cell naturally\n        self.ax = 0\n        self.ay = 0\n        \n\n\n# Visualisation class for showing the individuals and the grid. For the practical, you do not need to change this. \nclass Visualisation:    \n    def __init__(self, sim):\n        fig, ax = plt.subplots(figsize=(6, 6))\n        self.cell_x = [cell.x for cell in sim.cells]\n        self.cell_y = [cell.y for cell in sim.cells]\n        self.cell_vx = np.array([cell.vx for cell in sim.cells])\n        self.cell_vy = np.array([cell.vy for cell in sim.cells])\n        self.cell_stickiness = np.array([cell.stickiness for cell in sim.cells])\n        # Colour cells by stickiness using inferno colormap\n        self.cell_scatter = ax.scatter(self.cell_x, self.cell_y, c=self.cell_stickiness, cmap='inferno', s=50, edgecolor='white', vmin=0, vmax=1)\n        if(DRAW_ARROW): self.cell_quiver = ax.quiver(self.cell_x, self.cell_y, self.cell_vx * 0.5, self.cell_vy * 0.5, angles='xy', scale_units='xy', scale=0.02, color='white')\n        plt.subplots_adjust(bottom=0.2)\n\n        ax.set_xlim(0, WORLD_SIZE)\n        ax.set_ylim(0, WORLD_SIZE)\n        ax.set_aspect('equal', adjustable='box')\n        ax.set_title(f\"Timestep: 0\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n\n        target_point=ax.scatter(sim.target_position[0], sim.target_position[1], c='purple', s=100, edgecolor='red')\n        grid_im=ax.imshow(sim.grid.T, extent=(0, WORLD_SIZE, 0, WORLD_SIZE), origin='lower', cmap='viridis', alpha=1.0)\n\n        self.fig = fig\n        self.ax = ax\n        self.target_point = target_point\n        self.grid_im = grid_im\n\n        # Add a slider for selecting the number of cells\n        ax_slider = plt.axes([0.2, 0.05, 0.6, 0.03])\n        self.slider = Slider(ax_slider, 'Cells', 1, 1000, valinit=len(sim.cells), valstep=1)\n\n    def update_cell_positions(self, sim):\n        \"\"\"Update the positions of the cells in the visualisation.\"\"\"\n        self.cell_x = [cell.x for cell in sim.cells]\n        self.cell_y = [cell.y for cell in sim.cells]\n        self.cell_vx = np.array([cell.vx for cell in sim.cells])\n        self.cell_vy = np.array([cell.vy for cell in sim.cells])\n        self.cell_stickiness = np.array([cell.stickiness for cell in sim.cells])\n    \n    def update_plot(self, sim):\n        self.update_cell_positions(sim)\n        self.cell_scatter.set_offsets(np.c_[self.cell_x,self.cell_y])\n        self.cell_scatter.set_array(self.cell_stickiness)\n        if(DRAW_ARROW): \n            self.cell_quiver.set_offsets(np.c_[self.cell_x, self.cell_y])\n            self.cell_quiver.set_UVC(self.cell_vx * 0.5, self.cell_vy * 0.5)        \n\n    def redraw_plot(self, sim):\n        self.update_cell_positions(sim)\n        cell_scatter_new = self.ax.scatter(self.cell_x, self.cell_y, c=self.cell_stickiness, cmap='inferno', s=50, edgecolor='white', vmin=0, vmax=1)\n        if(DRAW_ARROW): \n            cell_quiver_new = self.ax.quiver(self.cell_x, self.cell_y, self.cell_vx * 0.15, self.cell_vy * 0.15, angles='xy', scale_units='xy', scale=0.02, color='white')\n            self.cell_quiver.remove()\n            self.cell_quiver = cell_quiver_new\n        self.cell_scatter.remove()\n        self.fig.canvas.draw_idle()\n        self.cell_scatter = cell_scatter_new\n        self.grid_im.remove()\n        self.grid_im = self.ax.imshow(sim.grid.T, extent=(0, WORLD_SIZE, 0, WORLD_SIZE), origin='lower', cmap='viridis', alpha=1.0)\n        self.target_point.remove()\n        self.target_point=self.ax.scatter(sim.target_position[0], sim.target_position[1], c='purple', s=100, edgecolor='red')\n        plt.pause(10e-20)\n            \n            \n# 4. Execute the main loop\nif __name__ == \"__main__\":\n    # with cProfile.Profile() as pr:\n        main()\n        # pr.print_stats()\n\n\n\n\n\nMake sure you inspect the code. What features does the Simulation class have? What features does a Cell have?\nAs you can see if you inspected the code properly, many functions are left empty (or at least do not do anything yet). You will start filling these with your own code.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-movingtarget",
    "href": "evo_practical_1.html#sec-movingtarget",
    "title": "13  Sticking together",
    "section": "13.2 Moving the target",
    "text": "13.2 Moving the target\nIf you run the code, you will see a purple dot (with a red outline). This may represent a “target”. It could represent a resource patch for bacteria, but it could also be a piece of fruit for a monkey (at this point, the model is still very abstract, so both could be true). Let’s make the target change position around after an individual touches it.\n\nExercise 13.2 (Playing with steering arrows - Algorithmic/mathematical thinking) To help you on your way, first answer the following questions for yourself:\n\nHow can you calculate the distance between an individual and the orange dot?\nWhen is an individual close enough to the orange dot?\nHow can we assign a random position to the dot?\n\n\nA working code to steer the individuals towards the target is shown below. Note that the acceleration that is applied is only small, otherwise the individuals will move in a straight line towards the target very rapidly.\nFirst try it yourself. If you get stuck, ask Bram for help.\n\nAnswer\ndef move_towards_dot(self, cell):\n    \"\"\"Apply forces in the direction of the dot.\"\"\"\n    # Calculate dx and dy\n    dx = self.target_position[0] - cell.x\n    dy = self.target_position[1] - cell.y\n    # Calculate the distance to the target (Pythagorean theorem)\n    distance = np.sqrt(dx**2 + dy**2)\n    \n    # Normalize dx and dy \n    dx /= distance\n    dy /= distance\n    # Apply a small force towards the target\n    cell.ax += dx * 0.01\n    cell.ay += dy * 0.01",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-reproduction",
    "href": "evo_practical_1.html#sec-reproduction",
    "title": "13  Sticking together",
    "section": "13.3 Reproduction",
    "text": "13.3 Reproduction\nLet’s reward the individual that found the target. To do this, we can call the ‘Cell’ constructor to make a new cell, and add it to the list of cells:\nnew_cell = Cell(new_x, new_y, new_vx, new_vy, new_speed)\nself.cells.append(new_cell)\n\nExercise 13.3 (The birth of an arrow - Biological / algorithmic thinking) Consider which properties of the parent cell get inherited to the child:\n\nShould the exact position be inherited to offspring? (yes/no)\nShould the offspring be placed nearby its parent? (yes/no)\nDoes the velocity get inherited?\n\n\n\nAnswer A working code to reproduce a cell is shown below. Note that I decided to first throw an individual away, and then add the newborn individual (instead of the other way around). This ensures the newborn cannot be immediately thrown away, which would be a rather pointless event.\ndef reproduce_cell(self, cell):\n       # Reproduce: Create a new cell with the same properties as the current cell\n       angle = np.random.uniform(0, 2 * np.pi)\n       radius = np.random.uniform(0.05, 1.5)\n       new_x = cell.x + radius * np.cos(angle)\n       new_y = cell.y + radius * np.sin(angle)\n       new_cell = Cell(new_x, new_y, cell.vx, cell.vy)\n       random_cell = np.random.choice(self.cells)   \n       self.cells.remove(random_cell)\n       self.cells.append(new_cell)\n\nNote that depending on the scenario, the above questions may change. When a planktonic algea reproduces the daughter cells may inherit the velocity of the mother cell, but if a monkey gives birth, it does not make a lot of sense to talk about the ‘velocity’ of the mother. If we consider plants, we should not even consider velocity of the individuals at any stage of their life.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-collision",
    "href": "evo_practical_1.html#sec-collision",
    "title": "13  Sticking together",
    "section": "13.4 Collision detection",
    "text": "13.4 Collision detection\nAs the cells move towards the dot, you may notice that cells start overlapping quite a bit. Let’s implement a simple form of collision detection, where overlap is resolved by pushing cells away from each other. Answer the following questions to get on your way:\n\nExercise 13.4 (Overlapping circles - Algorithmic/mathematical thinking)  \n\nHow can you calculate the distance between two cells?\nWhen are two points overlapping?\nWhat can we do when two points overlap?\n\n\n\nAnswer A working code for cell-cell collision is shown below. Note that we apply a large force as we want this force to be able to overpower other forces that make the cells overlap.\ndef avoid_collision(self, cell):\n    \"\"\"Avoidance forces to prevent cells from colliding.\"\"\"\n    for other_cell in self.cells:\n        if other_cell is not cell:\n            # Calculate the distance between the two cells\n            dx = cell.x - other_cell.x\n            dy = cell.y - other_cell.y\n            distance = np.sqrt(dx**2 + dy**2)\n\n            # If the cells are too close, apply a repulsion force\n            if distance &lt; 5.0 and distance &gt; 0:  # Threshold for \"too close\"\n                # repulsion force proportional to the inverse of the distance\n                force_magnitude = (5.0 - distance) / distance\n                cell.ax += force_magnitude * dx * 100\n                cell.ay += force_magnitude * dy * 100\n\nNow that we have implemented both target-finding, reproduction, and collision, we can study these individual mechanisms by commenting one or the other out. This is an important process in understanding a model. Try it for yourself!",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-resourcepeak",
    "href": "evo_practical_1.html#sec-resourcepeak",
    "title": "13  Sticking together",
    "section": "13.5 Implementing a resource peak",
    "text": "13.5 Implementing a resource peak\nAs you may have noticed when reading the code, it also includes a grid. However, you don’t see this grid yet, as the function fill_grid currently sets every point to 1.\nWe can loop over the grid coordinates by using a double for-loop:\nfor i in range(WORLD_SIZE):\n  for j in range(WORLD_SIZE):\n    grid[i,j] = 1\nThe above function loops over all the grid points, and set the value of each grid point to 1. Let’s use the function np.exp to calculate a Gaussian that we will place in the center of the grid. The fill_grid already takes as arguments the grid, a relative x-coordinate (0-1), a relative y-coordinate (0-1), and a standard deviation (0-1). Get started by answering the question below.\n\nExercise 13.5 (Bell curves in space - Mathematical thinking)  \n\nThe function \\(e^{-x^2}\\) gives a bell-curve centered around zero. How can you make it centered around a different value?\nCombined with the numpy function np.exp, how can we use the equation in question a to create a Gaussian that is at the center of the grid?\n\n\n\nAnswer A working code to implement a resource peak (with optional noise set to 0 by default) is shown below:\ndef fill_grid(self, grid, mean_x, mean_y, std_dev, noise=0):\n       \"\"\"Creates a Gaussian distribution with noise on the grid.\"\"\"\n       for i in range(WORLD_SIZE):\n           for j in range(WORLD_SIZE):\n               x = i / (WORLD_SIZE - 1)\n               y = j / (WORLD_SIZE - 1)\n               distance_squared = (x - mean_x)**2 + (y - mean_y)**2\n               grid[i, j] = np.exp(-distance_squared / (2 * std_dev**2)) * np.random.uniform(0.0, 1.0)**noise\n\n       # Normalize the grid to keep the total resource concentration the same\n       grid /= np.sum(grid)\n      self.grid = grid",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-runandtumble",
    "href": "evo_practical_1.html#sec-runandtumble",
    "title": "13  Sticking together",
    "section": "13.6 Run and tumble",
    "text": "13.6 Run and tumble\n\n\n\n“Run and tumble”\n\n\nBacterial cells are so small, that they cannot detect a gradient directly (in other words, they don’t know in which direction resources are higher!). Instead, bacteria often use a “run and tumble” strategy. When they are currently not detecting an increase in the concentration (over time), they tumble. If they do detect an increase, they keep moving in the same direction. This is a very simple strategy, but it can be very effective mechanism for chemotaxis.\nIn our earlier code of a single, moving vector, we rotated the arrow by changing the ‘angle’ variable. However, these cells do not have an angle parameters, but only a velocity vector with components \\(v_x\\) and \\(v_y\\). If we want to rotate the velocity vector, we can use the following equation (rooted in basic trigonometry):\n\\[\nv_x' = v_x \\cdot cos(\\theta) - v_y \\cdot sin(\\theta) \\\\\nv_y' = v_x \\cdot sin(\\theta) + v_y \\cdot cos(\\theta)\n\\]\nWhere \\(\\theta\\) is the angle we want the vector to rotate (in radians, not degrees!), and \\(v_x\\) and \\(v_y\\) are the components of the vector. With this in mind, let’s try and model chemotaxis.\n\nExercise 13.6 (Run and tumble - Biology / algorithmic thinking)  \n\nDetermine the concentration at the position of the cell, AND the predicted position of the cell after a small timestep (hint: use \\(v_x\\) and \\(v_y\\) to predict the future position! ask Bram if you get stuck)\nMake sure the future position is not outside of the grid! (hint: use Google, ChatGPT, or Copilot and figure out how the “modulo” operator works)\nIf the future position has a higher concentration than the current position, keep moving in more or less the same direction, with a very small change.\nIf the future position is a lower concentration, rotate the velocity vector a lot.\n\n\n\nAnswer A working code to implement a run-and-tumble mechanism shown below:\ndef find_peak(self, cell):\n       # Convert cell position to grid indices, as well as the previous position\n       grid_x = int(cell.x) % WORLD_SIZE\n       grid_y = int(cell.y) % WORLD_SIZE\n       next_x = (int(cell.x + 10*cell.vx) + WORLD_SIZE) % WORLD_SIZE \n       next_y = (int(cell.y + 10*cell.vy) + WORLD_SIZE) % WORLD_SIZE \n       # Get the resource value at cell's position, as well as the next position\n       resource_value = self.grid[grid_x, grid_y]\n       resource_next = self.grid[next_x, next_y]\n       \n       # Check if the cell is moving in the right direction\n       if resource_next &gt; resource_value:\n           # Moving in the right direction: small random adjustment\n           angle = np.random.uniform(-0.1, 0.1)  # Small angle change\n       else:\n           # Moving in the wrong direction: large random adjustment\n           angle = np.random.uniform(-np.pi*1.0, np.pi*1.0)  # Large angle change\n       \n       # Rotate the velocity vector by the angle calculated\n       new_vx = cell.vx * np.cos(angle) - cell.vy * np.sin(angle)\n       new_vy = cell.vx * np.sin(angle) + cell.vy * np.cos(angle)\n   \n       # Update the acceleration with the new velocity vector\n       cell.vx = new_vx\n       cell.vy = new_vy\n       cell.ax += cell.vx\n       cell.ay += cell.vy\n\nStudy if your individuals can find the resource peak. Notice that depending on your implementation, it may or may not work. Make sure to carefully investigate why it does or does not work.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-sticking",
    "href": "evo_practical_1.html#sec-sticking",
    "title": "13  Sticking together",
    "section": "13.7 Sticking together",
    "text": "13.7 Sticking together\nCells sticking together can be implemented in multiple ways. Cells could be connected by a Newtonian spring, or we could simple make sure that cells that are close to each other are attracted to one another. In this case, we will use the latter method. Note that this is not very different from collision avoidance, but it is the other way around. In fact, we now have two opposing forces: cells are attracted to one another but do not want to overlap. This can be a bit finicky to get right, so feel free to explore. Ask for help if you get stuck.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#cells",
    "href": "evo_practical_1.html#cells",
    "title": "13  Sticking together",
    "section": "13.8 1000 cells?",
    "text": "13.8 1000 cells?\nTry to run the model with 1000 cells. Also go back to the starting code again (without all the additions), and run this code with 1000 or cells too.\n\nExercise 13.7 (Algorithmic / computational thinking)  \n\nWhat happens? Can you explain this?\n\n\n\nAnswer It becomes quite slow, and in the microbial world 1000 cells is not all that much… The earlier code without all the collision and stickiness did not have such a noticable slowing down. This is because the collision and stickiness functions both loop over all cells for each cell. For 1000 cells, this results in 1,000,000 interactions to check per timestep, which can be quite slow in Python. In other words, because we have to compare everyone with everyone, a 10-fold increase in the number of cells results in a 100-fold increase in the computation time. Not ideal.\n\nThis is far as this introduction to IBMs in python goes. Simple IBMs can be efficiently implemented in basic Python, but for more complex models, it is better to i) use numpy operations to speed up your Python code, or ii) use a faster programming language like C, Rust, or Javascript. For the last part of this pratical, we will study the Javascript version of this model. But before that, here is the final code that I ended up with:",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#final-python-code-for-interested-students",
    "href": "evo_practical_1.html#final-python-code-for-interested-students",
    "title": "13  Sticking together",
    "section": "13.9 Final Python code (for interested students)",
    "text": "13.9 Final Python code (for interested students)\nThis combination of individuals moving in continuous space, combined with a grid (e.g. with resources, or other environmental states) is a very useful way to make a spatially structured model.\n\n\n\n\n\n\nFinal code\n\n\n\n\n\n\n###\n# PRACTICAL 1 | \"Every cell for themselves?\"\n# Things in this model that you have tried to implement yourself:\n# 1. Implement collision avoidance\n# 2. Implement reproduction\n# 3. Implement a Gaussian grid\n# 4. Implement \"run and tumble\"\n# 5. Add noise to Gaussian, what happens?\n# 5. Modify collision into STICKING (a little finicky)\n# 6. Try it out with 500 cells... \n###\n\n###\n# PRACTICAL 1 | PLENARY DISCUSSION\n# What else was discussed in the plenary?\n# 1. Why are grids so popular in modelling?\n# 2. Tessellation of space\n# 3. Automatic tessellation of space: quad tree\n# 4. In the full model (javascript/Cacatoo), a quad tree is present, impacting performance\n###\n\n# 1. IMPORTS AND PARAMETERS\n# Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider\n\n# Parameters for simulation\nWORLD_SIZE = 200  # Width / height of the world (size of grid and possible coordinates for cells)\nMAX_VELOCITY = 0.3  # Maximum velocity magnitude\nMAX_FORCE = 0.3  # Maximum force magnitude\nDRAG_COEFFICIENT = 0.01  # Friction to slow down the cell naturally\nRANDOM_MOVEMENT  = 0.01 # Random movement factor to add some noise to the cell's movement\nCELL_STICKINESS_LOW = 0.0 # Minimal stickiness of cells in population\nCELL_STICKINESS_HIGH = 0.10 # Maximal stickiness of cells in population\n# Parameters for display\nDRAW_ARROW = False  # Draw the arrows showing the velocity direction of the cells\nNOISE = 2 # Noise factor for the Gaussian grid (noise amount is raised to the power of this value)\nINIT_CELLS = 64 # Initial number of cells in the simulation\nSEASON_DURATION = 1000 # Duration of a season, after which the Gaussian grid is regenerated\nDISPLAY_INTERVAL = 5\n\n# 1. MAIN LOOP (using functions and classes defined below)\ndef main():\n    \"\"\"Main function to set up and run the simulation.\"\"\"\n    # Initialise simulation and its # The `visualis` variable in the code snippet provided is actually\n    # a misspelling of the correct variable name `vis`, which stands\n    # for the `Visualisation` class instance. The `Visualisation`\n    # class is responsible for managing the visualization of the\n    # simulation, including creating plots, updating them, and\n    # handling user interactions like the slider.\n    \n    num_cells = INIT_CELLS\n    sim = Simulation(num_cells) \n\n    plt.ion()\n    vis = Visualisation(sim)\n\n\n    def update_cells(val):\n        sim.initialise_cells(int(vis.slider.val))\n        vis.redraw_plot(sim)\n        \n    # Connect the slider to the update function\n    vis.slider.on_changed(update_cells)\n\n    # Run simulation\n    for t in range(1, 10000):\n        sim.simulate_step()\n        if(t % DISPLAY_INTERVAL == 0):\n            vis.update_plot(sim)\n            vis.ax.set_title(f\"Timestep: {t}\")\n            vis.fig.canvas.draw_idle()\n            plt.pause(10e-20)\n        if(t % SEASON_DURATION==0):\n            sim.fill_grid(sim.grid, 0.2+np.random.uniform(0,0.6), 0.2+np.random.uniform(0,0.6), 0.1, NOISE)\n            vis.redraw_plot(sim)# Create Gaussian grid\n        # Update title and redraw the plot\n\n    # Keep the final plot open\n    plt.ioff()\n    # plt.show()\n\n\n\n# 2. SIMULATION CLASS\nclass Simulation:\n    \"\"\"Manages the grid, cells, target, and simulation logic.\"\"\"\n    def __init__(self, num_cells):\n        self.grid = np.zeros((WORLD_SIZE, WORLD_SIZE))  # Initialise an empty grid\n        self.cells = []\n        self.target_position = [WORLD_SIZE/3, WORLD_SIZE/3]  # Initial target position at the center\n        self.target_position = [-1,-1]\n        self.fill_grid(self.grid, 0.5, 0.5, 0.1, NOISE)  # Create Gaussian grid\n        self.initialise_cells(num_cells)\n\n    def simulate_step(self):\n        \"\"\"Simulate one timestep of the simulation.\"\"\"\n        for cell in self.cells:\n            # Actions taken by each cell. Most of them are still undefined, so you can implement them yourself.\n            #self.move_towards_dot(cell)  \n            #if self.check_target_reached(cell):\n            #    print(f\"Target reached! New target position: {self.target_position}\")\n            #    self.reproduce_cell(cell) \n            \n            self.avoid_collision(cell)\n            self.stick_to_close(cell)\n            self.find_peak(cell)\n            \n            # Apply drag force to acceleration\n            cell.ax += -DRAG_COEFFICIENT * cell.vx\n            cell.ay += -DRAG_COEFFICIENT * cell.vy\n\n            # Apply forces and update position\n            \n            cell.apply_forces()\n            cell.update_position()\n\n            # Limit velocity to the maximum allowed\n            cell.vx = np.clip(cell.vx, -MAX_VELOCITY, MAX_VELOCITY)\n            cell.vy = np.clip(cell.vy, -MAX_VELOCITY, MAX_VELOCITY)\n\n    def initialise_cells(self, num_cells):\n        \"\"\"Initialise the cells with random positions and velocities.\"\"\"\n        self.cells = []\n        for _ in range(num_cells):\n            x = np.random.uniform(0, WORLD_SIZE)\n            y = np.random.uniform(0, WORLD_SIZE)\n            vx = np.random.uniform(-1, 1)\n            vy = np.random.uniform(-1, 1)\n            self.cells.append(Cell(x, y, vx, vy))\n\n    def fill_grid(self, grid, mean_x, mean_y, std_dev, noise=0):\n        \"\"\"Creates a Gaussian distribution with noise on the grid.\"\"\"\n        for i in range(WORLD_SIZE):\n            for j in range(WORLD_SIZE):\n                x = i / (WORLD_SIZE - 1)\n                y = j / (WORLD_SIZE - 1)\n                distance_squared = (x - mean_x)**2 + (y - mean_y)**2\n                grid[i, j] = np.exp(-distance_squared / (2 * std_dev**2)) * np.random.uniform(0.0, 1.0)**noise\n\n        # Normalize the grid to keep the total resource concentration the same\n        grid /= np.sum(grid)\n        self.grid = grid\n    \n    def find_peak(self, cell):\n        \"\"\"Make the cell move towards the peak of the resource gradient with a random walk.\"\"\"\n        # Convert cell position to grid indices, as well as the previous position\n        grid_x = int(cell.x) % WORLD_SIZE\n        grid_y = int(cell.y) % WORLD_SIZE\n        next_x = (int(cell.x + 10*cell.vx) + WORLD_SIZE) % WORLD_SIZE \n        next_y = (int(cell.y + 10*cell.vy) + WORLD_SIZE) % WORLD_SIZE \n        # Get the resource value at the cell's position, as well as the previous position\n        resource_value = self.grid[grid_x, grid_y]\n        resource_next = self.grid[next_x, next_y]\n        \n        # Check if the cell is moving in the right direction\n        if resource_next &gt; resource_value:\n            # Moving in the right direction: small random adjustment\n            angle = np.random.uniform(-0.1, 0.1)  # Small angle change\n        else:\n            # Moving in the wrong direction: large random adjustment\n            angle = np.random.uniform(-np.pi*1.0, np.pi*1.0)  # Large angle change\n        \n        # Rotate the velocity vector by the random angle according to trigonometric rotation formulas\n        new_vx = cell.vx * np.cos(angle) - cell.vy * np.sin(angle)\n        new_vy = cell.vx * np.sin(angle) + cell.vy * np.cos(angle)\n\n        # Update the acceleration with the new velocity vector, such that the cell moves towards the peak\n        cell.vx = new_vx\n        cell.vy = new_vy\n        cell.ax += cell.vx\n        cell.ay += cell.vy\n         \n    \n    def avoid_collision(self, cell):\n        \"\"\"Avoidance forces to prevent cells from colliding.\"\"\"\n        for other_cell in self.cells:\n            if other_cell is not cell:\n                # Calculate the distance between the two cells\n                dx = cell.x - other_cell.x\n                dy = cell.y - other_cell.y\n                distance = np.sqrt(dx**2 + dy**2)\n\n                # If the cells are too close, apply a repulsion force\n                if distance &lt; 5.0 and distance &gt; 0:  # Threshold for \"too close\"\n                    # Calculate the repulsion force proportional to the inverse of the distance\n                    force_magnitude = (5.0 - distance) / distance\n                    cell.ax += force_magnitude * dx  * 100\n                    cell.ay += force_magnitude * dy * 100\n                    \n    def stick_to_close(self, cell):\n        \"\"\"Stick to closeby cells.\"\"\"\n        for other_cell in self.cells:\n            if other_cell is not cell:\n                # Calculate the distance between the two cells\n                dx = cell.x - other_cell.x\n                dy = cell.y - other_cell.y\n                distance = np.sqrt(dx**2 + dy**2)\n\n                # If the cells are too close, apply a repulsion force\n                if distance &lt; 12 and distance &gt; 5:  # Threshold for \"close\"\n                    # Calculate the repulsion force proportional to the inverse of the distance\n                    cell.ax -= cell.stickiness * dx *10\n                    cell.ay -= cell.stickiness * dy *10\n    \n    def move_towards_dot(self, cell):\n        \"\"\"Apply forces in the direction of the dot.\"\"\"\n        # Calculate dx and dy\n        dx = self.target_position[0] - cell.x\n        dy = self.target_position[1] - cell.y\n        # Calculate the distance to the target (pythagorean theorem)\n        distance = np.sqrt(dx**2 + dy**2)\n        \n        # Normalize dx and dy \n        dx /= distance\n        dy /= distance\n        # Apply a small force towards the target\n        cell.ax += dx * 0.01\n        cell.ay += dy * 0.01\n    \n    def check_target_reached(self, cell):\n        distance_to_target = np.sqrt((cell.x - self.target_position[0])**2 +\n                                         (cell.y - self.target_position[1])**2)\n        if distance_to_target &lt; 3:\n            # Set a new target position\n            self.target_position = [np.random.uniform(0, WORLD_SIZE), np.random.uniform(0, WORLD_SIZE)]\n            return(True)\n        return(False)\n    \n    def reproduce_cell(self, cell):\n        # Reproduce: Create a new cell with the same properties as the current cell\n        angle = np.random.uniform(0, 2 * np.pi)\n        radius = np.random.uniform(0.05, 1.5)\n        new_x = cell.x + radius * np.cos(angle)\n        new_y = cell.y + radius * np.sin(angle)\n        new_cell = Cell(new_x, new_y, cell.vx, cell.vy)\n        random_cell = np.random.choice(self.cells)   \n        self.cells.remove(random_cell)\n        self.cells.append(new_cell)\n\n\n        \n        \n        \n# 3. CELL CLASS\nclass Cell:\n    \"\"\"Represents an individual cell in the simulation.\"\"\"\n    def __init__(self, x, y, vx, vy):\n        self.x = x\n        self.y = y\n        self.vx = vx\n        self.vy = vy\n        self.ax = 0\n        self.ay = 0\n        if(np.random.uniform(0,1) &lt; 0.5): \n            self.stickiness = CELL_STICKINESS_LOW\n        else:\n            self.stickiness = CELL_STICKINESS_HIGH\n        \n    def update_position(self):\n        \"\"\"Update the cell's position based on its velocity.\"\"\"\n        self.x = (self.x + self.vx ) % WORLD_SIZE  # Wrap around the world\n        self.y = (self.y + self.vy ) % WORLD_SIZE  # Wrap around the world\n\n    def apply_forces(self):\n        \"\"\"Apply a force to the cell, updating its velocity.\"\"\"\n        self.ax = np.clip(self.ax, -MAX_FORCE, MAX_FORCE)\n        self.ay = np.clip(self.ay, -MAX_FORCE, MAX_FORCE)\n        self.vx += self.ax + RANDOM_MOVEMENT * np.random.uniform(-1, 1)\n        self.vy += self.ay + RANDOM_MOVEMENT * np.random.uniform(-1, 1)\n        self.ax = 0\n        self.ay = 0\n        \n\n\n# Visualisation class for showing the individuals and the grid. For the practical, you do not need to change this. \nclass Visualisation:    \n    def __init__(self, sim):\n        fig, ax = plt.subplots(figsize=(6, 6))\n        self.cell_x = [cell.x for cell in sim.cells]\n        self.cell_y = [cell.y for cell in sim.cells]\n        self.cell_vx = np.array([cell.vx for cell in sim.cells])\n        self.cell_vy = np.array([cell.vy for cell in sim.cells])\n        self.cell_stickiness = np.array([cell.stickiness for cell in sim.cells])\n        # Colour cells by stickiness using inferno colormap\n        self.cell_scatter = ax.scatter(self.cell_x, self.cell_y, c=self.cell_stickiness, cmap='inferno', s=50, edgecolor='white', vmin=0, vmax=CELL_STICKINESS_HIGH*1.2)\n        if(DRAW_ARROW): self.cell_quiver = ax.quiver(self.cell_x, self.cell_y, self.cell_vx * 0.15, self.cell_vy * 0.15, angles='xy', scale_units='xy', scale=0.02, color='darkblue')\n        plt.subplots_adjust(bottom=0.2)\n\n        ax.set_xlim(0, WORLD_SIZE)\n        ax.set_ylim(0, WORLD_SIZE)\n        ax.set_aspect('equal', adjustable='box')\n        ax.set_title(f\"Timestep: 0\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n\n        target_point=ax.scatter(sim.target_position[0], sim.target_position[1], c='orange', s=50, edgecolor='white')\n        grid_im=ax.imshow(sim.grid.T, extent=(0, WORLD_SIZE, 0, WORLD_SIZE), origin='lower', cmap='viridis', alpha=1.0)\n\n        self.fig = fig\n        self.ax = ax\n        self.target_point = target_point\n        self.grid_im = grid_im\n        # Add a slider for selecting the number of cells\n        ax_slider = plt.axes([0.2, 0.05, 0.6, 0.03])\n        self.slider = Slider(ax_slider, 'Cells', 1, 1000, valinit=len(sim.cells), valstep=1)\n\n    def update_cell_positions(self, sim):\n        \"\"\"Update the positions of the cells in the visualisation.\"\"\"\n        self.cell_x = [cell.x for cell in sim.cells]\n        self.cell_y = [cell.y for cell in sim.cells]\n        self.cell_vx = np.array([cell.vx for cell in sim.cells])\n        self.cell_vy = np.array([cell.vy for cell in sim.cells])\n        self.cell_stickiness = np.array([cell.stickiness for cell in sim.cells])\n    \n    def update_plot(self, sim):\n        self.update_cell_positions(sim)\n        self.cell_scatter.set_offsets(np.c_[self.cell_x,self.cell_y])\n        self.cell_scatter.set_array(self.cell_stickiness)\n        if(DRAW_ARROW): \n            self.cell_quiver.set_offsets(np.c_[self.cell_x, self.cell_y])\n            self.cell_quiver.set_UVC(self.cell_vx * 0.15, self.cell_vy * 0.15)        \n\n    def redraw_plot(self, sim):\n        self.update_cell_positions(sim)\n        cell_scatter_new = self.ax.scatter(self.cell_x, self.cell_y, c=self.cell_stickiness, cmap='inferno', s=50, edgecolor='white', vmin=0, vmax=CELL_STICKINESS_HIGH*1.2)\n        if(DRAW_ARROW): \n            cell_quiver_new = self.ax.quiver(self.cell_x, self.cell_y, self.cell_vx * 0.15, self.cell_vy * 0.15, angles='xy', scale_units='xy', scale=0.02, color='darkblue')\n            self.cell_quiver.remove()\n            self.cell_quiver = cell_quiver_new\n        self.cell_scatter.remove()\n        self.fig.canvas.draw_idle()\n        self.cell_scatter = cell_scatter_new\n        self.grid_im.remove()\n        self.grid_im = self.ax.imshow(sim.grid.T, extent=(0, WORLD_SIZE, 0, WORLD_SIZE), origin='lower', cmap='viridis', alpha=1.0)\n        \n        plt.pause(0.01)\n            \n            \n# 4. Execute the main loop\nif __name__ == \"__main__\":\n    # with cProfile.Profile() as pr:\n        main()\n        # pr.print_stats()",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#exploring-the-full-javascript-model-with-evolution",
    "href": "evo_practical_1.html#exploring-the-full-javascript-model-with-evolution",
    "title": "13  Sticking together",
    "section": "13.10 Exploring the full Javascript model with evolution",
    "text": "13.10 Exploring the full Javascript model with evolution\nThe full model is implemented in Javascript, and can be found here.\nIn this full model, cells also reproduce every once in a while (when a “season” ends). Their reproductive success is shaped by the amount of resources at that position. Every time a cell reproduces there it inherits the parents stickiness, but it can also change this value a bit. This way, stickiness is an “evolvable” property on which natural selection will act. How much each cell is attracted to nearby cells depends on an internal “stickiness” parameter. Let the simulation run for some time.\n\nExercise 13.8 (The evolution of stickiness - Biology)  \n\nWhat happens to the evolution of stickiness?\nIdentify multiple advantages and disadvantages of stickiness.\nGiven your answer in b., can you name an important parameter that may determine the balance bewteen the advantages and disadvantages of stickiness? See if you can test it using the options provided.\n\n\n\nAnswer a. In the full model, stickiness evolves to be higher. Because we select for individuals that find the resource peaks, we can state that natural selection favours stickiness. There is however a limit to this, as stickiness does not go beyond ~0.25.  b. The advantages of stickiness are that larger groups of cells are better at steering towards the resources. Even if a single cell tries to randomly go in the wrong direction (due to its tumbling mechanic), it will be pulled back. In other words, together these cells are far less sensitive to noise. Another distinct advantage you may have noticed is that the stickiest cells are sorted to be in the center of the group (as you have already seen earlier in this course!). That means that even after the peak was found, the stickiest cells have an advantage over the other cells as they can occupy the peak of the resource distribution. The disadvantages of stickiness are that larger groups clearly move more slowly, and that the sticky clusters tend to stay together even if there are many resource peaks (and therewith miss out on resources).  c. The duration of the seasonal cycle is very important. If it is very long, it does not matter that the large clusters move slowly, as they will be better at finding and staying on the peak by being sticky. If the cycle is however very short, it may be more important to be able to move quickly and change direction rapidly. In that case, being too sticky may be a problem. As you modify this parameter in the online model, you will indeed see that shorter seasons mostly cause stickiness to drift close to 0.0, while longer seasons cause stickiness to evolve towards 0.25 or even higher.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#footnotes",
    "href": "evo_practical_1.html#footnotes",
    "title": "13  Sticking together",
    "section": "",
    "text": "The code also contains a Visualisation class that uses the matplotlib library to draw the cells and their movement, which we have tuned to speed things up a bit. You do not need to understand this part of the code, but if you are interested feel free to check it out.↩︎",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html",
    "href": "evo_practical_2.html",
    "title": "14  What gets selected?",
    "section": "",
    "text": "14.1 Genotypes, phenotypes, and evolutionary algorithms\nNear the end of the lecture, we discussed the differences between the genotype (that which mutates) and the phenotype (that which is selected). Although you have likely already heard about these concepts, how can we study them in evolutionary models? During this practical, you will write your own evolutionary algorithms of increasing complexity, in order to learn about these topics. By the end of this practical, you should understand why the translation from genotype to phenotype (often referred to as the genotype-phenotype map) is such an important concept in evolutionary biology.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#simple-model-where-fitness-as-a-number",
    "href": "evo_practical_2.html#simple-model-where-fitness-as-a-number",
    "title": "14  What gets selected?",
    "section": "14.2 Simple model where fitness as a number",
    "text": "14.2 Simple model where fitness as a number\nIn the introduction of this course part on evolution, we have already looked at as simple “Moran process”:\n\nStart with a population of individuals, each with a fitness value.\nSelect individuals based on their fitness to reproduce.\nReplace a random individual with this newly generate offspring.\nWith a small probability, modify the ‘fitness’ value of the newborn.\n\nAnd so on.\nWith a Moran process, competition between individuals is modelled in a very indirect (“implicit”) way. By always selecting fit individuals, and removing a random other, any individual in the populations could be replaced by another individual, which statistically is a fitter individual. One could thus say that “everyone is competing with everyone”. A different method is often applied in spatially structured models, as in this case only nearby individuals are competing. Then, we could sample who wins from an imaginary roulette wheel:\n\nAs can be seen, not all individual have the same size on the roulette wheel. That depicts differences in their growth rates, biomass, or other approximations of “fitness”. Also, the roulette wheel contains an area (shown in black) that shapes the chance that nobody reproduces. We will try and implement this rule to let individuals reproduce based on their fitness, but with only 10 competitors at a time. Let’s start with a code where individuals have a “fitness” value, but it not yet used for selection (see below). Read/test this code thoroughly before you move on to the next section.\n\n\n\n\n\n\nCODE FOR “fitness without fitness”\n\n\n\n\n\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\n# Set the random number seed for reproducibility\nrandom.seed(0)\n\nplt.ion()  # Enable interactive plotting\n\n# --- PARAMETERS ---\ninitial_fitness = 0.1            # Starting fitness for all individuals\npopulation_size = 500             # Number of individuals (should be a square number for grid mode)\ngenerations = 20000               # Number of generations to simulate\nmutation_rate = 0.005            # Probability of mutation per reproduction event\nsample_interval = 5               # How often to sample and plot data\n\n# --- INITIALIZATION ---\n# Create initial population: all individuals start with the same fitness\npopulation = [initial_fitness for _ in range(population_size)]\n\n# Lists to track average fitness and diversity over time\navg_fitness = []\ndiversity_over_time = []\n\n# --- CORE FUNCTIONS ---\n\ndef mutate(fitness, rate=mutation_rate):\n    \"\"\"Mutate the fitness value with a given probability.\"\"\"\n    if random.random() &lt; rate:\n        # Fitness changes by a random value in [-0.1, 0.1], clipped to [0, 1]\n        return min(1.0, max(0.0, fitness + random.uniform(-0.1, 0.1)))\n    return fitness\n\ndef calculate_diversity(population):\n    \"\"\"NOT YET IMPLEMENTED! Calculate diversity as the standard deviation of fitness values.\"\"\"\n    return 0 \n\n# --- PLOTTING SETUP ---\nfig, ax1 = plt.subplots(figsize=(12, 8))\nax1.set_xlabel(\"Generation\")\nax1.set_ylabel(\"Average Fitness\", color='tab:blue')\nax1.set_ylim(0, 1)\nline1, = ax1.plot([], [], color='tab:blue', linewidth=2, label='Fitness')\nax1.tick_params(axis='y', labelcolor='tab:blue')\n\n# Second y-axis for diversity\nax2 = ax1.twinx()\nax2.set_ylabel(\"Diversity\", color='tab:green')\nline2, = ax2.plot([], [], color='tab:green', linestyle=':', linewidth=2, label='Diversity')\nax2.tick_params(axis='y', labelcolor='tab:green')\n\nfig.suptitle(\"Evolution Toward Fitness 1\")\nfig.tight_layout()\nfig.legend(loc='upper right')\nplt.grid(True)\nplt.draw()\n\n# --- EVOLUTION LOOP ---\nbest_fitness = -1\nfound = False\n\nfor gen in range(generations):\n    total_fit = sum(population)\n    best = max(population)\n    # Print when a perfect solution is found\n    if best == 1 and not found:\n        found = True\n        print(\"Found perfect solution at generation\", gen)\n        \n    # Sample and plot data at intervals\n    if gen % sample_interval == 0:\n        avg_fitness.append(total_fit / population_size)\n        diversity_over_time.append(calculate_diversity(population))\n        x_vals = [i * sample_interval for i in range(len(avg_fitness))]\n        line1.set_data(x_vals, avg_fitness)\n        line2.set_data(x_vals, diversity_over_time)\n        ax1.relim(); ax1.autoscale_view()\n        ax2.relim(); ax2.autoscale_view()\n        fig.suptitle(f\"Best Fitness: {best:.2f}\", fontsize=14)\n        plt.pause(0.01)\n        \n\n    # --- MORAN PROCESS ---\n    # For each individual, perform a reproduction event\n    for _ in range(100):  # 100 competition events per generation\n        # Select 1 random individual for replication\n        probs = [1 for fit in population] # All probability weights are equal (1.0)\n        parent_idx = random.choices(range(len(population)), weights=probs)[0] # Grab one random individual based on an unweighted list...\n        # Select individual to be replaced (uniform random)\n        dead_idx = random.randrange(len(population))\n        # Copy population for next generation\n        new_pop = population.copy()\n        # Offspring replaces the dead individual (with possible mutation)\n        new_pop[dead_idx] = mutate(population[parent_idx])\n        population = new_pop\n\ninput(\"\\nSimulation complete. Press Enter to exit plot window...\")",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#making-a-roulette-wheel-with-everyone-in-it",
    "href": "evo_practical_2.html#making-a-roulette-wheel-with-everyone-in-it",
    "title": "14  What gets selected?",
    "section": "14.3 Making a roulette wheel with everyone in it",
    "text": "14.3 Making a roulette wheel with everyone in it\nIf you have read the code, you will see that we can pass a list of weights to the function random.choices, to determine who is most likely to be sampled. Currently, all the weights are set to 1:\nprobs = [1 for fit in population] # All probability weights are equal (1.0)\n\nRun the code with the current (all equal) weights. What happens?\nModify this line of code to take the fitness values as the weight, rather than 1. (hint: this is a VERY small change in the code).",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#a-roulette-wheel-of-a-subset-of-individuals",
    "href": "evo_practical_2.html#a-roulette-wheel-of-a-subset-of-individuals",
    "title": "14  What gets selected?",
    "section": "14.4 A roulette wheel of a subset of individuals",
    "text": "14.4 A roulette wheel of a subset of individuals\nInstead of letting everyone reproduce, let us modify the code to only sample from a smaller list of ‘competitors’, and spin a virtual roulette wheel to determine who wins. There are many ways to implement this, but here’s how we will do it. We will sample N individuals from the population, and implement the following algorithm:\n\nSelect a random subset of N individuals from the population.\nTake/compute the fitness of each selected individual.\nAdd a reproduction-skip option with a fixed weight.\nChoose one individual or the skip option using weighted random selection.\nIf an individual was chosen, mutate it and replace a random individual in the population.\n\nBelow, there’s a small snippet of code doing what is explained above1. The variable no_reproduction_chance is the fixed weight that nobody gets to reproduce:\n\n\n\n\n\n\nRoulette wheel algorithm\n\n\n\n\n\n  tournament_size = 10  \n  no_reproduction_chance = 1\n  \n  competitors = random.sample(population, tournament_size)\n  # Make a list of their fitness values\n  fitness_values = [fit for fit in competitors]\n  total = sum(fitness_values)\n  # Add a \"no reproduction\" dummy competitor with fitness = 0\n  competitors_with_dummy = competitors + [None]\n  probs = [f / total for f in fitness_values] + [no_reproduction_chance]\n  winner = random.choices(competitors_with_dummy, weights=probs, k=1)[0]\n  if winner is not None:\n      # Mutate winner to produce offspring\n      offspring = mutate(winner)\n      remove_idx = random.randrange(len(population))\n      population[remove_idx] = offspring  \n        \n\n\n\nAfter you understand the roulette wheel algorithm, do the following exercise:\n\nExercise 14.1 (Questions about the roulette wheel - Algorithmic thinking) \n\nLet’s imagine a moment where the roulette wheel contains only 10 highly unfit individuals (e.g. all fitness values are 0.01). What is the chance that someone will reproduce? (you don’t have to calculate it, but give your reasoning)\nAnswer the same question as in a., but now imagine that all 10 individuals have a fitness value of 1.\nAnswer question b. and c. again, but now assume that no_reproduction_chance is equal to 0.\nDescribe what the no_reproduction_chance parameter does in biological terms.\nSpatially structured populations are often placed on a grid. Describe how you could implement a roulette wheel to resolve local competition, e.g. when an empty grid point is competed for by the neighbours.\n\n\n\nAnswer a. If we sample 10 unfit individuals, the weight of the no_reproduction_event is proportionally high (the black slice of the roulette wheel is big). Hence, there is only a small chance that anyone will reproduce to begin with. b. If we sample 10 fit individuals (fitness 1), the chances are much higher that someone will reproduce. c. If the dummy value is 0, the chances that someone will reproduce are the same in both scenarios, as even with very unfit individuals there is no chance that nobody reproduces. d. In nature, if no individual is sufficiently fit, reproduction may not occur at all. For example, if the competing individuals are bacteria with very low glucose uptake rates, they may not yet be physiologically ready to reproduce. In such cases, population size should remain stable or even decline if death is also occurring. The no_reproduction_event captures this by ensuring that fitness is not judged solely in relative terms against other individuals, but also in absolute terms against environmental demands. e. If a grid point is empty (contains no individual), make a list of (up to) 8 individuals around that grid point. Apply the roulette wheel for those individuals, and place the ‘winner’ inside the empty grid point.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#diversity-patterns",
    "href": "evo_practical_2.html#diversity-patterns",
    "title": "14  What gets selected?",
    "section": "14.5 Diversity patterns",
    "text": "14.5 Diversity patterns\nModify the function calculate_diversity to calculate the diversity of the population as the standard deviation of the fitness values.\n\nExercise 14.2 (The dynamics of diversity - Biology) \n\nUse a low mutation rate and study the dynamics of diversity. Describe the pattern verbally.\n\n\n\nAnswer A snippet to calculate the standard deviation of a population is shown below. Note that this value is already being plotted, so if you modify this function you ought to be able to see what happens immediately.\ndef calculate_diversity(population):\n   \"\"\"Calculate diversity as the standard deviation of fitness values.\"\"\"\n   mean = sum(population) / len(population)\n   variance = sum((x - mean) ** 2 for x in population) / (len(population) - 1)\n   return math.sqrt(variance)\n\nThe green dotted line is the standard deviation of the values in the population (‘diversity’). From this we can see that the population is only briefly diverse whenever a new, fit individual appears. In between these phases, diversity is 0. This makes intuitive (biological) sense, as with a low mutation rate the only moments where there is more than 1 species is during the invasion of a new mutant. During all other phases, there is just a single (fittest) species.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#evolving-a-dna-sequence",
    "href": "evo_practical_2.html#evolving-a-dna-sequence",
    "title": "14  What gets selected?",
    "section": "14.6 Evolving a DNA sequence",
    "text": "14.6 Evolving a DNA sequence\nA big problem with the previous model is there is no true distinction between genotype (that which mutates) and phenotype (that which is selected). Let us try and adapt the model to be more biologically meaningful, by making each individual represented by a DNA sequence. Copy the following code:\n\n\n\n\n\n\nStarting code for “evolving a DNA sequence”\n\n\n\n\n\nimport random\nimport math\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# set the random number seed\nrandom.seed(0)\n\nplt.ion()  # Enable interactive mode\n\n# Parameters\nalphabet = \"ATCG\"\ntarget_sequence = \"GATGCGCGCTGGATTAAC\"  # Example target sequence\ndna_length = len(target_sequence)\ntarget_length = len(target_sequence)\n\n# Simulation settings\npopulation_size = 500  # must be a square number for grid mode\ngenerations = 20000\nmutation_rate = 0.001\nsample_interval = 5\nsample_size = population_size\nno_reproduction_chance = 0.1\n\n# Core functions\ndef fitness(dna):\n    return 1 - sum(a != b for a, b in zip(dna, target_sequence)) / target_length\n\ndef mutate(dna, rate=mutation_rate):\n    return ''.join(\n        random.choice([b for b in alphabet if b != base]) if random.random() &lt; rate else base\n        for base in dna\n    )\n\ndef count_beneficial_mutations(dna):\n    f0 = fitness(dna)\n    count = 0\n    for i in range(len(dna)):\n        for b in alphabet:\n            if b != dna[i]:\n                mutant = dna[:i] + b + dna[i+1:]\n                if fitness(mutant) &gt; f0:\n                    count += 1\n    return count\n\ndef diversity(pop):\n    counts = {}\n    for ind in pop:\n        counts[ind] = counts.get(ind, 0) + 1\n    total = len(pop)\n    return -sum((c/total) * math.log(c/total + 1e-9) for c in counts.values()) if total &gt; 0 else 0\n\n# Initialize population\ninitial_sequence = \"GATAGCGAAGTTTAGCCG\" # far from target (only first 3 are correct)\npopulation = [initial_sequence for _ in range(population_size)]\n\navg_fitness = []\navg_beneficial = []\ndiversity_over_time = []\nbest_individuals = []\n\ndef get_neighbors(i, j):\n    return [(x % side, y % side)\n            for x in range(i-1, i+2)\n            for y in range(j-1, j+2)]\n\n# Initialize interactive plot\nfig, ax1 = plt.subplots(figsize=(12, 8))\nax1.set_xlabel(\"Generation\")\nax1.set_ylabel(\"Average Fitness\", color='tab:blue')\nax1.set_ylim(0, 1)\nline1, = ax1.plot([], [], color='tab:blue', linewidth=2, label='Fitness')\nax1.tick_params(axis='y', labelcolor='tab:blue')\n\nax2 = ax1.twinx()\nax2.set_ylabel(\"Beneficial Mutations / Diversity\", color='tab:purple')\nline2, = ax2.plot([], [], color='tab:purple', linestyle='--', linewidth=2, label='Beneficial Mutations')\nline3, = ax2.plot([], [], color='tab:green', linestyle=':', linewidth=2, label='Diversity')\nax2.tick_params(axis='y', labelcolor='tab:purple')\nfig.suptitle(\"Evolution Toward Target Sequence\")\nfig.tight_layout()\nax2.set_ylim(0, 20)\nfig.legend(loc='upper right')\nplt.grid(True)\nplt.draw()\n\nbest_seq = \"\"\nbest_score = -1\nfound = False\n\n# Evolution loop\nfor gen in range(generations):\n    fitnesses = [fitness(ind) for ind in population]\n    total_fit = sum(fitnesses)\n    best = max(fitnesses)\n    if(best == 1 and not found):\n        found = True\n        print(\"Found perfect solution at generation\", gen)\n        \n    if gen % sample_interval == 0:\n        sample = random.sample(population, sample_size)\n        avg_beneficial.append(sum(count_beneficial_mutations(ind) for ind in sample) / sample_size)\n        diversity_over_time.append(diversity(population))\n\n        # Update plot data\n        line1.set_data(range(len(avg_fitness)+1), avg_fitness + [sum(fitnesses)/population_size])\n        line2.set_data(range(len(avg_beneficial)), avg_beneficial)\n        line3.set_data(range(len(diversity_over_time)), diversity_over_time)\n        ax1.relim(); ax1.autoscale_view()\n        ax2.relim(); ax2.autoscale_view()\n        best = max(population, key=fitness)\n        fig.suptitle(f\"Best: {best} (target: {target_sequence})\", fontsize=14)\n        plt.pause(0.01)\n\n    else:\n        avg_beneficial.append(avg_beneficial[-1])\n        diversity_over_time.append(diversity_over_time[-1])\n\n    # Tournament selection (as in evolving_fitness_final.py)\n    new_pop = []\n    tournament_size = 10  # can be adjusted\n\n    for _ in range(population_size):\n        # Select tournament_size individuals randomly\n        competitors = random.sample(population, tournament_size)\n        # Pick the one with highest fitness\n        fitness_values = [fitness(ind) for ind in competitors]\n        total = sum(fitness_values)\n        \n        probs = [f / total for f in fitness_values]\n        winner = random.choices(competitors, weights=probs, k=1)[0]\n        # Mutate winner to produce offspring\n        offspring = mutate(winner)\n        new_pop.append(offspring)\n\n    population = new_pop\n\n    avg_fitness.append(sum(fitness(ind) for ind in population) / population_size)\n    if gen % 250 == 0:\n        best = max(population, key=fitness)\n        best_individuals.append((gen, best))\n\ninput(\"\\nSimulation complete. Press Enter to exit plot window...\")\n\n\n\nAnswer the following questions using the options available in the model:\n\nExercise 14.3 (Evolving DNA - Biology / abstract thinking)  \n\nRun the code. What does the new (dashed blue) line represent? Do you understand how is changes over time?\n\nThe program reports after how many generations it manages to find the target sequence. With default settings this can take a long time… (default: 429 generations)\n\nModify the mutation rate to see how it affects the time to find the target sequence. Try different mutation rates between 0.0001 and 0.1. Keep track of both how long (number of generations) it takes to find the target, and how fit the population is once the target is found. What do you observe?\nDiversity is no longer calculated as the standard deviation in fitness, but as the Shannon diversity of all present sequences (although it is not super complex, you do not need to fully understand this function). Because of this, the exact number (quantities) cannot be compared to our ealrier model. Do you see a qualitative differences?\nStudy how fitness is calculated in this model. Is there a distinction between genotype en phenotype? Why/why not?\n\n\n\nAnswer a. The new blue dotted line represents how many mutations are beneficial (towards the target). As the population gets closer to the target, the number of beneficial mutations decreases. As such, this line is a mirror image of the fitness in the population. We will look a bit deeper into this line in the next model.  b. Generally speaking, a higher mutation rate helps to find the target faster. However, with high mutation rates (0.01 or higher), the fitness after the target is found starts to decrease, as individual produce many (unfit) mutants. In fact, if mutation rate is too high (approximately 0.04 or higher), the population fails to find the target at all, as reproduction is too inaccurate! This concept is known as the ‘Error Threshold’ or the ‘Error Catastrophe’ in evolutionary biology.  c. Qualitatively, there is no clear difference to what we saw before: diversity only peaks at moments when there is a new mutant coming in, but otherwise diversity is still 0. d. There is no distinction between genotype and phenotype. Fitness is directly calculated from the DNA sequence, so there is no ‘genotype-to-phenotype mapping’.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#evolving-a-protein-sequence",
    "href": "evo_practical_2.html#evolving-a-protein-sequence",
    "title": "14  What gets selected?",
    "section": "14.7 Evolving a protein sequence",
    "text": "14.7 Evolving a protein sequence\nNext we will extend the simulation a little more. The individual genotypes will still be represented as a DNA sequence, but before evaluating fitness this will be translated into a protein sequence. To do so, the code first defines the codon table (which we of course all know by heart =)), and then translates the DNA sequence into a protein sequence. The protein sequence is then used to calculate the fitness of the individual, which is based on how well the protein sequence matches a target protein sequence. The code is as follows:\n\n\n\n\n\n\nStarting code for evolving a protein sequence\n\n\n\n\n\nimport random\nimport math\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# set the random number seed\nrandom.seed(0)\n\nplt.ion()  # Enable interactive mode\n\n# Codon table\ncodon_table = {\n    'TTT': 'F', 'TTC': 'F', 'TTA': 'L', 'TTG': 'L', 'CTT': 'L', 'CTC': 'L', 'CTA': 'L', 'CTG': 'L',\n    'ATT': 'I', 'ATC': 'I', 'ATA': 'I', 'ATG': 'M',\n    'GTT': 'V', 'GTC': 'V', 'GTA': 'V', 'GTG': 'V',\n    'TCT': 'S', 'TCC': 'S', 'TCA': 'S', 'TCG': 'S', 'AGT': 'S', 'AGC': 'S',\n    'CCT': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P',\n    'ACT': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n    'GCT': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A',\n    'TAT': 'Y', 'TAC': 'Y', 'CAT': 'H', 'CAC': 'H',\n    'CAA': 'Q', 'CAG': 'Q', 'AAT': 'N', 'AAC': 'N',\n    'AAA': 'K', 'AAG': 'K', 'GAT': 'D', 'GAC': 'D',\n    'GAA': 'E', 'GAG': 'E', 'TGT': 'C', 'TGC': 'C',\n    'TGG': 'W', 'CGT': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R', 'AGA': 'R', 'AGG': 'R',\n    'GGT': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G',\n    'TAA': '*', 'TAG': '*', 'TGA': '*'\n}\n\n# Parameters\nalphabet = \"ATCG\"\ntarget_protein = \"DARWIN\"\ndna_length = len(target_protein)*3\ntarget_length = len(target_protein)\n\n# Simulation settings\npopulation_size = 625  # must be a square number for grid mode\ngenerations = 20000\nmutation_rate = 0.0005   \nsample_interval = 5\nsample_size = population_size\nno_reproduction_chance = 0.01\n\n# Core functions\ndef translate(dna):\n    return ''.join(codon_table.get(dna[i:i+3], '?') for i in range(0, len(dna), 3))\n\ndef fitness(dna):\n    protein = translate(dna)\n    return 1 - sum(a != b for a, b in zip(protein, target_protein)) / target_length\n\ndef mutate(dna, rate=mutation_rate):\n    return ''.join(\n        random.choice([b for b in alphabet if b != base]) if random.random() &lt; rate else base\n        for base in dna\n    )\n\ndef count_beneficial_mutations(dna):\n    f0 = fitness(dna)\n    count = 0\n    for i in range(len(dna)):\n        for b in alphabet:\n            if b != dna[i]:\n                mutant = dna[:i] + b + dna[i+1:]\n                if fitness(mutant) &gt; f0:\n                    count += 1\n    return count\n\ndef diversity(pop):\n    counts = Counter(pop)\n    total = len(pop)\n    return -sum((c/total) * math.log(c/total + 1e-9) for c in counts.values()) if total &gt; 0 else 0\n\n# Initialize population\ninitial_sequence = ''.join(random.choice(alphabet) for _ in range(dna_length))\npopulation = [initial_sequence for _ in range(population_size)]\n\navg_fitness = []\navg_beneficial = []\ndiversity_over_time = []\nbest_individuals = []\n\n# Grid setup\nside = int(math.sqrt(population_size))\nassert side * side == population_size, \"Population size must be a square number for grid mode\"\n\ndef get_neighbors(i, j):\n    return [(x % side, y % side)\n            for x in range(i-1, i+2)\n            for y in range(j-1, j+2)]\n\n# Initialize interactive plot\nfig, ax1 = plt.subplots(figsize=(12, 8))\nax1.set_xlabel(\"Generation\")\nax1.set_ylabel(\"Average Fitness\", color='tab:blue')\nax1.set_ylim(0, 1)\nline0, = ax1.plot([], [], color='black', linewidth=1, label='Max fitness')\nline1, = ax1.plot([], [], color='tab:blue', linewidth=2, label='Fitness')\nax1.tick_params(axis='y', labelcolor='tab:blue')\n\nax2 = ax1.twinx()\nax2.set_ylabel(\"Beneficial Mutations / Diversity\", color='tab:purple')\nline2, = ax2.plot([], [], color='tab:purple', linestyle='--', linewidth=2, label='Beneficial Mutations')\nline3, = ax2.plot([], [], color='tab:green', linestyle=':', linewidth=2, label='Diversity')\nax2.tick_params(axis='y', labelcolor='tab:purple')\nfig.suptitle(\"Evolution Toward \" + str(target_protein))\nfig.tight_layout()\nax2.set_ylim(0, 5)\nfig.legend(loc='upper right')\nplt.grid(True)\nplt.draw()\n\nbest_seq = \"\"\nbest_score = -1\nbest_fitnesses = []\nfound = False\n\n# Evolution loop\nfor gen in range(generations):\n    fitnesses = [fitness(ind) for ind in population]\n    total_fit = sum(fitnesses)\n    best = max(fitnesses)\n    best_fitnesses.append(best)\n    if(best == 1 and not found):\n        found = True\n        print(\"Found perfect solution at generation\", gen)\n        \n    if gen % sample_interval == 0:\n        sample = random.sample(population, sample_size)\n        avg_beneficial.append(sum(count_beneficial_mutations(ind) for ind in sample) / sample_size)\n        diversity_over_time.append(diversity(population))\n\n        # Update plot data\n        line0.set_data(range(len(avg_fitness)+1), best_fitnesses)\n        line1.set_data(range(len(avg_fitness)+1), avg_fitness + [sum(fitnesses)/population_size])\n        line2.set_data(range(len(avg_beneficial)), avg_beneficial)\n        line3.set_data(range(len(diversity_over_time)), diversity_over_time)\n        ax1.relim(); ax1.autoscale_view()\n        ax2.relim(); ax2.autoscale_view()\n        best = max(population, key=fitness)\n        fig.suptitle(f\"Best: {translate(best)} (target: {target_protein})\", fontsize=14)\n        plt.pause(0.01)\n\n    else:\n        avg_beneficial.append(avg_beneficial[-1])\n        diversity_over_time.append(diversity_over_time[-1])\n\n    new_pop = []\n    tournament_size = 10  # can be adjusted\n    \n    for _ in range(population_size):\n        # Select tournament_size individuals randomly\n        competitors = random.sample(population, tournament_size)\n        # Pick the one with highest fitness\n        fitness_values = [fitness(ind) for ind in competitors]\n        total = sum(fitness_values)\n        \n        probs = [f / total for f in fitness_values]\n        winner = random.choices(competitors, weights=probs, k=1)[0]\n        # Mutate winner to produce offspring\n        offspring = mutate(winner)\n        new_pop.append(offspring)\n\n    population = new_pop\n\n\n    avg_fitness.append(sum(fitness(ind) for ind in population) / population_size)\n    if gen % 250 == 0:\n        best = max(population, key=fitness)\n        best_individuals.append((gen, translate(best)))\n\ninput(\"\\nSimulation complete. Press Enter to exit plot window...\")\n\n\n\nAnswer the following question about the model:\n\nExercise 14.4 (Evolving protein sequences - Biology / abstract thinking)  \n\nAnother line was added to the model. What new information can you obtain from analysing this line?\nStudy carefully how the other lines (also present in previous models) change over time. What do you observe? Try and capture what you see into words.\nIn biology, multiple genotypes can translate to the same phenotype (this is called a many-to-one genotype-phenotype map), or alternatively, one genotype can produce multiple phenotype (this is called phenotypic plasticity, or a one-to-many genotype-phenotype map). Which genotype-phenotype (GP) mapping applies to this model? Why?\nBonus question for motivated students modify the code to include a second target protein sequence, and alternate between the two targets. If you see something interesting, please share it with the class!\n\n\n\nAnswer a. The new black line is the maximum fitness in the population. Thanks to this line we can see that, sometimes, an individual is present that is fitter but it does not manage to take over the population. We are dealing with a stochastic process, so this is very natural.  b. The fitness line once again goes in distinct steps. The diversity line (green dotted) has a distinct behaviour compared to earlier models. Instead of only going up during the discovery of a new mutant, it constantly creeps up during periods where fitness does not change. This is because the codon-table is partially redundant: many different DNA sequences can code for the same amino acid sequence, so diveristy increases. However, when a new “fitter” individual comes in, diversity goes down as that individual clonally takes over the population. Then, diversity slowly increases again. A similar pattern is reflected in the line that represents the “beneficial mutations” (blue dotted line). TLDR; even when fitness is not changing there is still a lot going on in this population! c. This model represents a many-to-one mapping between genotype and phenotype. The DNA sequence (genotype) is translated into an amino acid sequence (phenotype), which is then used to calculate fitness. This means that many different genotypes can lead to the same phenotype, and thus the same fitness. Earlier in this course you have learned about development, and those processes often lead to effects where the same genotype can produce many different shapes (phenotypes).  d. BONUS: I have not personally done this, and I do not know the answer to this question yet. However, it is sometimes observed that with many-to-one mapping, populations can because better and better at switching between two alternating targets. This is because the alternating selection pressures make populations move towards genotypes that are “close” to both targets, and because there is some neutrality coding this can be acchieved without losing fitness in either environment. For cool paper on this principle, see Crombach and Hogeweg (2008). I suspect however that our current model will not be able to do the same.\n\n\n\n\n\nCrombach, Anton, and Paulien Hogeweg. 2008. “Evolution of Evolvability in Gene Regulatory Networks.” PLoS Computational Biology 4 (7): e1000112.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#footnotes",
    "href": "evo_practical_2.html#footnotes",
    "title": "14  What gets selected?",
    "section": "",
    "text": "Note that this is from the full code, so this code does not work stand-alone↩︎",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_3.html",
    "href": "evo_practical_3.html",
    "title": "15  Public goods",
    "section": "",
    "text": "15.1 Building a model from scratch\nOver the last weeks you have been given many model of biology, and you have modified or extended upon them. For this practical, I will give you a only a description. Your challenge will be to see how far you get in trying to get this model working yourself. I advice you use AI-assisted programming only to solve small steps, otherwise you have no clue what you are doing. But if you try and do everything yourself, it may take a little long.\nAt the end of the pratical, we will compare different implementations by students, as well as my implementation. Hopefully, we will see some generic patterns, because the model description should be good enough to give “similar models”. The description should be “vague enough” to lead to some differences, but “precise enough” to yield similar results. This is an experiment in and of itself. So let’s see :’)\nNote that I also do not yet know exactly what will happen in this simulation (although I have tried it out), so I’m hoping we will learn some cool stuff together!",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Public goods</span>"
    ]
  },
  {
    "objectID": "evo_practical_3.html#simulating-a-simple-microbial-ecosystem-with-public-goods",
    "href": "evo_practical_3.html#simulating-a-simple-microbial-ecosystem-with-public-goods",
    "title": "15  Public goods",
    "section": "15.2 Simulating a simple microbial ecosystem with “public goods”",
    "text": "15.2 Simulating a simple microbial ecosystem with “public goods”\n\nModel description\nMicrobes often produce public goods, from which surrounding microbes can benefit. This can lead to interesting dynamics, such as cooperation and competition. Most models however consider on 1 public good at a time, which leads to limited diversity (a producer, and a non-producer may or may not coexist). Here, we will an ecosystem with many public goods, and simulate them on a grid.\nAn individual microbe will carry a “genome” that is represented by a binary string (101010010011). Each position in the string represents a public good, and whether the individual can produce it (1) or not (0). The individual can rely on other individuals to produce public goods.\nWe will simulate individuals (microbial cells) reproducing and dying on a grid. A grid point either contains an individual, or it is empty. Every empty point, will be competed for by individuals that are in that neighbourhood. The neighbourhood is defined as the 8 surrounding grid points (this is called the “Moore” neighbourhood). The cells can only replicate if they have all the public goods they need, which means that they can rely on other individuals in their neighbourhood to produce them. If they do not have all public goods available, they cannot replicate. The “winner” from these (max) 8 viable competitors will be determined by a roulette wheel selection, where the relative probability is determined by their fitness:\n\\[\nF_i = 1 - c \\cdot \\sum({bitstring})\n\\]\nIn other words, fitness goes down as the number of public goods produced increases, and there is a cost \\(c\\) associated with producing each public good. Make sure this roulette wheel contains a probability that nobody wins, such that highly unfit individuals are less likely to replicate than highly fit individuals (also see earlier practicals).\nThe individual that replicates, can undergo mutations in the bitstring (gene loss and gene gain). Assume gene loss is more likely than gene gain (initial parameters to explore are summarised below)\nFinally, implement a function that allows you to mix the grid (all individuals are placed in a random position).\n\n\nModel output\nThe model will have the following output: a grid that is coloured by the number of public goods produced (for consistency, let’s all use a ‘viridis’ scale), and a line graph that plots the total population sizes, as well as the population sizes of species producing 0 public goods, 1 public good, 2 public goods, etc. (see Figure 15.1)\n\n\nParameters to start out with\n\nGrid size: 50 x 50\nInitial population: produces all public goods (1111…1)\nDeath rate: 0.1\nCost (c): 0.05\nBitstring 1 to 0 mutation (losing a gene): 0.01\nBitstring 0 to 1 mutation (gaining a gene): 0.001\nNumber of public goods (i.e. bitstring length): 10\n“No-event” size of roulette wheel: 1\n\n\n\n\n\n\n\nFigure 15.1: Example of what the simulation could look like\n\n\n\n\n\nProposed experiments\nTry investigating how the model behaves with different values of \\(c\\) (the cost of producing public goods). Can you explain what happens at \\(c=0.0\\)?\nTry studying the effect of mixing the whole grid every timestep, such that neighbourhoods are constantly “randomised”. Look at the population size, as well as the distribution of different types. Can you explain the observations in biological terms?\nTry studying what happens at different mutation rates.\n\nAnswer This practical is quite open-ended, so instead of answers it is more useful to have a scientific discussion. The image below was first run on a spatially structured grid, but from the dashed line onwards the grid was mixed every timestep. From this we can see that it matters “who you compete with”. On the spatial grid, individuals compete mostly with other individuals of their own species. That means that, on average, they cannot complement each other by providing public goods (they produce the same set!). Because of this, the “omni-producers” (yellow line) dominates, surrounded by a cloud of mutants that rely on other types. The population size fluctuates strongly, as local populations can collapse due to the loss of public goods, typically followed by the “omni-producers” once again invading the available niche space (empty space on the grid).  When mixing the grid every time step, who you interact with is randomised, and individuals can now rely on other genotypes in the population. Although there is some luck involved in this, you can see that the total population (the black line) increases from this point onward. Thus, although the omni-producers are less dominant, statistically individuals are much better off in this mixed world. That said, there is a risk to this mixed population: if the cost of producing public goods is too high, the population can collapse as there is a strong incentive to lose production of public goods and start relying on others. Depending on your implementation, you may find that mixed systems therefor perform better or worse than spatially structured systems. If they behave identically though, let me (Bram) know, as that is something I almost never observe in models like this ;)\n\n\n\n\n\nDynamics of the producer-types in a system with 10 ‘public goods’",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Public goods</span>"
    ]
  },
  {
    "objectID": "project_1.html",
    "href": "project_1.html",
    "title": "16  Miniproject: Pattern formation in vegetation in semi-arid areas",
    "section": "",
    "text": "16.1 Mini description\nIn semi-arid areas, instead of a spatially continuous vegetation pattern, shrubs and bushes typically occur in a spot, stripe, labyrinthine or inverted spot pattern with the pattern type depending on the availability of water and the severity of grazing by cattle (Figure 1). These vegetation patterns are thought to arise from a positive effect of plant presence on water infiltration in the soil and vice versa from soil water to plant growth, giving rise to a positive feedback loop.\nOne of the models frequently used for these water-vegetation systems is described by the following set of 3 equations:\n\\[\n\\frac{\\mathrm{d}N}{\\mathrm{d}t} = rN(1 - \\frac{N}{K})\n\\tag{16.1}\\]\nAnd Equation 16.1 is a reference to the equation above.\nThe model has three variables, O = surface water, W = soil water and P is plant biomass.\nThe parameters are R=rainfall, alpha=maximum uptake of surface water to become soil water, W0= the plant biomass independent fraction of surface water uptake, k2 =saturation constant determining how much plant biomass you need to reach half maximum plant mediated surface water uptake, gmax = the maximum uptake of soil water by plants, k1=saturation constant of the soil water level at which this uptake is half maximum, rw = soil water loss due to evaporation, c=factor for the conversion of uptaken water into plant biomass, and d=decay rate of plant biomass.\nClassically, it was thought that the positive feedback between water and plants gives rise to a bistable system in which at a certain point, a further gradual decrease in water level or increase in cattle grazing could lead to the sudden collapse of the vegetation, a phenomenon referred to as a catastrophic bifurcation or tipping point. Furthermore, it was thought transitions in spatial patterning could be interpreted as warning signs for the system becoming closer to this point of sudden collapse (Fig 2A). More recent mathematical analysis instead suggests that spatial patterning may allow for a more gradual transition from high to low vegetation levels without the occurrence of sudden transitions (Fig2B).\nIn this project you will investigate for the given equations which of the two situations apply for this model. Since we are not mathematicians, we will go about this in a predominantly numerical manner. First let us study the type of behavior the bove given model generates. For this you can use the python code ……. Investigate how the type of pattern, but also the average vegetation density changes as a function of rainfall, and whether you think the changes you observe reflect a catastrophic transition or rather a gradual change. Hint: Start with a rainfall level of 0.7 and increment by 0.1. Once you find patterns you can go back and use smaller steps to find the precise transition point. Once you find homogeneous patterns you can do 2 more 0.1 increments and then stop. Note that you will need to write additional code to quantify average levels, pattern characteristics etc. Analyse whether there is a critical transition or not.\nBelow R=1 no patterns, boundary at R=1.02. At that point P=4.7 so transition in terms of levels is sudden\nNow investigate the dependence of vegetation on rainfall in a non-spatial setting where Turing patterns can not occur. If we want to do things like a phase plane analysis, enabling us to study e.g. if our model is bistable, which equilibria are stable, and how this changes when we change parameters (i.e. to understand our sudden transitions in vegetation levels), we often try to reduce our models to 2D. So let us try to simplify the given 3D model to 2D by using a so-called QSSA (quasi steady state assumption) for the surface water. (Hint: To do so write dO/dt=0 and solve this for O, which gives you an algebraic equation. Now you can drop the dO/dt equation and in the other equations, substitute O for this algebraic equation.) Analyse this model using an R package called Grind that is made for analysing ODE models:……..\nYou can check the outcomes of this analysis by running the spatial python code and put all the slow diffusion parameters to the value of the fast diffusion parameter. Analyse whether you observe a critical transition as function of rainfall or not.\nThe fold bifurcation occurs at R=1.17 so earlier than in the spatial model. The fold bifurcation occurs at a vegetation level of 2 something so lower than for the spatial model. Also range of R with bistability is quite limited, already soon you get a transcritical bifurcation.\nIn the model we used here, spatial patterns allow vegetation to persist at lower rainfall levels (no vegetation below R=1 instead of R=1.17), however the transition from vegetation to no vegetation is actually more sudden (veg=4.7 instead of 2 something). So we are neither in the situation of Fig2A nor Fig2B but in a third combined situation where transitions in vegetation pattern do warn for collapse (Fig 2A), but also postpone yet not gradualize the collapse (Fig 2B).\nIn Madagascar people are trying to combat deforestation not by planting small trees, but by digging so-called foxholes, shallow round holes in the ground that are more or less evenly spaced across the terrain where they put in tree seeds in the holes. Due to their somewhat increased depth the foxholes receive more water than their environment, thus locally promoting germination and plant growth. It is said that this approach is more efficient and successful than simply planting trees.\nLet us consider a situation where vegetation is absent. Let us now compare two approaches, one in which little trees are planted uniformly, and one in which even tinier trees (i.e. germinating seeds) are placed in potholes where local water conditions are more favorable. Determine in your model, using the same equations as before what vegetation density level is necessary for homogeneous planting as an initial condition to result in a stable vegetated state. Next compare this to planting tiny plants in potholes. You can play with the size of the potholes and their numbers, as well as the precise level of starting vegetation within them and the regulatory of their positioning. To simplify the comparison you may do so for a single rainfall level “R”. Make sure to choose a value for R that allowed for vegetation in the spatial model\nWhen implementing the potholes some programming is required to create circles with certain sizes and pattern them either regularly or irregularly and at different distances\nWater conditions are better in the potholes, this can be achieved by localling making the “R” level higher or have preferential diffusion or convection of water towards them\nFor master students. Increase the value of alpha in the spatial model. Study again how vegetation patterns depend on rainfall level. Asses what happened to model outcomes and provide an explanation.\nIf alpha is high the infiltration is fast and hence the positive feedback loop is no longer local. As a consequence spatial patterning disappears and the system behaves like the non-spatial one.",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Miniproject: Pattern formation in vegetation in semi-arid areas</span>"
    ]
  },
  {
    "objectID": "project_1.html#mini-description",
    "href": "project_1.html#mini-description",
    "title": "16  Miniproject: Pattern formation in vegetation in semi-arid areas",
    "section": "",
    "text": "Figure 1. Vegetation patterns in semi-arid ecosystems. Depending on rainfall and terrain slope varies types of vegetation patterns may arise. Adjusted from Rietkerk et al., Science 2024.\n\n\n\n\n\n\n\n\n\n\n\n\nFig 2. Relation between Turing patterns and vegetation collapse. A) Transitions in Turing pattern where average vegetation level decreases are a warning sign indicating that the system is approaching a tipping point. B) Alternatively, transitions in Turing patter are a way of postponing and softening the sudden collapse in vegetation that would be expected in a non-spatially patterned ecosystem. Figure taken from Rietkerk et al., Science, 2021.\n\n\n\n\n\n\n\n\nNow compare how vegetation depends on rainfall between the spatial and non-spatial model and compare this to the situations in Figure 2A and 2B.",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Miniproject: Pattern formation in vegetation in semi-arid areas</span>"
    ]
  },
  {
    "objectID": "project_2.html",
    "href": "project_2.html",
    "title": "17  Miniproject: maintaining a healthy microbiome",
    "section": "",
    "text": "17.1 Mini description\nMicrobes often form intricate relationships, not only amongst each other, but also with larger organisms from all kingdoms: plants, animals, and fungi. While they often provide useful services, microbes typically evolve much faster than these hosts. What stops a microbe from taking advantage of its host, taking more resources than it provides, or even damaging the host tissue to gain access to even more resources? The latter scenario, we would call a pathogen, and the likelyhood of this transition from occuring could be called the disease pressure.\nUnderstanding the fundamental principles behind disease pressure can help mitigating disease outbreaks. For example, if we understand the conditions under which a microbe is likely to become a pathogen, we can take steps to prevent this from happening.\nTo phrase the above story a little differently: the microbes in our gut or in the soil of our favourite crops, are constantly evolving on a parasitism-mutualism continuum (see Figure 17.1). In this mini project, you will investigate the dynamics of microbiomes evolving on such a continuum. We will particularly focus on how the properties of the host (plant, animals, etc.) shape the likelihood of disease outbreaks. To that end, here are a few key questions to get you started, but you don’t need to focus on each and every one of them at the same time. Plus, more (better!) questions will likely emerge as you work on the project. That’s science.",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Miniproject: maintaining a healthy microbiome</span>"
    ]
  },
  {
    "objectID": "project_2.html#mini-description",
    "href": "project_2.html#mini-description",
    "title": "17  Miniproject: maintaining a healthy microbiome",
    "section": "",
    "text": "Figure 17.1: Parasitism-mutualism continuum of host-associated microbes. Note that this is a cartoon, and that nature is in almost all cases more complicated than this.",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Miniproject: maintaining a healthy microbiome</span>"
    ]
  },
  {
    "objectID": "project_2.html#guiding-questions",
    "href": "project_2.html#guiding-questions",
    "title": "17  Miniproject: maintaining a healthy microbiome",
    "section": "17.2 Guiding questions:",
    "text": "17.2 Guiding questions:\n\nDo motile hosts (e.g. animals) experience different disease pressures than non-motile hosts (e.g. plants)?\nAre mutualistic microbes easier to maintain in short- or long-lived hosts?\nDoes non-local reproduction (e.g. seed/spore dispersal) change these patterns?\nHow does an adaptive immune system (animals) affect the evolution of microbiomes, compared to plants, who don’t have an adaptive immune system?",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Miniproject: maintaining a healthy microbiome</span>"
    ]
  },
  {
    "objectID": "project_2.html#key-references",
    "href": "project_2.html#key-references",
    "title": "17  Miniproject: maintaining a healthy microbiome",
    "section": "17.3 Key references",
    "text": "17.3 Key references\n\nVan Vliet and Doebeli (2019): a model of self-sacrificing microbes in hosts with different transmission modes.\nKoskella and Bergelson (2020): an opinion piece on host-microbe (co)evolution and levels of selection",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Miniproject: maintaining a healthy microbiome</span>"
    ]
  },
  {
    "objectID": "project_2.html#getting-started",
    "href": "project_2.html#getting-started",
    "title": "17  Miniproject: maintaining a healthy microbiome",
    "section": "17.4 Getting started",
    "text": "17.4 Getting started\nMaintaining self-sacrificial microbiomes isn’t easy, as shown by the work of Van Vliet and Doebeli (2019). They show that the evolution of self-sacrificing microbes (which they call “helpers”) is highly sensitive to the host’s longevity and transmission of microbes in between hosts (horizontal transmission). To understand why, start by reading their paper. If you have read it, unfold the text below to see my summary of their methods.\n\n\n\n\n\n\nMy summary of the model by van Vliet et al.\n\n\n\n\n\nThis model studies the maintanance of self-sacrificial “helper” microbes. Here, I will refer to helpers as “allies” (A), as to clearly differentiate it from “hosts” (H). Microbes that are not allies are referred to as neutral (N).\nAllies provide a benefit to their host, while neutral microbes do not. The microbes are modelled with simple ordinary differential equations (ODEs):\n\\[\n\\begin{aligned}\n\\color{#555}{\n\\frac{dN}{dt} =\n\\underbrace{rN}_{\\textrm{Growth N}} -\n\\underbrace{\\delta N(A+N)}_{\\textrm{Density-dependent death}}\n}\\\\\n\\color{green}{\n  \\frac{dA}{dt} =\n  \\underbrace{rA(1-\\gamma)}_{\\textrm{Growth A}} -\n  \\underbrace{\\delta A(A+N)}_{\\textrm{Density-dependent death}}\n}\\\\\n\\end{aligned}\n\\tag{1}\n\\] As you can see, allies grow slower than neutral microbes and are therefore at a competitive disadvantage, and will eventually be outcompeted. To counter-act the loss of allies, van Vliet’s model considers selection at the level of the host. To achieve this, the birth rate of hosts (\\(B_i\\)) depends on the frequency of allies A in the microbiome:\n\\[\nB_i = \\frac{r}{G_H}(1+s_b\\cdot \\frac{A}{A+N})\n\\]\nHere, \\(G_H\\) is a parameter that scales the host generation time w.r.t. the growth rate of microbes (\\(r\\)), with \\(G_H \\gg r\\) ensuring hosts are long-lived compared to microbes. The term \\(s_b\\) is the maximum benefit that hosts get from carrying the ally strain.\nThe death rate of hosts (\\(D_i\\)) increases linearly with the density of hosts at any given time (\\(H(t)\\)), and is given by:\n\\[\nD_i = \\frac{r}{G_H} \\frac{H(t)}{K_H}\n\\]\nWhere \\(H(t)\\) gives the number of hosts at a given time, and \\(K_H\\) denotes the basal carrying capacity in the absence of allies. Note that the true carrying capacity can be higher, as helpers increase the birth rate of hosts.\nWhenever a host reproduces, the microbiome is transmitted vertically to their offspring. The frequency of allies in the offspring is sampled from a normal distribution with mean \\(f_A\\):\n\\[\nf_{offspring}  = \\mathcal{N} (f_{A},\\sigma^2),\n\\\\\\text{with } f_A = \\frac{A}{A+N}\n\\]\nTo avoid negative ally frequencies, this number is truncated such that \\(0&lt;A_{offspring}&lt;1\\). The total density in the newborn is set by another parameter \\(n_0\\). The model by van Vliet also considers “horizontal” transmission of microbiomes, where \\(f_A\\) is not the ally frequency in the parent, but the frequency of allies in the microbiome of a random individual.\nSo far so good with all the math. Now the simulation…\n\nThe simulation loop\n\nThe birth and death rates of all hosts is calculated\nCalculate the probability of a host-level event (birth/death) occurring\nIf an event occurs, draw a random event proportional to its probability.\nExecute the event sampled in step 3\nUpdate all the microbiome ODEs.\n\n\n\n\n\nTo start the project, we will first replicate van Vliet’s results in Python. As a guideline, use my model summary above. I am ready to help where needed, but at this point in the course your experience should go a long way! If the simulation works, see if you can reproduce the following two figures:\n\n\n\n\n\n\n\n\n\n\n\n(a) Allies are maintained within a population of hosts (black line shows the average), despite each individual host (green thinner lines) constantly decreasing in ally types. This can be explained by selection at the level of the host.\n\n\n\n\n\n\n\n\n\n\n\n(b) Allies are not maintained within a population of hosts (black line shows the average) when transmission of microbiomes is horizontal (in between random individuals).\n\n\n\n\n\n\n\nFigure 17.2: Helper maintenance in hosts with different transmission modes",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Miniproject: maintaining a healthy microbiome</span>"
    ]
  },
  {
    "objectID": "project_2.html#extending-the-model",
    "href": "project_2.html#extending-the-model",
    "title": "17  Miniproject: maintaining a healthy microbiome",
    "section": "17.5 Extending the model",
    "text": "17.5 Extending the model\nNow that you have a working base-line model, let’s extend it to address the questions we phrased earlier: how does mobility of the host affect the evolution of microbiomes, and what about non-locally reproducing fungi? How do these host-level traits affect the likelyhood of “disease” outbreaks? Note that so far, we have only discussed “helpers” and “neutral” microbes, but the same principles apply to pathogens but perhaps a little more extreme. For example, the microbes may evolve such high levels of nastiness, that hosts do not only replicate slower, but die. Think about ways to extend the model that allows you to tune these distinctions.\n\n\n\n\nKoskella, Britt, and Joy Bergelson. 2020. “The Study of Host–Microbiome (Co) Evolution Across Levels of Selection.” Philosophical Transactions of the Royal Society B 375 (1808): 20190604.\n\n\nVan Vliet, Simon, and Michael Doebeli. 2019. “The Role of Multilevel Selection in Host Microbiome Evolution.” Proceedings of the National Academy of Sciences 116 (41): 20591–97.",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Miniproject: maintaining a healthy microbiome</span>"
    ]
  },
  {
    "objectID": "project_3.html",
    "href": "project_3.html",
    "title": "18  Miniproject: The devil is in the details – auxin transport in the root",
    "section": "",
    "text": "18.1 Mini description\nAuxin is a plant hormone that regulates a great diversity of processes in plants. Auxin can move between cells through the activity of membrane proteins acting as auxin efflux and influx transporters, allowing the coordination of cells. In the cells, auxin activates a signalling pathway in which a family of transcription factors, the auxin response factors (ARF), activate or repress gene expression. To study the role of auxin on plant development, experimentalists have found several ways to impinge on the different steps preceding auxin responses. For example, auxin levels can be increased using a synthetic auxin molecule (NAA) or by inhibiting the activity of the PIN transporters (N-1-naphthylphthalamic acid, NPA), while auxin responses can be actively elicited by interfering directly with components the auxin signalling pathway.\nIn the root, WOX5 regulation by auxin has been studied using these three strategies. The model we used in Practical 6 makes a rather coarse representation of the auxin treatments by increasing the auxin levels equally in all cells. Yet, the actual treatments made experimentally may affect auxin levels and responses in very different ways. Here you will extend the model we used before, by implementing each of these strategies and compare how each intervention modifies WOX5 expression in time and space.",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Miniproject: The devil is in the details – auxin transport in the root</span>"
    ]
  },
  {
    "objectID": "project_3.html#guiding-steps",
    "href": "project_3.html#guiding-steps",
    "title": "18  Miniproject: The devil is in the details – auxin transport in the root",
    "section": "18.2 Guiding steps",
    "text": "18.2 Guiding steps\nIncreasing auxin levels in the root apex\nImplement:\n\nthe effect of using a synthetic auxin molecule (NAA) whose transport is not universally PIN-dependent and thus it enhanced signal but no change in spatial pattern (Friml et al., 2013).\nthe inhibition of PIN efflux transporters when treating plants with NPA (Abas et al., 2020).\n\n• Simulate single NAA and NPA treatments, and combined NPA+IAA (natural auxin) treatments.\n• Evaluate in each case how WOX5 is regulated in time and space. Does this match the experiments?\n• Are these treatments equivalent?",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Miniproject: The devil is in the details – auxin transport in the root</span>"
    ]
  },
  {
    "objectID": "project_3.html#question-only-for-master-students",
    "href": "project_3.html#question-only-for-master-students",
    "title": "18  Miniproject: The devil is in the details – auxin transport in the root",
    "section": "18.3 Question only for master students",
    "text": "18.3 Question only for master students\nModifying auxin responses in the root apex\n\nModify the gene regulatory network function to include the negative feedback from WOX5 to auxin signaling pathway (Tian et al., 2014).\n\n• Simulate the experiments in Figure 2 of the paper. If necessary, propose and test new interactions.",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Miniproject: The devil is in the details – auxin transport in the root</span>"
    ]
  },
  {
    "objectID": "project_3.html#key-references",
    "href": "project_3.html#key-references",
    "title": "18  Miniproject: The devil is in the details – auxin transport in the root",
    "section": "18.4 Key references:",
    "text": "18.4 Key references:\n\nhttps://www.nature.com/articles/nature13663\nhttps://www.pnas.org/doi/10.1073/pnas.1000672107\nhttps://www.cell.com/molecular-plant/fulltext/S1674-2052(14)60288-4",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Miniproject: The devil is in the details – auxin transport in the root</span>"
    ]
  },
  {
    "objectID": "project_4.html",
    "href": "project_4.html",
    "title": "19  Miniproject: Patterning – Asymmetric localization of ZIF-1 in the C. elegans embryo",
    "section": "",
    "text": "19.1 Mini description\nIn the early C. elegans embryo, the first cell division of the zygote (P0 cell) is asymmetric. The anterior daughter forms the AB cell that gives rise to somatic tissues, while the posterior daughter forms the P1 cell that gives rise to both somatic and germline tissues. These different fates are a result of asymmetric segregation of proteins to the anterior and posterior sides of the P0 cell before division. In fact, at this stage in development, the embryo has no active transcription, but “survives” off of maternally-inherited mRNAs and proteins.\nAsymmetric protein distribution is orchestrated by the PAR polarity proteins. One of the affected proteins is MEX-5 which is phosphorylated by the posterior PAR protein PAR-1 (Griffin et al. 2011). Since PAR-1 is present only in the posterior half of the cell while the phosphatase PP2A is ubiquitous, the balance of phosphorylation and dephosphorylation of MEX-5 differs along the anterior-posterior axis. Importantly, phosphorylated MEX-5 diffuses faster. As a result, slow-diffusing unphosphorylated MEX-5 accumulates in the anterior part of the cell. This gradient-formation mechanism was also recapitulated by a mathematical model (Griffin et al. 2011).\nThe MEX-5 protein binds to the 3’ untranslated region of various mRNAs, thereby activating their translation. One of its targets is zif-1, the mRNA encoding for the ZIF-1 protein, a E3 ubiquitin ligase required to degrade factors that promote germline fate. To perform its function, ZIF-1 protein needs to be enriched anteriorly in the future AB cell, and needs to be excluded posteriorly in the future P1 cell. ZIF-1 is also known to inhibit MEX-5 by binding to it, resulting in a negative feedback on its own translation activation.\nOne mechanism for the specific localization of ZIF-1 protein to the future AB cell is local zif-1 mRNA translation in the anterior half of the cell via the action of MEX-5. But what prevents the ZIF-1 protein from simply diffusing into the posterior half of the cell (the future P1 cell)? There are several possible mechanisms, for example:\n##Key References\nGriffin, Erik E., David J. Odde, and Geraldine Seydoux. “Regulation of the MEX-5 gradient by a spatially segregated kinase/phosphatase cycle.” Cell 146.6 (2011): 955-968.\n##Guiding Questions\nIn this project, you will implement a model to explore the above hypothetical mechanisms of ZIF-1 localization. Consider the following:\nDo the proposed mechanisms qualitatively result in spatially-localized ZIF-1?\nIf yes, how steep is the resulting ZIF-1 gradient?\n[Master-level] How robust is the result to parameter variation?\n[Master-level] Compare the mechanisms: Are they identical? Do they make different predictions (e.g. if a mutant is engineered, or the size of the cell is changed)?",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Miniproject: Patterning – Asymmetric localization of ZIF-1 in the C. elegans embryo</span>"
    ]
  },
  {
    "objectID": "project_4.html#mini-description",
    "href": "project_4.html#mini-description",
    "title": "19  Miniproject: Patterning – Asymmetric localization of ZIF-1 in the C. elegans embryo",
    "section": "",
    "text": "ZIF-1 is bound to MEX-5 which accumulates anteriorly, thus depleting posterior ZIF-1.\nZIF-1 is specifically degraded in the posterior side.\nZIF-1 is actively transported towards the anterior side, e.g. using the cytoskeleton.",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Miniproject: Patterning – Asymmetric localization of ZIF-1 in the C. elegans embryo</span>"
    ]
  },
  {
    "objectID": "project_4.html#hints",
    "href": "project_4.html#hints",
    "title": "19  Miniproject: Patterning – Asymmetric localization of ZIF-1 in the C. elegans embryo",
    "section": "19.2 Hints",
    "text": "19.2 Hints\n\nStart by replicating the MEX-5 diffusion-gradient model of Griffin et al. 2011 [Reference #1].\nExtend the model to also include:\n\nzif-1 mRNA\nZIF-1 protein\nbinding of ZIF-1 to MEX-5 resulting in an inactive ZIF-1·MEX-5 complex\nTo simplify the model, start by assuming that zif-1 mRNA is uniformly distributed and does not diffuse.",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Miniproject: Patterning – Asymmetric localization of ZIF-1 in the C. elegans embryo</span>"
    ]
  },
  {
    "objectID": "project_5.html",
    "href": "project_5.html",
    "title": "20  Miniproject: Oncogenic propensity of different stem cell niches",
    "section": "",
    "text": "20.1 Mini description\nStem cells are present in our bodies, and their activity is important for the renewal of our tissues. For example, the turnover of our skin cells is ~30 days For instance, our tissues are renewed throughout our life thanks to the activity of stem cells housed in our skin, lung, intestine epithelia. The organization of stem cell niches is generic such that stem cells produce progeny that divides actively in the transit-amplifying zone, and then differentiates. Notably, different tissues in have a different propensity of developing cancer, and it has been that this is due to“a combination of stem cell mutagenesis and extrinsic factors that enhance the proliferation of these cell populations, creates a “perfect storm” that ultimately determines organ cancer risk” (Zhu et al., 2016). Here you will explore with a multicelular model what is the relation of differences in SC activity and cancer risk.\nThey will receive a line of cells that duplicate with the same probability. They have to implement differences in division and make a measure of cell production. Compare cell production with and without gradient of differentiation. Probabilities: 0.1, 0.7, 0.0. They should see it is better to have no gradient.\nThe model: I will prepare a simple 1D tissue-level model of a SCN, and let the students implement the division dynamics of different SCNs. Then quantify the propensity of different tissues to develop oncogenic mutations.",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Miniproject: Oncogenic propensity of different stem cell niches</span>"
    ]
  },
  {
    "objectID": "project_5.html#mini-description",
    "href": "project_5.html#mini-description",
    "title": "20  Miniproject: Oncogenic propensity of different stem cell niches",
    "section": "",
    "text": "Why? SCN organization is generic, but cycling of cells is different: Basis of model is that mutation probability is constant, and that division frequency is what changes in different tissues (see ref 1 below).\n\n\n\n\n\nTomasetti and Vogelstein, 2015. Science.\n\n\n\n\n\nZhu et al., 2016.",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Miniproject: Oncogenic propensity of different stem cell niches</span>"
    ]
  },
  {
    "objectID": "project_5.html#guiding-questions",
    "href": "project_5.html#guiding-questions",
    "title": "20  Miniproject: Oncogenic propensity of different stem cell niches",
    "section": "20.2 Guiding questions:",
    "text": "20.2 Guiding questions:\n\nImplement a mutation rate probability (0.01). Now quantify mutations per cell population, which one gets a higher production of oncogenic mutations?\nWhich zone has long-lasting effects?\nWhat happens with mutations occurring in SC and in TA?\nNow simulate different SC activities to mimic different tissues cell renewal. Plot SC activity vs mutagenesis. Does it match the plot of the paper?\nParameter sweep, is there a combination of division probabilities that can optimize cell production with minimal mutation rates? That is, minimal tumour probability?\nPropose how to decrease cancer incidence in this tissue-level model. One option is making divisions in TA more frequent than in SCs as long as overall tissue renewal is maintained (Paper proposing that cancer incidence decreases with age… they claim that tissues self-renew less frequently).\n\n\n\nMsc:\n\nExpand to 2D model, how it affects spreading?\nSimulate injuries and see how it relates to spreading of mutations?",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Miniproject: Oncogenic propensity of different stem cell niches</span>"
    ]
  },
  {
    "objectID": "project_5.html#references",
    "href": "project_5.html#references",
    "title": "20  Miniproject: Oncogenic propensity of different stem cell niches",
    "section": "20.3 References:",
    "text": "20.3 References:\n\nSC divisions and cancer susceptibility https://www.nature.com/articles/nature23302\nVariation in cancer risk among tissues can be explained by the number of stem cell divisions https://www.science.org/doi/full/10.1126/science.1260825\nZhu - SCs are more susceptible https://doi.org/10.1016/j.cell.2016.07.045 extra: Tomassetti. https://www.science.org/doi/10.1126/science.aaf9011",
    "crumbs": [
      "Mini-projects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Miniproject: Oncogenic propensity of different stem cell niches</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ben-Zvi, Danny, and Naama Barkai. 2010. “Scaling of Morphogen\nGradients by an Expansion-Repression Integral Feedback Control.”\nProceedings of the National Academy of Sciences 107 (15):\n6924–29.\n\n\nBulusu, Vinay, Nicole Prior, Marteinn T Snaebjornsson, Andreas Kuehne,\nKatharina F Sonnen, Jana Kress, Frank Stein, Carsten Schultz, Uwe Sauer,\nand Alexander Aulehla. 2017. “Spatiotemporal Analysis of a\nGlycolytic Activity Gradient Linked to Mouse Embryo Mesoderm\nDevelopment.” Developmental Cell 40 (4): 331–41.\n\n\nCarraco, Gil, Ana P Martins-Jesus, and Raquel P Andrade. 2022.\n“The Vertebrate Embryo Clock: Common Players Dancing to a\nDifferent Beat.” Frontiers in Cell and Developmental\nBiology 10: 944016.\n\n\nCheung, David, Cecelia Miles, Martin Kreitman, and Jun Ma. 2011.\n“Scaling of the Bicoid Morphogen Gradient by a Volume-Dependent\nProduction Rate.” Development 138 (13): 2741–49.\n\n\nCrombach, Anton, and Paulien Hogeweg. 2008. “Evolution of\nEvolvability in Gene Regulatory Networks.” PLoS Computational\nBiology 4 (7): e1000112.\n\n\nDornbusch, Tino, Séverine Lorrain, Dmitry Kuznetsov, Arnaud Fortier,\nRobin Liechti, Ioannis Xenarios, and Christian Fankhauser. 2012.\n“Measuring the Diurnal Pattern of Leaf Hyponasty and Growth in\nArabidopsis–a Novel Phenotyping Approach Using Laser Scanning.”\nFunctional Plant Biology 39 (11): 860–69.\n\n\nDriever, Wolfgang, and Christiane Nüsslein-Volhard. 1988. “The\nBicoid Protein Determines Position in the Drosophila Embryo in a\nConcentration-Dependent Manner.” Cell 54 (1): 95–104.\n\n\nFried, Patrick, and Dagmar Iber. 2014. “Dynamic Scaling of\nMorphogen Gradients on Growing Domains.” Nature\nCommunications 5 (1): 5077.\n\n\nHe, Feng, Chuanxian Wei, Honggang Wu, David Cheung, Renjie Jiao, and Jun\nMa. 2015. “Fundamental Origins and Limits for Scaling a Maternal\nMorphogen Gradient.” Nature Communications 6 (1): 6679.\n\n\nHerrgen, Leah, Saúl Ares, Luis G Morelli, Christian Schröter, Frank\nJülicher, and Andrew C Oates. 2010. “Intercellular Coupling\nRegulates the Period of the Segmentation Clock.” Current\nBiology 20 (14): 1244–53.\n\n\nHester, Susan D. 2012. “Multi-Scale Cell-Based Computational\nModels of Vertebrate Segmentation and Somitogenesis Illuminate\nCoordination of Developmental Mechanisms Across Scales.” PhD\nthesis, Indiana University.\n\n\nInomata, Hidehiko. 2017. “Scaling of Pattern Formations and\nMorphogen Gradients.” Development, Growth &\nDifferentiation 59 (1): 41–51.\n\n\nIshimatsu, Kana, Tom W Hiscock, Zach M Collins, Dini Wahyu Kartika Sari,\nKenny Lischer, David L Richmond, Yasumasa Bessho, Takaaki Matsui, and\nSean G Megason. 2018. “Size-Reduced Embryos Reveal a Gradient\nScaling-Based Mechanism for Zebrafish Somite Formation.”\nDevelopment 145 (11): dev161257.\n\n\nKoskella, Britt, and Joy Bergelson. 2020. “The Study of\nHost–Microbiome (Co) Evolution Across Levels of Selection.”\nPhilosophical Transactions of the Royal Society B 375 (1808):\n20190604.\n\n\nLewis, Julian. 2003. “Autoinhibition with Transcriptional Delay: A\nSimple Mechanism for the Zebrafish Somitogenesis Oscillator.”\nCurrent Biology 13 (16): 1398–408.\n\n\nMichaud, Olivier, Anne-Sophie Fiorucci, Ioannis Xenarios, and Christian\nFankhauser. 2017. “Local Auxin Production Underlies a Spatially\nRestricted Neighbor-Detection Response in Arabidopsis.”\nProceedings of the National Academy of Sciences 114 (28):\n7444–49.\n\n\nOostrom, Marek J van, Yuting I Li, Wilke HM Meijer, Tomas EJC Noordzij,\nCharis Fountas, Erika Timmers, Jeroen Korving, Wouter M Thomas, Benjamin\nD Simons, and Katharina F Sonnen. 2025. “Scaling of Mouse\nSomitogenesis by Coupling of Cell Cycle to Segmentation Clock\nOscillations.” bioRxiv, 2025–01.\n\n\nOskam, Lisa, Basten L Snoek, Chrysoula K Pantazopoulou, Hans van Veen,\nSanne EA Matton, Rens Dijkhuizen, and Ronald Pierik. 2023. “A\nLow-Cost and Open-Source Imaging Platform Reveals Spatiotemporal Insight\ninto Arabidopsis Leaf Elongation and Movement.” BioRxiv,\n2023–08.\n\n\nPraat, Myrthe, Zhang Jiang, Joe Earle, Sjef Smeekens, and Martijn van\nZanten. 2024. “Using a Thermal Gradient Table to Study Plant\nTemperature Signalling and Response Across a Temperature\nSpectrum.” Plant Methods 20 (1): 114.\n\n\nSaga, Yumiko. 2012. “The Mechanism of Somite Formation in\nMice.” Current Opinion in Genetics & Development 22\n(4): 331–38.\n\n\nSeki, Motohide, Takayuki Ohara, Timothy J Hearn, Alexander Frank,\nViviane CH Da Silva, Camila Caldana, Alex AR Webb, and Akiko Satake.\n2017. “Adjustment of the Arabidopsis Circadian Oscillator by Sugar\nSignalling Dictates the Regulation of Starch Metabolism.”\nScientific Reports 7 (1): 8305.\n\n\nSonnen, Katharina F, Volker M Lauschke, Julia Uraji, Henning J Falk,\nYvonne Petersen, Maja C Funk, Mathias Beaupeux, Paul François, Christoph\nA Merten, and Alexander Aulehla. 2018. “Modulation of Phase Shift\nBetween Wnt and Notch Signaling Oscillations Controls Mesoderm\nSegmentation.” Cell 172 (5): 1079–90.\n\n\nSoroldoni, Daniele, David J Jörg, Luis G Morelli, David L Richmond,\nJohannes Schindelin, Frank Jülicher, and Andrew C Oates. 2014. “A\nDoppler Effect in Embryonic Pattern Formation.” Science\n345 (6193): 222–25.\n\n\nStickney, Heather L, Michael JF Barresi, and Stephen H Devoto. 2000.\n“Somite Development in Zebrafish.” Developmental\nDynamics: An Official Publication of the American Association of\nAnatomists 219 (3): 287–303.\n\n\nTam, PPL. 1981. “The Control of Somitogenesis in Mouse\nEmbryos.” Development 65 (Supplement): 103–28.\n\n\nTomka, Tomas, Dagmar Iber, and Marcelo Boareto. 2018. “Travelling\nWaves in Somitogenesis: Collective Cellular Properties Emerge from\nTime-Delayed Juxtacrine Oscillation Coupling.” Progress in\nBiophysics and Molecular Biology 137: 76–87.\n\n\nVan Vliet, Simon, and Michael Doebeli. 2019. “The Role of\nMultilevel Selection in Host Microbiome Evolution.”\nProceedings of the National Academy of Sciences 116 (41):\n20591–97.\n\n\nWolpert, Lewis. 1969. “Positional Information and the Spatial\nPattern of Cellular Differentiation.” Journal of Theoretical\nBiology 25 (1): 1–47.",
    "crumbs": [
      "References"
    ]
  }
]