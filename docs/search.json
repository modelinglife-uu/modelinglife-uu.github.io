[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modeling Life",
    "section": "",
    "text": "Modeling life\nThis website accompanies the Modeling Life course at Utrecht University. It primarily serves as a central hub for all the practicals (werkcolleges), with the necessarily files, links and other resources for each day.\nEach practical has its own page containing background explanations, code snippets, and questions that are designed for you to learn about the models. Most of these exercises build directly on the lectures, allowing you to explore biological questions‚Äîsuch as how morphogen gradients form, how spatial patterns emerge, or how cells evolve to ‚Äústick together‚Äù.\nYou can navigate the site using the sidebar or the left. The General Course Info section outlines the general course info (exams, learning goals, etc.). You can also find the Schedule on this website. The individual practicals sections provides detailed instructions for each day. In the second part of the course you will get even more experience doing things yourself by working on a mini-project.\nWe hope you‚Äôll use this website actively. There‚Äôs lots to read, simulate, modify, and explore.\nNote: this website is to help you, but is by no means perfect. Please let us know if you see any mistakes, typo‚Äôs or other issues. Any constructive feedback on how to make things better and easier for you is always welcome.",
    "crumbs": [
      "Course information",
      "Modeling life"
    ]
  },
  {
    "objectID": "general.html",
    "href": "general.html",
    "title": "General course info",
    "section": "",
    "text": "Contact\nCourse coordinator: Monica Garcia Gomez (m.l.garciagomez@uu.nl). In this website you will find all practical information about the course. You can reach the instructors via email at modelinglife@uu.nl .\nWe are part of the Theoretical Biology group of Utrecht University).",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#contact",
    "href": "general.html#contact",
    "title": "General course info",
    "section": "",
    "text": "Teachers: Kirsten ten Tusscher (K.H.W.J.tenTusscher@uu.nl), Erika Tsingos (e.tsingos@uu.nl),Monica Garcia Gomez (m.l.garciagomez@uu.nl), Bram van Dijk (b.vandijk@uu.nl).",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#course-content",
    "href": "general.html#course-content",
    "title": "General course info",
    "section": "Course content",
    "text": "Course content\nThis course will consist of lectures (HC) covering various modelling approaches to study life at different levels accompanied by computer practicals (WC) where students will get hands-on experience on running computational models of the development or the evolution of animals, plants, and microbial systems.\n\n\n\nRough roadmap of the Modeling life course",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#general-schedule",
    "href": "general.html#general-schedule",
    "title": "General course info",
    "section": "General schedule:",
    "text": "General schedule:\nMonday ‚Äì Guest lectures (mandatory)\nTuesdays / Thursdays‚Äì Lecture and practicals (mandatory)\n\nKey dates:\n\nDecember 18, 2025: Exam\nJan 6, 2026: Miniprojects presentations\nJan 29, 2026: Minisymposium",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#learning-goals",
    "href": "general.html#learning-goals",
    "title": "General course info",
    "section": "Learning Goals",
    "text": "Learning Goals\nThe course aims to provide you with an introductory understanding of computational modeling to study living systems, and their large range of applications.\nAfter this course the student can:\n\nExplain how biological systems can be studied with computational models,\nTranslate a biological system into a working computational model using python,\nUse algorithmic thinking to break down problems into programmable steps,\nIdentify the underlying assumptions and limitations of computational models,\nIdentify the modeling approach & formalism best suited for a research question.",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#communication",
    "href": "general.html#communication",
    "title": "General course info",
    "section": "Communication",
    "text": "Communication\nFeedback and other matters: m.l.garciagomez@uu.nl\nSchedule, slides, practicals: https://modelinglife-uu.github.io/\nCode: https://github.com/moneralee/UU_Modeling-Life-course\nBrightspace: is the channel for all official communications (e.g. final grades).\nModeling Life 2025 in Microsoft Teams. You can sign up as follows:\n\nOpen MS Teams\nIn the menu on the left, select the ‚ÄúTeams‚Äù icon\nClick the ‚ÄúJoin or create a team‚Äù button at the bottom left of the screen (or, depending on the Teams version, at the top right)\nFind the tile that says ‚ÄúJoin a team with a code‚Äù\nEnter the following code in this field: 6jhv6z5\nSelect ‚ÄúJoin team‚Äù\nTeams is not used to livestream lectures. However, if you have any questions for the instructors, student assistants, or your fellow students, it‚Äôs very helpful to share them on Teams (General channel) so everyone can benefit.",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#grading",
    "href": "general.html#grading",
    "title": "General course info",
    "section": "Grading",
    "text": "Grading\nTo pass this course, a minimum of 5,5 is mandatory. Your grade is calculated by the following components:\nExam: 70%\nMini project: 30%\nYou will find your Final grade on Brigthspace.",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#attendance",
    "href": "general.html#attendance",
    "title": "General course info",
    "section": "Attendance",
    "text": "Attendance\nAttendance is mandatory to all lectures and practicals. Should you be unable to attend, please communicate this by email to: m.l.garciagomez@uu.nl.",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "general.html#feedback",
    "href": "general.html#feedback",
    "title": "General course info",
    "section": "Feedback",
    "text": "Feedback\nPlease help us improve the course by providing feedback via Caracal and other means. We want to make this new course as good as possible for you and future students.\nThis website is to help you, but is by no means perfect. Please let us know if you see any mistakes, typo‚Äôs or other issues. Any constructive feedback on how to make things better and easier for you is always welcome.",
    "crumbs": [
      "Course information",
      "General course info"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week 1\nNov 10, 2025 ‚Äì Welcome and Intro to Python (HC 13:15-17:00).\nNov 11, 2025 ‚Äì Pattern formation I: Gradients and segments (HC 10:00-12:45 and WC1 13:15-17:00).\nNov 13, 2025 ‚Äì Pattern formation II: Turing patterns (HC 10:00-12:45 and WC2 13:15-17:00).",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-2",
    "href": "schedule.html#week-2",
    "title": "Schedule",
    "section": "Week 2",
    "text": "Week 2\nNov 17, 2025 ‚Äì Guest lecture: Max Rietkerk, Ecosystem‚Äôs spatial patterns (HC 13:15-17:00).\nNov 18, 2025 ‚Äì Pattern formation III: Clock and Wavefront (HC 10:00-12:45 and WC3 13:15-17:00).\nNov 20, 2025 ‚Äì Morphogenesis: Cell sorting (HC 10:00-12:45 and WC4 13:15-17:00).",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-3",
    "href": "schedule.html#week-3",
    "title": "Schedule",
    "section": "Week 3",
    "text": "Week 3\nNov 24, 2025 ‚Äì Guest lecture: Ina Sonnen, Signal encoding in multicellular systems (HC 13:15-17:00).\nNov 25, 2025 ‚Äì Cell differentiation: gene regulation in time (HC 10:00-12:45 and WC5 13:15-17:00).\nNov 27, 2025 ‚Äì Cell differentiation: gene regulation in space (HC 10:00-12:45 and WC6 13:15-17:00).",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-4",
    "href": "schedule.html#week-4",
    "title": "Schedule",
    "section": "Week 4",
    "text": "Week 4\nDec 1, 2025 ‚Äì Guest lectures: Vivek Bhardwaj and Kaisa Kajala, What is a cell type? genomic and functional perspectives (HC 13:15-17:00).\nDec 2, 2025 ‚Äì Environment and Development (HC 10:00-12:45 and WC7 13:15-17:00).\nDec 4, 2025 ‚Äì Evolving populations I: Sticky cells (HC 10:00-12:45 and WC8 13:15-17:00).",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-5",
    "href": "schedule.html#week-5",
    "title": "Schedule",
    "section": "Week 5",
    "text": "Week 5\nDec 8, 2025 ‚Äì Guest lecture: Rutger Hermsen (HC 13:15-17:00).\nDec 9, 2025 - Evolving populations II: Genotype-phenotype map (HC 10:00-12:45 and WC9 13:15-17:00).\nDec 11, 2025 - Evolving populations III: Microbial communities (HC 10:00-12:45 and WC10 13:15-17:00).",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-6-self-study-and-exam",
    "href": "schedule.html#week-6-self-study-and-exam",
    "title": "Schedule",
    "section": "Week 6 (self-study and exam)",
    "text": "Week 6 (self-study and exam)\nDec 18, 2025 ‚ÄìExam",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-7-9-mini-projects",
    "href": "schedule.html#week-7-9-mini-projects",
    "title": "Schedule",
    "section": "Week 7-9 (mini projects)",
    "text": "Week 7-9 (mini projects)\nJan 6, 2026 - Miniprojects presentation and making teams\nThe following weeks you will have classrooms available to work on your miniprojects (check in myTimetable, 13:15-17:00). Also, you should schedule meetings with your supervisor to discuss progress of your miniproject.",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-10-mini-symposium",
    "href": "schedule.html#week-10-mini-symposium",
    "title": "Schedule",
    "section": "Week 10 (mini symposium)",
    "text": "Week 10 (mini symposium)\nJan 29, 2026 - Miniprojects final presentation",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "intro_to_python.html",
    "href": "intro_to_python.html",
    "title": "0) Introduction to Python",
    "section": "",
    "text": "Installing Spyder and Anaconda\nThis introductory page explains how to install Python and Spyder, and provides a brief overview of Python programming. You will also have access codes to a great StudyLens app to quickly get a grasp of Python programming, especially the parts directly relevant to this course.\nTo best way to install Spyder is to do so via Anaconda, which is a free and open-source distribution of Python for scientific computing. You can download Anaconda from this page (note: use the download link on the left, and not the one called ‚Äúminiconda‚Äù which does not include Spyder). After downloading, follow the installation instructions.\nOnce done, you now have:",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#installing-spyder-and-anaconda",
    "href": "intro_to_python.html#installing-spyder-and-anaconda",
    "title": "0) Introduction to Python",
    "section": "",
    "text": "Python\nSpyder (our main editor)\nMost scientific packages like numpy, matplotlib, and pandas\nConda and pip (for if you want to install other packages)",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#launching-spyder",
    "href": "intro_to_python.html#launching-spyder",
    "title": "0) Introduction to Python",
    "section": "Launching Spyder",
    "text": "Launching Spyder\nYou can now launch Spyder via the ‚ÄòAnaconda Navigator‚Äô application. In MacOS you find this under ‚ÄòApplications‚Äô &gt; ‚ÄòAnaconda-Navigator‚Äô (it doesn‚Äôt always show up in the Spotlight search immediately). In Windows, you can search for ‚ÄòAnaconda Navigator‚Äô in the Start Menu. Once the Anaconda Navigator is open, you can launch Spyder by clicking on the ‚ÄòLaunch‚Äô button under the Spyder icon.",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#disabling-inline-plots",
    "href": "intro_to_python.html#disabling-inline-plots",
    "title": "0) Introduction to Python",
    "section": "Disabling inline plots",
    "text": "Disabling inline plots\nBy default, Spyder shows figures inside the ‚ÄúPlots‚Äù pane, but in this course we typically use plots that update dynamically (animations), which don‚Äôt work well in that mode. So we need to change that.\nHere‚Äôs how:\n\nClick the ‚Äòpreferences‚Äô icon in the top panel (a wrench icon üîß).\nGo to ‚ÄòIPython Console‚Äô ‚Üí Graphics ‚Üí Backend\nChoose QT, QT5, or QT6 (whichever is available)\nClick Apply and OK\nRestart Spyder may or may not be necessary depending on your operating system.\n\nNow your plots will open in a separate window and can animate properly!",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#testing-your-setup",
    "href": "intro_to_python.html#testing-your-setup",
    "title": "0) Introduction to Python",
    "section": "Testing your setup!",
    "text": "Testing your setup!\nThroughout this course you will either work with scripts you have been handed out, or scripts that you can copy/paste from this website. Let‚Äôs test your Spyder setup with the following script:\nimport random, matplotlib.pyplot as plt\n\nplt.ion()\nfig, ax = plt.subplots()\nax.set_ylim(-2, 2)\nax.set_xlim(0, 100)\nax.set_title(\"Random Wiggle Test\")\nline, = ax.plot([], [], color='seagreen', lw=2)\n\ny = [0]\nfor i in range(100):\n    y.append(y[-1] + random.uniform(-0.1, 0.1))  # random wiggle\n    line.set_data(range(len(y)), y)\n    ax.set_xlim(0, len(y))\n    plt.pause(0.03)\n\nax.set_title(\"Animation works! (you can close this window)\")\nplt.ioff()\nplt.show()\nPaste this into a new script in Spyder and hit the ‚Äòplay‚Äô icon (or press F5). Does the animation show up in a seperate window and is it animated? Good, you‚Äôre ready to go!",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#variables-and-data-types",
    "href": "intro_to_python.html#variables-and-data-types",
    "title": "0) Introduction to Python",
    "section": "Variables and Data Types",
    "text": "Variables and Data Types\nIn Python, variables are used to store data. You can create a variable by assigning a value to it using the = operator. For example:\nx = 10\ny = \"Hello, World!\"\nPython has many built-in data types, including:\n\nIntegers (int): Whole numbers, e.g., 10, -5\nFloating-point numbers (float): Decimal numbers, e.g., 3.14, -0.001\nStrings (str): Text, e.g., \"Hello\", 'Python'\nBooleans (bool): True or False values, e.g., True, False\nLists (list): Ordered collections of items, e.g., [1, 2, 3], ['a', 'b', 'c']\nDictionaries (dict): Key-value pairs, e.g., {'name': 'Alice', 'age': 25}\nAnd many more",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#control-structures",
    "href": "intro_to_python.html#control-structures",
    "title": "0) Introduction to Python",
    "section": "Control Structures",
    "text": "Control Structures\nControl structures allow you to control the flow of your program. Common control structures in Python include:\n\nConditional statements (if, elif, else):\n\nif x &gt; 0:\n    print(\"x is positive\")\nelif x == 0:\n    print(\"x is zero\")\nelse:\n    print(\"x is negative\")\n\nLoops (for, while):\n\nfor i in range(5):\n    print(i)\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#functions",
    "href": "intro_to_python.html#functions",
    "title": "0) Introduction to Python",
    "section": "Functions",
    "text": "Functions\nFunctions are reusable blocks of code that perform a specific task. You can define a function using the def keyword:\ndef greet(name):\n    return f\"Hello, {name}!\"\n    \nprint(greet(\"Alice\"))",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#libraries",
    "href": "intro_to_python.html#libraries",
    "title": "0) Introduction to Python",
    "section": "Libraries",
    "text": "Libraries\nPython has a rich ecosystem of libraries that extend its functionality. Most scientific packages are shipped with Anaconda, so you don‚Äôt need to worry about these. These include:\n\nNumPy: For numerical computing\nPandas: For data manipulation and analysis\nMatplotlib: For data visualization\nSciPy: For scientific computing\n\nBut if you still wish to install other packages, you can use pip or conda in the terminal. For example, to install the html5 library using pip, you would run:\npip install html5lib\nFrom the command line. You can also run this bit of code within the Spyder console by adding an ! in front of the command, which tells the console to run it as a shell command:\n!pip install html5lib\nBut you probably won‚Äôt have to install a lot of packages during this course. (nearly) All scripts we use only rely on the base packages.",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "intro_to_python.html#studylens-practice",
    "href": "intro_to_python.html#studylens-practice",
    "title": "0) Introduction to Python",
    "section": "StudyLens practice",
    "text": "StudyLens practice\nIf you‚Äôve installed Python and read through the basics of Python above, it‚Äôs time to dive into the StudyLens exercises. For this, login to StudyLens and use the username and password that was shared with you on Brightspace.",
    "crumbs": [
      "0) Introduction to Python"
    ]
  },
  {
    "objectID": "pattern_intro_text.html",
    "href": "pattern_intro_text.html",
    "title": "1¬† Pattern formation (intro)",
    "section": "",
    "text": "The field of pattern formation studies the mechanisms that underly the formation of spatial patterns in biological systems. Patterns may arise at any biological level of organization, from single cells polarizing to decide in which direction to move, grow or divide, to the formation of body axes, different cell types and shapes that set apart complex multi-cellular organisms from amorphous blobs, to entire ecosystems patterning where e.g.¬†plants do and do not grow. An important concept in pattern formation is so called ‚Äúsymmetry breaking‚Äù, which refers to the destruction of an originally homogeneous or non-patterned state, to a non-homogeneous patterned state (see figure below).\n\n\n\n\n\nFocusing on multi-cellular development, pattern formation addresses how within an organism in which all cells (except for germ cells that have undergone meiosis and immune cells applying VJ recombination) share the same genetic material ‚Äúsymmetry is broken‚Äù resulting in usage of a different subset of genes and functions by different cells. Symmetry breaking is needed for the creation of body axes, domains with different functions as well as repeating elements.\nA limited range of mechanisms leading to symmetry breaking exist, which have been used time and again by evolution to create patterns in animals, plants, fungi and multicellular algae. Major mechanisms are morphogen gradients, where graded distribution of a signal provides distinct input to cells parallel to the gradient allowing both regionalization and segmentation; Turing patterns, where initial noise combined with diffusion induced destabilization lead to repetitive patterns, and clock-and-wavefront patterning where autonomous oscillations combined with growth and a memorization mechanism provide an alternative means for segmenting a tissue. We will explore these 3 mechanisms in the practicals. In addition, in plants self-organized patterning of auxin transporters underly phyllotaxis (the positioning of new leaf organs at the shoot apex) and leaf venation. Note that this list of symmetry breaking mechanisms is not exhaustive and additional mechanisms such as lateral inhibition and planar cell polarity exist.\n\n\n\n\n\n\nNoteLinks throughout part I-V\n\n\n\nAlthough this course is divided into 5 clearly distinct topics, there is also substantial overlap. We challenge you to see how all sections use similar concepts, and to think about how different types of models may even be combined. To help you along with this integrated view of modelling biology, we will below discuss some of the links with future topics. It may be a good idea to go back to this text later in the course, and reflect if you indeed see all the links.\n\n\nOften, initial signals like gradients and clocks are transient, implying that the patterns they induce require additional mechanisms of memorization. Critical for understanding this memorization process is the concept of multistability, where two or more alternative stable states of the system exist and the initial signals bring the system from the original to a new patterned state. This concept will be further explored in Part III of the course which focuses on Differentiation. Of course, to form a multicellular organism with a functional shape, simply telling a blob of cells where the head or tail needs to be or which cells will become finger bones and which cells will apoptose to carve out the tissue between the fingers is insufficient, and actual shape changing processes need to occur. This we will discuss in Part II.\nParticularly for animal development, simply breaking symmetry and stably memorizing formed patterns is not enough, scaling of the pattern with body plan size and robustness against noise from gene expression, cell division, cell signalling and other processes is essential for fitness. In contrast, in plants developmental plasticity, the potential to adjust developmental patterning to environmental conditions, plays a key role. This latter concept will be discussed in Part IV of the course on Environment.\nThe repeated usage of a limited number of possible symmetry breaking mechanisms also raises interesting evolutionary questions (Part V). Are there indeed only a limited number of options, or did evolution select for specific mechanisms that are more robust or more evolvable? Or are some mechanisms simply easier to find? This way of evolutionary thinking will be further discussed in Part V.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Pattern formation (intro)</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html",
    "href": "pattern_practical_1.html",
    "title": "2¬† Gradients and segments",
    "section": "",
    "text": "2.1 Morphogen Gradients and Patterning\nIn this practical, we a going to look at how an organism can form segments along its body axis . The mathematical model that we will implement and study is an implementation of the so-called French flag conceptual model first proposed by Lewis Wolpert (Wolpert 1969). It assumes the spatially graded expression of a morphogen ‚ÄúM‚Äù that influences the expression of some downstream genes A, B and C. Their expression is often visualized by red, white and blue and the arising pattern resembles the French flag, hence the name (see the power of visualization).\nOne of the most well-studied organisms when it comes to body axis segmentation (although its segmentation mechanism is evolutionary derived and a-typical!) is the development of the fruit fly Drosophila melanogaster. Supporting the ideas of Wolpert, it was found that through tethering maternal Bicoid mRNA to one side of the embryo, Bicoid protein can form a gradient extending along the anterior-posterior axis, with so called gap genes as a first tier in the segmentation hierarchy differentially responding to different Bicoid protein levels (Driever and N√ºsslein-Volhard 1988). Later it was found that often at least two opposing morphogen gradients drive downstream gene expression and genes typically respond to multiple inputs.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html#mathematical-modeling---integrating-multiple-signals",
    "href": "pattern_practical_1.html#mathematical-modeling---integrating-multiple-signals",
    "title": "2¬† Gradients and segments",
    "section": "2.2 Mathematical modeling - integrating multiple signals",
    "text": "2.2 Mathematical modeling - integrating multiple signals\nPromotors/enhancers driving gene expression frequently make use of so called OR and AND gates to integrate inputs from different transcription factors. An OR gate can be implemented mathematically with a sum of the effect of the transcription factors, while an AND can be implemented mathematically with a product. Some examples:\n\n\\(\\frac{dX}{dt} = a(\\text{tf1}) + b(\\text{tf2})\\): Gene X is induced by transcription factor 1 and 2 in an OR fashion, either \\(a(\\text{tf1})\\) or \\(b(\\text{tf2})\\) needs to be high to give high transcription of X.\n\\(\\frac{dY}{dt} = a(\\text{tf1})\\cdot b(\\text{tf2})\\): Gene Y is induced by transcription factor 1 and 2 in an AND fashion, both \\(a(\\text{tf1})\\) and \\(b(\\text{tf2})\\), which are being multiplied, need to be high to give high transcription of X.\n\nNote that the shape of \\(a(\\text{tf1})\\) and \\(b(\\text{tf2})\\) (increasing or decreasing function of the transcription factor) determines whether tf1 and tf2 are repressing or activating.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html#python-code",
    "href": "pattern_practical_1.html#python-code",
    "title": "2¬† Gradients and segments",
    "section": "2.3 Python code",
    "text": "2.3 Python code\n\n\n\n\n\n\nNoteStarting code for this practical\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nLx = 40.0  # Length of the domain in x in microm\nLy = 10.0  # Length of the domain in y in microm\nT = 200  # Total time in seconds\ndx = 0.5  # Grid spacing in x\ndt = 0.1  # Time step\nnx = int(Lx/dx)+2  # Number of grid points in x + padding grid points\nny = int(Ly/dx)+2  # Number of grid points in y + padding grid points\n# Padding grid points to account for boundary conditions\nnt = int(T/dt)  # Number of time steps\nD = 0.4  # Diffusion coefficient in mm^2/s\ndecayM =0.01 # Decay rate in 1/s\n\n\n# Parameters for A, B, C\n... # TODO create parameters for A, B, C as needed in Q5\n\n# Stability criterion\nif D * dt / dx**2 &gt; 0.5:\n    raise ValueError(\"Stability criterion not met\")\n\n# A, B and C are required for later exercises.\nA = np.zeros((nx, ny))\nB = np.zeros((nx, ny))\nC = np.zeros((nx, ny))\n\n# Initial condition\nu = np.zeros((nx, ny))\nu[0, :] = 100\n\n# Reaction-diffusion equation\ndef reaction_diffusion_step(u, D, dt, dx, decay):\n    un = u.copy()\n    u[1:-1, 1:-1] = un[1:-1, 1:-1] +  D *dt / dx**2 * (un[2:, 1:-1] + un[:-2, 1:-1] + \\\n                    un[1:-1, 2:]  + un[1:-1, :-2] - 4 * un[1:-1, 1:-1]) - \\\n                    decay * un[1:-1, 1:-1] * dt\n    ## for loop version to understand the equation\n    # for i in range(1, nx-1):\n    #     for j in range(1, ny-1):\n    #         u[i, j] = (un[i, j] +\n    #                    D * dt / dx**2 * (un[i+1, j] + un[i-1, j] - 2 * un[i, j] +\n    #                    un[i, j+1] + un[i, j-1] - 4 * un[i, j]) - decay * un[i, j] * dt)\n    #boundary conditions\n    u[-1, :] = (u[-2, :]/u[-3, :])*u[-2, :]  if sum(u[-3, :]) != 0 else np.zeros(ny)\n    #to understand this line:\n    #if sum(u[-3, :]) != 0:\n    #    u[-1, :] = (u[-2, :]/u[-3, :])*u[-2, :]#extrapolate from third to last row\n    #else:\n    #    u[-1, :] = np.zeros(ny) #if already zero in third to last row, also zero in last row\n    u[:, 0] = u[:, 1]\n    u[:, -1] = u[:, -2]\n\n    return u\n\ndef reaction_diffusion_gradient(t, u, D, dx, decay, switch_time = None, noise = False):\n    '''\n    Function to create a gradient in the u array that could decay after a certain time.\n    t: current time step\n    u: array to create the gradient in\n    D: diffusion coefficient\n    dx: grid spacing\n    decay: decay rate\n    switch_time: time step after which the gradient decays. If no switch is desired, set to None\n    noise: whether to add noise to the gradient\n    '''\n    # TODO for student: write code for the noise and the switch.\n    added_noise = np.zeros_like(u)  # Initialize noise array\n    if noise:\n        ...  # TODO: add noise generation code here for Q10\n    \n    if switch_time is None or t &lt;= switch_time:\n        # define a exponential decay gradient over the array in the x direction with numpy array operations using the index\n        for i in range(u.shape[0]):\n            u[i, :] = np.maximum(100 * np.exp(-i*dx/np.sqrt(D/decay))+added_noise[i, :], 0)\n        return u\n    if t &gt; switch_time:\n        ...# TODO Q7: implement a gradient that decays over time, otherwise return the original u array\n        return u\n    # In all other cases, return the original u array        \n    return u\n\ndef hill(x, Km, pow):\n    \"\"\"Hill function for the reaction kinetics.\"\"\"\n    return (x**pow) / (Km**pow + x**pow) \n\ndef ihill(y, Km, pow):\n    \"\"\"Inverse Hill function for the reaction kinetics.\"\"\"\n    return( (Km**pow) / (y **pow  + Km**pow))\n\n# TODO for student: write update functions for A, B, C as needed in Q5\n\n\n# initilize figure and axes for plotting\n# TODO for student: Add a new axis for the ABC flag visualization as suggested in Q5\nfig, (ax_M, ax_lines) = plt.subplots(2, figsize=(10, 8), gridspec_kw={'height_ratios': [3, 1]})  # Make the first graph 3 times the height of the second\n\n# Time-stepping simulation loop\nfor n in range(nt):\n    # Update all variables\n    u = reaction_diffusion_step(u, D, dt, dx, decayM)\n    # TODO for student: use precomputed gradient, update A, B, C as needed in Q5\n    \n    if n == 0:  # Initial plot\n        imshow_M_plot = ax_M.imshow(u.T, cmap='viridis', origin='lower', aspect='auto')\n        ax_M.set_title(f\"Time: {n*dt}\")\n        ax_M.set_xlabel('x direction')\n        ax_M.set_ylabel('y direction')\n        ax_M.set_xticks([])\n        ax_M.set_yticks([])\n\n        # Plot the concentration at a specific y index (e.g., y=2)    \n        line_plot = ax_lines.plot([x*dx for x in range(nx)], u[:, 2], label='M', color='green')\n        # TODO: Add lines for A, B, C as needed in Q5\n\n        \n        ax_lines.legend(loc='upper right')\n        ax_lines.set_ylim(0, 100)\n        ax_lines.tick_params(axis='y')\n        ax_lines.set_xlim(0, dx*nx)\n        ax_lines.set_xlabel('x')\n        ax_lines.set_ylabel('Concentration at y=2')\n        ax_lines.tick_params(axis='x')\n\n    if n % 20 == 0:  # Update plot every so many time steps\n        #update the imshow M plot with the new data\n        imshow_M_plot.set_data(u.T)\n        ax_M.set_title(f\"Time: {n*dt}\")\n            \n        # Update the line plots with new data\n        line_plot[0].set_ydata(u[:, 2])\n        # TODO: Update A, B, C line plots as needed in Q5\n\n    plt.pause(0.001)  # Pause to refresh the plot\n\n# And keep the last plot open\n# plt.show()\n\n# Or close the plot window when done\nplt.close(fig)",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html#questions",
    "href": "pattern_practical_1.html#questions",
    "title": "2¬† Gradients and segments",
    "section": "2.4 Questions",
    "text": "2.4 Questions\n\nExercise 2.1 (Algorithmic thinking) Have a look at the reaction-diffusion equation for the morphogen and the implementation of it in the file morphogengradient_to_segments.py in the reaction_diffusion_step function, as well as at the initialization of the array u. How is this different from the gradient formation modeling we discussed during the lecture? What is done at the terminal boundary in the length direction and why does this make sense? (hint: outcommented we provided code doing essentially the same but not using numpy arrays and hence written in a less compact matter to help you understand what is happening)\n\n\nAnswer An important difference relative to the lecture is that instead of implementing a production term at the x=0 position, we instead fixed the concentration of u at x=0 at 100 and have nowhere a production term. This implies that we assume that production is very high and different D and d values have negligible effects on the level at x=0. The advantage of this approach is that we can easily study what different D and d does, as it only affects the shape of the gradient but not the maximum.   Instead of using if/else to check for boundary conditions, we only compute the Laplacian in the inner, non-boundary points. For the x=0 boundary we already set a constant value. For y=0 and y=n‚àí1 we copy the values from y=1 and y=n‚àí2 as the patterning is identical in that direction. For x=n‚àí1 we use a special approach: if n‚àí3=0 we assume it is zero; otherwise, we calculate the ratio of x at n‚àí2 and n‚àí3 to approximate the decay and multiply this with the value at x=n‚àí2 to estimate x at n‚àí1. Simply copying x at n‚àí2 would add matter to the system and prevent the gradient from reaching a steady state.\n\n\nExercise 2.2 (Important concept) Play with the morphogen diffusion rate and the morphogen decay rate and describe what happens. What happens in terms of dynamics and steady state if you change both, but the ratio stays the same? Hint: it might help to draw a horizontal line at a certain height to ease comparison.\n\n\nDiffusion length concept =‚àö(D/d), it is the ratio of these factors that determines the length scale lambda over which the morphogen concentration decays an e-fold. So with the same ratio, you get the same steady state gradient. However, it will still approach the steady state faster.\n\n\nExercise 2.3 (Biology & mathematical thinking) Next, we are going to introduce the genes A, B and C in the model. We want these genes to be expressed dependent on M, and in the head, trunk and tail respectively, so A on the left, B in the middle and C on the right. Think of what conditions in terms of M should lead to expression of A/B/C. What Hill functions (normal/inverse/any combination) corresponds to those conditions? Write down (pen and paper, not in code) full equations for the genes, do we need any other terms than just Hill functions, would we need specific parameter conditions?\n\n\nAnswer  \\(dA/dt\\) = normal hill (M) ‚Äì decay term \\(dB/dt\\) = normal hill (M) * inverse hill(M) ‚Äì decay term, \\(dC/dt\\) = inverse hill (M) -decay term. Furthermore, \\(kA=kB1&lt;kB2&lt;kC\\) to ensure the switches happen at the right places.\n\n\nExercise 2.4 (Biology & algorithmic/mathematical thinking) From hereon we assume that the morphogen gradient reaches steady state very quickly, and no longer use the numerical implementation of the diffusion equation and instead work with a superimposed morphogen profile defined by reaction_diffusion_gradient to save time. (Hint: to not call the function any more use # in front of where it is called) Change the simulation loop such that it computes the morphogen gradient once. What type of function is the superimposed morphogen profile and how does this relate to question 2.\n\n\nAnswer The imposed morphogen profile is an exponential gradient, the ‚Äì indicates it is a declining function, the \\(i\\cdot dx\\) is to convert position in the spatial grid to actual distance, and the /sqrt(D/d) ensures that if \\(i\\cdot dx=lambda=\\sqrt(D/d)\\) we have \\(e^{-1}=1/e\\) and hence an e-fold decline as is the case for a steady state diffusion gradient.\n\n\nExercise 2.5 (Biology & algorithmic/mathematical thinking) ¬†\n\nNow create functions to update A, B and C according to your equations from the previous question. You may use the predefined hill and ihill functions that are provided in the code. For simplicity, you may keep most of the parameters the same across genes, but some have to be different to ensure the right location of the genes (see your reasoning to the previous question). (Hint: using array properties to update A/B/C, like in the reaction_diffusion_step function, makes your code run a lot faster than using for-loops)\nAlso make sure that the levels of A, B and C are updated in the simulation loop. (Hint: using array properties to update A/B/C, like in the reaction_diffusion_step function, makes your code run a lot faster than using for-loops)\nNext, ensure A, B and C are visualized in the bottom plot axis (copy and adapt the code for the visualization of M). You can add an extra third axis to the plot to visualize the (French) flag pattern, by getting which of the three genes is maximal at each location with np.argmax(np.array([A, B, C]), axis=0)and turning that into an array of RGB colors of choice.\nDo you get the expected ‚ÄúFrench flag‚Äù pattern? If not, think of why not and improve your equations from previous question, parameters or your code.\n\n\n\nAnswer Look at code 02_answer_morphogengradient_superimposed_quickABC.py. You will find that gene C needs to be induced by low and repressed by high M. And as explained above: kA=kB1&lt;kB2&lt;kC.\n\n\nExercise 2.6 (Biology) At some point in development, the morphogen gradient will disappear, for example in the case of the Drosophila Bicoid gradient because the maternal mRNA is degraded. Predict what will happen to the expression of A,B and C (and hence the French flag pattern) if the morphogen gradient disappears over time and assume A, B, and C are regulated by our equations (first try think about this without actually simulating this).\n\n\nAnswer As M decays A and B will no longer be activated and C no longer repressed, hence the takeover by C. The French flag pattern disappears.\n\n\nExercise 2.7 (Algorithmic/mathematical thinking) Now write the code in reaction_diffusion_gradient that updates the morphogen concentration, such that after the time point A, B and C have gotten close to equilibrium, the morphogen gradient gradually disappears. Adapt the simulation loop where necessary and run your code: was your prediction on A/B/C from the previous question correct? Why/why not? Hint: you might need to increase the duration of your simulation, especially if it takes a long time for A, B and C to reach equilibrium (or you can change the parameters to speed things up by using same production/degradation rate ratio yet higher absolute values of the individual parameters).\n\n\nAnswer It should always result in C taking over. See answer to 6.\n\n\nExercise 2.8 (Biological & mathematical thinking) A, B and C cannot remain in a stable pattern if they are only influenced by M. How can we stabilize the pattern in absence of M? Test your ideas by creating new update functions for A, B and C and let these new ‚Äòrules‚Äô kick in at the same time when M starts to decline. Again, you may find Hill functions useful and perhaps also the before/after switch time structure used in reaction_diffusion_gradient for M. (Hint: think about how the genes should affect each other, and assume that in this new phase, when genes have already been initialized in absence of repression the genes will be expressed). Can you maintain the expression domains of A, B and C and does their shape change?\n\n\nAnswer See 03_answer_morphogengradient_superimposed_quickABC_phase2.py As M decays A and B will no longer be activated and C no longer repressed, hence the takeover by C. They will find mutual repression with only your neighbor gene is not enough, you need it with both 2 other genes. So A needs to be repressed by B and C, B by A and C, and C by A and B. We can maintain A at the beginning, B in the middle and C at the end, but the shape of the expression domains becomes less gradual and more sharply defined.\n\n\nExercise 2.9 (Biology & algorithmic/mathematical thinking) A sudden switch from phase 1 (stable M gradient) to phase 2 (decaying M gradient) resulting in the genes following different differential equations is biologically implausible. In reality, genes have complex promotors and enhancers integrating different inputs that arise in different developmental stages. Try to come up with one integrated expression for each gene, incorporating simultaneous input from the morphogen gradient and the other genes. Create new update functions for A/B/C and test your ideas. Can you get a stable pattern before and after the decline in M? A couple of things you could consider: a. Think of how positively and negatively M regulated expression behave once the gradient starts declining: what is the best way to combine (AND/OR) that with the regulation by the genes? a. Consider splitting up the M regulated expression of middle gene B into a positive and negatively regulated morphogen part before integrating the other genes inputs. a. It is also possible to give certain genes a bit of a constant boost to prevent their takeover by other genes due to timing issues\n\n\nAnswer This is not at all easy (or as the professor would say ‚Äúhighly non trivial‚Äù). It requires that for genes (or part of their input) being positively dependent on M that there is an OR gate (sum) (so they stay on once M is absent as long as the mutual repressors are absent) while for genes (or part of their input) being negatively dependent on M that there is an AND gate (so they do not expand once M is absent but require both M absent and mutual repressors absent). See the answer code: 05_&lt;longname&gt;_quickABC_phase2_combined.py\n\n\nExercise 2.10 (Biology & algorithmic/mathematical thinking) Write code to get noise in the morphogen gradient, both in its steady state and decaying phase. During which phase do the expression domains of A, B and C suffer more from the noise. Explain why? How could we make the system more robust in a manner that is also likely occurring in nature?\n\n\nAnswer See the answer code: 05_&lt;longname&gt;_quickABC_phase2_combined_noise.py The noise has more effect in the initial phase when mostly morphogen is driving the expression domains, as gene expression becomes established and morphogen levels decay the mutual repression ensures stability against the noise Diffusion of A, B and C, in the lateral direction this evens out differences (not implemented right now)",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html#extra-questions",
    "href": "pattern_practical_1.html#extra-questions",
    "title": "2¬† Gradients and segments",
    "section": "2.5 Extra questions",
    "text": "2.5 Extra questions\nIf you‚Äôre done early or a master student, you can make these extra questions. These questions need not be made in the order they are provided, you can choose what you would like to investigate.\n\nExercise 2.11 (Biological thinking) Play with the size of the domain to see what happens to the gene expression domains. What would this mean for an organism?\n\n\nAnswer You may find that if the domain expands the C domain expands and that if the domain shrinks the C domain disappears. This would mean having either a very large or loosing your ‚Äúbutt‚Äù part.\n\n\nExercise 2.12 (Biological & algorithmic thinking) Various mechanisms have been proposed to ensure that the domains of the morphogen controlled genes scale with the size of the domain. One proposed mechanism suggests the existence of an also diffusible ‚Äúexpander‚Äù molecule which expression is repressed by the morphogen but which itself either represses the degradation of the morphogen or enhances its diffusion (see e.g. https://www.pnas.org/doi/full/10.1073/pnas.0912734107 and https://onlinelibrary.wiley.com/doi/10.1111/dgd.12337).\nFor the expander we can write:\n\\[\n‚àÇE/‚àÇt=p K^h/(K^h+M^h )-dE+D_E ‚àÜE\n\\]\nAssume E reduces degradation of M (easier to implement than enhancement of diffusion) and study the effect of scaling. (Hint, vary the size of the domain in the length direction but plot domains as a function of relative instead of absolute domain size to compare domain sizes).\n\n\nAnswer We have not tried this out. Let us know what you find!\n\n\nExercise 2.13 (Algorithmic thinking) In b, we can also implement other boundary conditions, especially for the right boundary. What happens to the profile if we make a no-flux boundary by copying the value at n-2 to n-1? And what if we set it to a sink (force concentration to zero)?\n\n\nAnswer Copying n-2 to n-1 adds material, so now concentration at end keeps rising and gradient fails to reach equilibrium. Putting it to sink not tried, may speed up stabilization of gradient\n\n\nExercise 2.14 (Algorithmic thinking) From question 4 onwards, how do things change if we do not assume a quasi-steady-state for the morphogen gradient (i.e.¬†keep the morphogen dynamics instead of replacing it by the superimposed exponential)? How would you implement a disappearing gradient and how does it shape change the outcomes of the flag?\n\n\nAnswer Instead of only initializing position x=0 at 100, you now need to include it in the update of the diffusion and use a decaying exponential behind the value of 100 beyond a certain point. Not tried myself, likely somewhat different shape change than simply decreasing an exponential along the entire axis proportionally, may affect sizes and dynamics of domains.\n\n\nExercise 2.15 (Algorithmic thinking) Implement your solution to make the system more robust for noise from question 10. Did it work?\n\n\nAnswer This requires also having A, B and C diffuse and is expected to smooth out differences.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_1.html#relevant-literature-further-reading",
    "href": "pattern_practical_1.html#relevant-literature-further-reading",
    "title": "2¬† Gradients and segments",
    "section": "2.6 Relevant literature / further reading",
    "text": "2.6 Relevant literature / further reading\nDagmar Iber group, scaling from non-steady state dynamics and uniform growth: Fried and Iber (2014) https://www.nature.com/articles/ncomms6077\nCheung et al. (2011) https://pmc.ncbi.nlm.nih.gov/articles/PMC3109599/\nHe et al. (2015) https://www.nature.com/articles/ncomms7679\nBicoid gradient: larger eggs get more Bicoid mRNA so higher production rate, when it scales with volume this helps scale the gradient: Inomata (2017) https://onlinelibrary.wiley.com/doi/10.1111/dgd.12337\nExpansion repression model. Players: Chordin, Bmp, Sizzled. Chordin represses Bmp which induces Sizzled (which has low decay rate), yet Sizzled reduces Chordin decay. Chordin is morphogen. Sizzled is expander (by reducing decay of morphogen) Ben-Zvi and Barkai (2010) https://www.pnas.org/doi/abs/10.1073/pnas.0912734107\n\n\n\n\nBen-Zvi, Danny, and Naama Barkai. 2010. ‚ÄúScaling of Morphogen Gradients by an Expansion-Repression Integral Feedback Control.‚Äù Proceedings of the National Academy of Sciences 107 (15): 6924‚Äì29.\n\n\nCheung, David, Cecelia Miles, Martin Kreitman, and Jun Ma. 2011. ‚ÄúScaling of the Bicoid Morphogen Gradient by a Volume-Dependent Production Rate.‚Äù Development 138 (13): 2741‚Äì49.\n\n\nDriever, Wolfgang, and Christiane N√ºsslein-Volhard. 1988. ‚ÄúThe Bicoid Protein Determines Position in the Drosophila Embryo in a Concentration-Dependent Manner.‚Äù Cell 54 (1): 95‚Äì104.\n\n\nFried, Patrick, and Dagmar Iber. 2014. ‚ÄúDynamic Scaling of Morphogen Gradients on Growing Domains.‚Äù Nature Communications 5 (1): 5077.\n\n\nHe, Feng, Chuanxian Wei, Honggang Wu, David Cheung, Renjie Jiao, and Jun Ma. 2015. ‚ÄúFundamental Origins and Limits for Scaling a Maternal Morphogen Gradient.‚Äù Nature Communications 6 (1): 6679.\n\n\nInomata, Hidehiko. 2017. ‚ÄúScaling of Pattern Formations and Morphogen Gradients.‚Äù Development, Growth & Differentiation 59 (1): 41‚Äì51.\n\n\nWolpert, Lewis. 1969. ‚ÄúPositional Information and the Spatial Pattern of Cellular Differentiation.‚Äù Journal of Theoretical Biology 25 (1): 1‚Äì47.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Gradients and segments</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html",
    "href": "pattern_practical_2.html",
    "title": "3¬† Turing digit patterns",
    "section": "",
    "text": "A foundational work in Theoretical Biology is Alan Turing‚Äôs paper ‚ÄúThe Chemical Basis of Morphogenesis‚Äù [1]. In this work, Turing mathematically demonstrates a plausible mechanism whereby a reaction-diffusion system can lead to the formation of a spatially-patterned solution in steady state. Mathematically and conceptually, Turing‚Äôs key innovation was considering diffusion-driven instability. That is: An unpatterned steady state solution becomes unstable in the presence of diffusion, leading to the formation of a spatially patterned state.\nTuring-type patterns emerge when a system exhibits short-ranged activation and long-ranged inhibition. The most well-known realization of a Turing reaction-diffusion system is the combination of a slow-diffusing activator and fast-diffusing inhibitor. Many more examples exist which can be reduced to a mathematically equivalent form. In this practical, we will explore a 3-component Turing system developed by Raspopovic and coauthors to explain the patterning of fingers on the paws of mice [2].\nIn this practical you will work with a total of 6 different python codes. Please note that each code simply differs by an extension or modification from the previous code, so that in the end you will have a long script that performs all the steps in one go.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#section",
    "href": "pattern_practical_2.html#section",
    "title": "3¬† Turing digit patterns",
    "section": "",
    "text": "Turing, A. M. ‚ÄúThe Chemical Basis of Morphogenesis.‚Äù Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences 237.641 (1952): 37‚Äì72.\n\nRaspopovic, Jelena, et al.¬†‚ÄúDigit patterning is controlled by a Bmp-Sox9-Wnt Turing network modulated by morphogen gradients.‚Äù Science 345.6196 (2014): 566-570.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#equivalence-of-turing-models",
    "href": "pattern_practical_2.html#equivalence-of-turing-models",
    "title": "3¬† Turing digit patterns",
    "section": "3.1 Equivalence of Turing models",
    "text": "3.1 Equivalence of Turing models\nThe model proposed by Raspopovic et al.¬†for generating Turing patterns underlying limb bone modeling consists of 3 instead of 2 interacting variables. We will refer to this model as ‚ÄúBSW‚Äù model, as it represents interactions between Bmp, Sox9, and Wnt.\n\nExercise 3.1 (Conceptual thinking) Study the schemes below. The arrows represent activating or inhibiting interactions, while the wavy lines represent diffusion of components. Simplify the BSW model scheme to a 2-component system and explain why the relevant Turing constraints apply to the BSW model. In your answer, think about the logic of the interactions in the system conceptually; we do not expect you to answer by writing down equations.\n\n\n\n\n\n\n\n\nFigure¬†3.1: Schemes and numerical solutions of two prototypical 2-component Turing models\n\n\n\n\n\n\n\n\n\n\nFigure¬†3.2: Scheme of the BSW model\n\n\n\n\n\n\n\nAnswer ‚Äúfuse‚Äù Sox9 and Wnt nodes to get Substrate-Depletion model a) Sox9 inhibits Wnt which inhibits Sox9 -&gt; summarize as self-activation b) Wnt self-inhibition arrow -&gt; part of self-activation in reduced scheme c) Wnt slow diffusion -&gt; Sox9 slow diffusion Turing instability requires short-range activation and long-range inhibition.  In the substrate-depletion model, the growth of the slow-diffusing activator A is constrained by the local loss of fast-diffusing substrate S. Around an A-enriched domain a halo of S will form, which will induce nearby A domains to grow, generating the alternating spatial pattern. In the BSW model, Sox9 indirectly locally self-activates by inhibiting its slow-diffusing inhibitor Wnt, but the growth of Sox9 domains is constrained by depletion of the fast-diffusing Bmp (playing the role of substrate in this model). Around Sox9-enriched domains a halo of Bmp will form and induce the generation of more Sox9 domains in the surroundings.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#patterning-in-a-fixed-domain-size",
    "href": "pattern_practical_2.html#patterning-in-a-fixed-domain-size",
    "title": "3¬† Turing digit patterns",
    "section": "3.2 Patterning in a fixed domain size",
    "text": "3.2 Patterning in a fixed domain size\nThe code in the script 01__digits_squarehomogeneoustissue.py simulates the model in a 2D square tissue of constant size. This setup can be considered as a small region of a large Petri dish in which mesenchymal cells have been plated with the correct chemicals to undergo bone formation (Supplementary Figure S3 in the paper).\n\nExercise 3.2 (Biology) Play with the reaction parameters and diffusion constants to study their effects on the wavelength of the patterns. Note down the following:\n\nWhich parameter changes lead to a pattern with thicker/thinner stripes?\nWhich parameter changes lead to a pattern with spots?\n\nNote: To make things easier on yourself, only change one parameter at a time. Use a narrow range around the reference value to stay in the numerically stable regime of the forward Euler solver. If the solution becomes blank or wildly oscillating, you‚Äôre likely in the numerically unstable regime and you should try a smaller change of values. When increasing diffusion constants, you may need to reduce the time stepping dt.\n(only Master students) Based on the BSW interaction scheme, give a short interpretation of why the following parameter alterations change the spatial pattern:\n\nParameters of Sox9-BMP interaction k2 and k4\nParameters of Sox9-Wnt interaction k3 and k7\nFold change difference in BMP diffusion compared to Wnt diffusion\n\nHint: Use the maximum and minimum values of the variables to guide your thinking.\n\n\nFor simplicity I will only show Sox9 patterns:\n\n\n\n\nIn general, all three components follow similar trends in terms of concentrations as any impact on one component will affect the others via feedbacks. Increasing or decreasing the parameters k2 and k4 has the same effect on the pattern.  With increased Sox9 activation (high k2) a lower amount of BMP is sufficient to get Sox9 to increase which in turn inhibits both Wnt and Bmp. As a result, the ‚Äúpeaks‚Äù are reached sooner in space resulting in shorter lower amplitude wavelengths. Similarly, when Sox9 inhibition of BMP is high (high k4) then BMP peaks are lower, which means there is less Sox9 activation leading to smaller amplitude peaks and hence shorter wavelength. Conversely, if Sox9 activation by BMP is weak (low k2), then it takes more BMP to build up to a peak, resulting in longer higher amplitude wavelengths. When Sox9 inhibition of BMP is weak (low k4), then it takes larger Sox9 domains to deplete BMP, resulting in longer higher amplitude wavelengths. If Sox9 activation (k2) or BMP inhibition (k4) are extremely strong, Sox9 locally depletes BMP faster than it can diffuse, which in turn leads to a reduction in Sox9; this could explain local oscillations and travelling waves, though this result should be verified with a better numerical scheme. Changing BMP self-inhibition (k5) has the opposite effect as Sox9 activation (k2) and BMP inhibition (k4). BMP self-inhibition restricts the size of BMP domains. Stronger self-inhibition (high k5) means small pockets of BMP are destabilized and Sox9 regions grow until the point that they completely deplete the local BMP, resulting in larger higher amplitude peaks. Vice-versa, reducing BMP self-inhibition (low k5) allows BMP peaks to more easily form, leading to shorter pattern wavelengths. Unlike the other two parameters, this reduction only happens up to a point, as the other feedback interactions take over.\n\n\n\nIncreasing or decreasing the parameters k3 and k7 has the same effect on the pattern. When Wnt inhibition of Sox9 is increased (high k3) or Sox9 inhibition of Wnt is increased (high k7) the cross-inhibition of both variables is stronger, meaning they will tend to create larger exclusion zones which translates as longer higher amplitude wavelengths. When Wnt inhibition of Sox9 is decreased (low k3) or Sox9 inhibition of Wnt is decreased (low k7) smaller regions can coexist, reducing the wavelength and the amplitude. Even lower cross-inhibition means the regions overlap, and small time delays could lead to oscillations, although this should be verified with a numerically better scheme. Wnt self-inhibition (k9) limits the growth of Wnt domains. When self-inhibition is strong (high k9) Wnt domains shrink and therefore the wavelength is shorter with lower amplitude. Even stronger self-inhibition leads to overlapping domains and similar oscillations as observed for low k3 and low k7. Vice-versa, a decrease of Wnt self-inhibition (low k9) allows larger patches of Wnt to form, which increases the wavelength and the amplitude.\n\n\n\nThe equation for Sox9 includes a ‚Äúsaturation term‚Äù in the form of a negative cubic power of Sox9, which means that effectively there is Sox9 self-inhibition in the model. The parameter delta multiplies a quadratic Sox9-dependent term in the equation for Sox9, which by default is set to 0. A positive delta effectively means that Sox9 has a self-activation term that counteracts the self-inhibition by the cubic saturation term, whereas a negative delta means the Sox9 self-inhibition is stronger. Introducing a stronger Sox9 self-activation (delta &gt; 0) increases the amplitude of Sox9 domains. Due to this local strong increase in Sox9, there is also a local strong depletion of BMP which prevents stripes from elongating, resulting in a spot pattern. Introducing a stronger Sox9 self-inhibition (delta &lt; 0) reduces the amplitude of Sox9 domains. There is thus less BMP depletion which enables stripes to elongate and fuse more easily. Wnt and Sox9 are cross-inhibitory, and thus their domains will compete. Lower Wnt diffusion means Wnt domains form locally, leading to a reduced wavelength. Vice-versa a higher Wnt diffusion means that Wnt spreads away from Sox9 domains enabling them to expand ultimately resulting in longer wavelengths. Note that since BMP diffusion scales with Wnt diffusion, both parameters were changed at the same time. The ratio of diffusion speeds defines the size of the Turing space. As the diffusion of BMP is set equal to that of Wnt, the homogenous solution becomes stable. Reducing the diffusion of BMP further leads to travelling waves or oscillations. This is because a slow (or stationary) BMP is in a local negative feedback loop with Sox9, which results in oscillatory behavior. The establishment of Wnt domains around the oscillating Sox9 regions is also oscillatory. Increasing BMP diffusion allows to more effectively ‚Äúfeed‚Äù Sox9 domains, which leads to a subtle increase in the wavelength and amplitude.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#the-homogenous-steady-state-in-turing-models",
    "href": "pattern_practical_2.html#the-homogenous-steady-state-in-turing-models",
    "title": "3¬† Turing digit patterns",
    "section": "3.3 The homogenous steady state in Turing models",
    "text": "3.3 The homogenous steady state in Turing models\nIn a system exhibiting Turing instability, the spatially unpatterned steady state is unstable in the presence of diffusion.\n\nExercise 3.3 (Conceptual thinking) Modify the initial condition in the script 01__digits_squarehomogeneoustissue.py to be exactly in this state (Sox9, BMP, Wnt should be all zero everywhere) and run the simulation. Explain the simulation result.\n\n\nAnswer The homogenous steady state continues to exist, and if starting in that state the system will not evolve to a different state. A small perturbation is needed to nudge the system out of the steady state.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#patterning-in-a-growing-domain-size",
    "href": "pattern_practical_2.html#patterning-in-a-growing-domain-size",
    "title": "3¬† Turing digit patterns",
    "section": "3.4 Patterning in a growing domain size",
    "text": "3.4 Patterning in a growing domain size\nBesides parameter values, the pattern that emerges from a Turing system is also strongly influenced by initial conditions and the size and shape of the domain where reactions and diffusion happen.\nIn the script 02__digits_growingsquare_PD.py we added tissue growth in the proximo-distal (body to limb) direction. Growth is controlled by a growth rate vi.\n\nExercise 3.4 (Biology) ¬†\n\nStudy the code to understand what it does.\n\nIncrease the horizontal growth rate vi in small increments (try: 0.01, 0.05, 0.1, 0.2, 0.5) and study what happens to the pattern. Explain why you think this happens.\n\nNote: Increase the totaltime parameter to 5000 when using growth rate of 0.01.\n\n\nAnswer At low growth rates, the pattern forms stripes parallel to the growth direction. As the growth speed increases, the alignment shifts to stripes perpendicular to the growth direction. When growth is slow, the ‚Äúnew domain‚Äù added by growth is too small to fit a Turing wavelength. As a result, the existing pattern gets extended into the new parts. When growth is fast, the ‚Äúnew domain‚Äù added by growth is sufficiently large to fit a Turing wavelength, which becomes the selected mode. Additionally for faster growth, the diffusion/growth ratio is now lower making it harder to simple extend the existing pattern from diffusion of the old cells that previously made up the boundary.\n\n\n\n\nSimulations from this practical with domain growth\n\n\n\n\n\nExperimental work using a light-gated chemical reaction that forms Turing patterns when illuminated. The illuminated area was shifted over time to produce a growing domain, work by M√≠guez et at., 2006",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#patterning-along-two-growth-axes",
    "href": "pattern_practical_2.html#patterning-along-two-growth-axes",
    "title": "3¬† Turing digit patterns",
    "section": "3.5 Patterning along two growth axes",
    "text": "3.5 Patterning along two growth axes\nIn reality, the limb bud grows in both the proximo-distal and anterior-posterior direction. In the script 03__digits_growingsquare.py this is implemented with growth rates vi for proximo-distal growth and vj for anterior-posterior growth.\n\nExercise 3.5 (Biology) Play with the relative size of these growth rates and study what happens. Compare the order of appearance of stripes to the experimental data in the paper (see Figure 3F in the Raspopovic paper).\n\n\nAnswer As in the previous question, the rate of growth influences whether stripes grow parallel to or perpendicular to the growth boundary. Also as before, slow horizontal growth promotes horizontal stripes (vi&lt;=0.1), and when this becomes faster stripes reorient to become vertical. However now, if we add vertical growth and it is not small enough relative to the horizontal growth, above and below the horizontal stripes we get vertical stripes and these domains are larger the faster vertical growth is relative to horizontal growth. So to get ok-ish patterns we need horizontal growth not too fast, and vertical growth to be slower. These results are consistent with horizontal growth of the ‚Äúpaddle‚Äù being faster than vertical growth. Also, we see that middle horizontal stripes appear earlier than the outer ones (in those situation where not all is vertical), similar to what we see in the paper in Fig 3F. Still interestingly, if you put vj=0.2 and vi=0.1 you promote initially vertical and later on horizontal stripes, this is an effect of in the vertical direction hitting the boundaries of the domain first and then still slowly continuing growth in the horizontal dimension at a not too high rate that was supportive of extending horizontal stripes.\n\n\n\n\nSimulations from this practical with domain growth",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#making-a-virtual-tissue-grow",
    "href": "pattern_practical_2.html#making-a-virtual-tissue-grow",
    "title": "3¬† Turing digit patterns",
    "section": "3.6 Making a virtual tissue grow",
    "text": "3.6 Making a virtual tissue grow\nThink about how tissue growth is implemented in the script 03__digits_growingsquare.py.\n\nExercise 3.6 (Algorithmic and conceptual thinking) What is the default value of the newly added tissue when it grows? Is the concentration elsewhere in the tissue changed?\nDo you think this approach is reasonable to model biological growth of a tissue? Explain why/why not.\nHint: Think about it from the perspective of a growing cell containing a number of molecules ‚ÄúX‚Äù.\nIf the cell increases in volume and does not produce/degrade X, what happens to the concentration of X?\nWhat would be the concentration of X in the two daughter cells if the cell divides?\nHow would the situation change if X is constantly produced and degraded?\nIn the paper, Raspopovic et al.¬†create a ‚Äútissue growth map‚Äù (see Figure 3A), which they use to map concentration values from one timepoint to the next using interpolation-based transformations.\nInspired by this approach, the script 04__digits_growingsquare.py implements a different growth function that uses bilinear interpolation to expand the tissue. Play around with the growth rate parameters and compare the results to what you found earlier. Do the patterns differ? If yes, how do they differ?\nHint: Try the supplementary script supp_g__bilinear_interpolation.py to understand how interpolation-based growth works.\nFor master students: yet another way of implementing tissue growth would be to take cell division and inheritance of maternal state by the two daughter cells literal and implement this by new boundary cells copying the state of their direct neighbors. Implement this alternative growth and see how this affects your results.\n\n\nAnswer The default value of newly added tissue is 0, because the entire matrix is initialized with 0s, and only a small rectangle corresponding to the initial tissue domain is filled with random values and then updated based on the equations. When growth happens, the tissue domain extends to cover new parts of the matrix that were not included in the initialization. Extending the edges into 0-valued territory would be as if new, empty cells appear at the edges of the tissue, which is not a reasonable approach to model limb bud growth.If a cell increases in volume without producing/degrading molecules ‚ÄúX‚Äù, then the concentration of X will decrease as the cell increases in volume. Daughter cells would have on average half the concentration of X as the mother cell. If ‚ÄúX‚Äù is constantly produced and degraded (and the cell is a well-mixed environment) then the concentration of X could be in quasi-steady state during growth. A growing cell would maintain the same concentration of X over time. When the cell divides, both daughter cells would inherit the same concentration of X as the mother cell had. Assuming the latter scenario is more likely, then growth of a tissue should work by ‚Äústretching‚Äù the values of the growing parts. (Some tissues actually do grow only at the edges, but even there we would expect the newly added ‚Äúterritory‚Äù to be made from previous cells and therefore still inherit their concentrations.)\n\n\nThe different growth appears to affect pattern formation, e.g for vj=0.01 the pattern is still predominantly horizontal for 0.15, which was less so before. In constrast for vj=0.05 and vi=0.15 pattern is mostly vertical whereas in other growth regime it was still partly horizontal. Seems that transition from horizontal to vertical stripes is thus modulated. So for low vj it is easier to get horizontal stripes.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#spatial-modulation-of-parameters-k4-and-k7",
    "href": "pattern_practical_2.html#spatial-modulation-of-parameters-k4-and-k7",
    "title": "3¬† Turing digit patterns",
    "section": "3.7 Spatial modulation of parameters k4 and k7",
    "text": "3.7 Spatial modulation of parameters k4 and k7\nLet us now ignore growth and its role on orienting stripes for a while and explore once again the influence of parameters on the patterns. In reality, digits are patterned further apart at the distal end than at the proximal end, where they need to converge on a hand and wrist. This implies that the wavelength of the Turing pattern should not be constant. In a previous question you probably found that the k7 and k4 parameters impact the wavelength of the Turing pattern. The authors speculate that the FGF and Hox gene gradients observed in the limb bud exert an effect on the Sox9-BMP-Wnt patterning module through these parameters.\n\nExercise 3.7 (Biology) The script 05__digits_squarek4k7gradient.py allows you to implement exponential gradients of k4 and k7 across the tissue. You can set the minimum and maximum values, axis (x=horizontal, y=vertical) and direction (0=decreasing, 1=increasing). Look in the supplement of the Raspopovic paper, page 48 Fig 2C, left for the estimated change of k4 and k7 across the tissue and try to reproduce (not exactly but similarly) Figure 2C, right side, by playing with the parameters affecting the k4 and k7 gradients. Describe along which axis, in which direction and to what extent k4 and k7 are changing. Estimate the change in Turing pattern wavelength this results in.\n\n\nAnswer See the script 05__digits_squarek4k7gradient_answer.py for a solution. The output may look sometimes like this:",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#hoxd13-and-fgf",
    "href": "pattern_practical_2.html#hoxd13-and-fgf",
    "title": "3¬† Turing digit patterns",
    "section": "3.8 Hoxd13 and FGF",
    "text": "3.8 Hoxd13 and FGF\nAs mentioned above, the biological factors thought to underly the variation in the k4 and k7 parameters are Hoxd13 and FGF. Hoxd13 is highly expressed in the distal domain of the paddle and not expressed elsewhere, and this expression domain is growing as the paddle grows. FGF is expressed from the distal edge of the paddle and spreads through diffusion. For an illustration see Fig S8A of the supplement of the Raspopovic paper (page 10). Experimental data furthermore suggest that Hoxd13 and FGF together affect k4 and k7, which the authors decided to model using the following equations:\n\\[\nk4^* = k4 - k_{HF_{bmp}} * fgf(i,j) * hox(i,j)\n\\]\n\\[\nk7^* = k7 + k_{HF_{wnt}} * fgf(i,j) * hox(i,j)\n\\]\n\nExercise 3.8 (Algorithmic and biological thinking) The script 06__digits_squarehoxdfgf.py implements how k4 and k7 are a function of local FGF and Hox values, but does not yet include Hox and FGF spatial patterns and how these develop over time. Adjust the code to incorporate the observed Hoxd13 and FGF patterns and study how the digit patterns compare to what you found under question 7. Note that arrays for hox and fgf are included, and decay and diffusion values for fgf are provided already in the code. Also note that hox and fgf each have a maximum value of 1.\n\n\nAnswer See the script 06_digits_squarehoxdfgf_answer.py for a solution. The output may look sometimes like this:\n\n\nThe Hox domain starts as a narrow vertical band and expands backward. The FGF gradient develops from the rightward boundary. Since a certain amount of Hox and FGF is needed to result in k4 and k7 values permissive of Turing patterns the stripes develop from right to left and because of the wavelength changes induced by different Hox and FGF values become closer together towards the left. Note that here a nice example run is shown but patterns are variable.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#shape-makes-the-tissue",
    "href": "pattern_practical_2.html#shape-makes-the-tissue",
    "title": "3¬† Turing digit patterns",
    "section": "3.9 Shape makes the tissue",
    "text": "3.9 Shape makes the tissue\nHaving studied the effects of growth and genetic factors modulating the Turing pattern now it is time for the final step. So far, we‚Äôve approximated the limb bud very crudely with a rectangle. As you probably noticed in your parameter exploration, the tissue boundary can bias stripe orientation. Therefore it is interesting to study the interplay between the shape of the ‚Äúpaddle‚Äù or limb bud in which the digits develop and how it grows with the temporal development of the hox and fgf patterns. In the script 07__digits_growingpaddle.py we implemented two different tissue geometries: An ellipse and a ‚Äúpaddle‚Äù that imitates the shape of the limb bud. Note that to implement Hox and FGF domains on a complex growing shape, quite a bit of complex code involving masks describing the tissue domain and angles to control the Hox and FGF domains was needed. At first, we are going to ignore these technical details.\n\nExercise 3.9 (Biology) Switching between geometries is easily done by changing the value of the parameter geometry on line 78 of the script. Start with the ‚Äúellipse‚Äù geometry. What do you observe with regards to the FGF gradient? What consequences does this have? Why would this be ‚Äúa smart thing to do‚Äù?\nPlay around with vi/vj and Lx0 to investigate the effect of ellipse shape and development over time on the number, shape and robustness of digits that form.\nNow switch to the paddle geometry. Observe the patterns that emerge and compare these to the simulated and experimentally observed patterns in Figure 3E and 3F of the Raspopovic article. According to you what does and what doesn‚Äôt the model explain well?\nThe model explains the formation and positioning of the 4 digits, and how the distance between these digits increases along the limb bud. However, the model clearly does not simulate/explain the formation of the wrist bones that is observed experimentally. Interestingly the experimental data suggest that while digits may form from the ‚Äúright to the left‚Äù, the ‚Äúleftmost‚Äù bones of the wrist are there the earliest, suggesting there locally different conditions apply allowing earlier Turing patterning but with a very large wavelength happening there.\nTo implement non-square geometries, the script uses ‚Äúmasks‚Äù. Study how these masks are created in the functions create_ellipse_mask and create_tissue_mask.\nHint: Try the supplementary script supp_i__geometries.py.\nNote: For computational convenience, we use the ‚ÄúClass‚Äù data structure to implement common functions shared by the masks. This is a somewhat more advanced programming concept, so it‚Äôs ok if you don‚Äôt understand what it does yet. For the extra curious and motivated, feel free to check out the supplementary script supp_i__class_data_structure.py.\n\n\nAnswer Fgf gradient forms from bottom to top, as a consequence bottom fingers form first, often one of the bottom fingers forms and than splits. While in general Turing mechanisms give patterns they are not reproducible in terms of the positioning of the stripes. By starting the highest point of the FGF gradient locally it is determine where the first digits can arise, generating control over the positioning of the digits. Vi 0.1 goes okay, 0.2 stretched and splitting of digits  Vj too low too little room so less than 5 digits, vi too high more than 5 fingers, higher rates result in less reproducible patterns Below a nice pattern showing 5 fingers, but also showing near splitting of one of the fingers at the end The model explains the formation and positioning of the 4 digits, and how the distance between these digits increases along the limb bud. However, the model clearly does not simulate/explain the formation of the wrist bones that is observed experimentally. Interestingly the experimental data suggest that while digits may form from the ‚Äúright to the left‚Äù, the ‚Äúleftmost‚Äù bones of the wrist are there the earliest, suggesting there locally different conditions apply allowing earlier Turing patterning but with a very large wavelength happening there.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_2.html#master-students-exercise",
    "href": "pattern_practical_2.html#master-students-exercise",
    "title": "3¬† Turing digit patterns",
    "section": "3.10 Master students exercise",
    "text": "3.10 Master students exercise\n\nExercise 3.10 (Poly and oligodactyly) Above we played with ellipse size and growth rates, however in addition to mutations affecting tissue size and growth, also mutations in genes affecting the Hox and FGF morphogens may occur. Play with parameters that determine Hox and FGF expression zones on the paddle geometry. Which parameter changes lead to the formation of supernumerary fingers (polydactyly)? Which parameters lead to too few fingers (oligodactyly)?\nHint: The following supplementary scripts will be helpful to understand how the FGF and Hox domains were coded:\n\nsupp_i__ellipse_slice.py visualizes the usefulness of masks based on the example of selecting a slice of an ellipse;\nsupp_i__smooth_function.py visualizes the function smooth_function();\nsupp_i__morphological_operations.py visualizes morphological operations (binary dilate/erode and Gaussian blur) which are used for mask operations, calculating the Laplacian on the irregular domain, and to select pixels to generate the FGF and Hox pattern;\nsupp_i__maskDifference.py visualizes the difference of masks, which is how the function single_step_growth() checks if the tissue domain is growing or shrinking.\n\n\n\nAnswer Reference values:\n\n# Reaction-diffusion parameters for FGF\nDfgf = 0.001            # diffusion coefficient\nfgf_decay_rate = 0.0003 # degradation parameter\n\n# Opening angle of fgf source at the start and end\nfgf_angle_start_1 = -0.3*np.pi\nfgf_angle_start_2 = -0.5*np.pi\nfgf_angle_end_1   =  0.4*np.pi\nfgf_angle_end_2   = -0.6*np.pi    \nfgf_angle_rate    = 0.0002      # Exponential rate of change of angle\n\n# Opening angle of Hoxd13 expression domain at the start and end\nhox_angle_start_1 = -0.1*np.pi\nhox_angle_start_2 = -0.3*np.pi\nhox_angle_end_1   =  0.6*np.pi\nhox_angle_end_2   = -0.6*np.pi\nhox_angle_rate    = 0.001       # Exponential rate of change of angle\n\nhox_minR   = 0.3    # Minimum radius for Hoxd13 permissive zone at the end\nhox_maxR   = 0.85   # Maximum radius for Hoxd13 permissive zone (also minimum at the start)\nhox_R_rate = 0.001  # Rate of change of the Hoxd13 minimum radius over time\n\nDecreasing FGF diffusion increases the number of fingers and creates a fin-like appearance. However note that it also creates a shorter finger forming domain. Increasing FGF diffusion decreases the number of fingers.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Turing digit patterns</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html",
    "href": "pattern_practical_3.html",
    "title": "4¬† Clock-and-wavefront patterning",
    "section": "",
    "text": "4.1 Goal of the tutorial:\nIn this tutorial you will look at gradient formation and patterning using different mechanisms than you have seen previously. We will model the so-called clock-and-wavefront pattern, which stems from oscillations in gene expression and gene products and results in a regularly striped segmentation pattern in a growing tissue. Note that as compared to the Drosophila French Flag type case, this is thought to be the evolutionary ancestral model of segmenting animal body axes.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html#the-model-system",
    "href": "pattern_practical_3.html#the-model-system",
    "title": "4¬† Clock-and-wavefront patterning",
    "section": "4.2 The model system",
    "text": "4.2 The model system\nThe clock-and-wavefront model is an important model in describing somitogenesis. In this process early in embryo development, the somites, a precursor tissue for the vertebrae and other tissues later in development, are formed from the pre-somitic mesoderm. This mesodorm extends on the posterior end by growth, and the somites bud off periodically at the anterior end in the order of a couple of weeks (in humans).\nKey players in this model system are the protein FGF (fibroblast growth factor), and many genes and gene products that have an oscillatory pattern that will determine cell fate. However, for this tutorial, we simplify this to a single pair of gene mRNA and product, denoted with \\(m\\) and \\(p\\) for short.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html#programming-with-classes",
    "href": "pattern_practical_3.html#programming-with-classes",
    "title": "4¬† Clock-and-wavefront patterning",
    "section": "4.3 Programming with classes",
    "text": "4.3 Programming with classes\nBecause we are going to make a more complex model with a tissue existing of multiple cells, and each cell having its own concentrations of FGF, \\(m\\) and \\(p\\), we are going to use Classes in our code. You have been using Classes already: the data types such as int, str and bool have their own class, and the str class has many methods (=functions working on that class) defined, such as \"hello world\".upper(), but it is also possible to create custom classes. With classes, you can easily make objects, which is part of the object-oriented programming paradigm.\nToday, we are going to use classes for the different levels of our model: 1) tissue, 2) cell, 3) \\(m\\) & \\(p\\) clock and 4) the plotting. By using classes, we can separate things that happen on a tissue/cellular/clock scale, and seperate the model from the visualization of it. You will first work with 1 & 2 & 4, then 3 on its own and then combine all four yourself into one model.\nImportant concepts when working with classes are\n\nClass versus instance\nDefining the __init__ method and other methods\nClass attributes and using the keyword self\n\nThis tutorial should be doable without an extensive knowledge of classes as there are plenty of examples to copy-paste from, but feel free to read up on these concepts here at this online tutorial",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html#questions",
    "href": "pattern_practical_3.html#questions",
    "title": "4¬† Clock-and-wavefront patterning",
    "section": "4.4 Questions",
    "text": "4.4 Questions\n\nExercise 4.1 (Biology - An alternative gradient forming mechanism) In practical 1 we saw how gradients can be created through local production, diffusion and decay. However, other mechanisms for gradient formation are possible, such as cell lineage transport. Here we work with a model for the FGF gradient (fgfgradientfromgrowth.py) where only the rightmost/posterior cell produces FGF and grows, and in which cells upon division inherit this FGF from their mother cell. First study the code to see how it uses Classes and see what happens. Next, play with the model by varying the decay rate of FGF and the division rate of the cells. How does this affect the gradient? What happens if divisions are only allowed during the first half of the simulation (put divisiontime to 0.5 instead of 1).\n\n\nAnswer If growth rate goes up cell volumes increase faster, causing more dilution of FGF and hence a lower maximum of the gradient as due to the stable protein quite some time is needed to compensate for this decrease by more production, at the same time the gradient is less steep as cell division follow up faster and hence less time has passed and less day has taken place.\nIf decay rate goes up, maximum goes down but the gradient become steeper and shorter as for same time between divisions more decay takes place. If divisions stop halfway the simulation FGF in last cell goes up (no longer any loss due to growth-induced dilution) and FGF in all other cells goes to zero (no fresh influx from newly divided cells still having FGF).\n\n\nExercise 4.2 (Conceptual thinking) When would this type of gradient formation be more applicable than the earlier studied production, diffusion, decay type of gradient formation? Compare how this model is built-up to a production/diffusion/decay gradient formation model.\n\n\nAnswer Earlier gradient model: gradient length increases and slope decreases with faster diffusion and slower degradation. Here gradient driven by combination of growth/division and degradation. This new mechanism can only work if growth and patterning are occurring simultaneously, not if growth precedes patterning..\n\n\nExercise 4.3 (Mathematics) Let us now move to the other half of the clock and wavefront model, the clock part (clock.py), in which we implemented one of the earliest models for the somitogenesis oscillator from Lewis (2003) which models a gene that codes for a mRNA (\\(m\\)) that encodes a protein (\\(p\\)) that acts as a repressive transcription factor of this same gene. In class we discussed how for oscillations negative feedbacks, delays and non-linearity are important. Examine the code to find the differential equations governing this model and determine the negative feedback, delay and non-linearities in them.\n\n\nAnswer Non-linearity is in saturation function with power n, negative feedback is because affects own expression negatively, delay is here modeled explicitly through a special delay type differential equation\n\n\nExercise 4.4 (Biology) Play with the parameters of the model. How does the delay (\\(\\tau\\)) affect oscillations?\n\n\nAnswer Larger value of \\(\\tau\\)=delay results in a longer period (lower frequency) and higher amplitude. Remember how in class we discussed how removing introns reduced delays and affected oscillator period.\n\n\nExercise 4.5 (Algorithmic thinking) In the file rolling_clock.py, there is a different implementation of the clock. Compare the two files and find out how they differ. What benefits for studying the model does clock.py have over rolling_clock.py and vice versa? Ignore the added functions __copy__ and set_tau in this comparison. Some differences become clearer when you run the code too.\n\n\nAnswer Working with delay functions implies that you need the value of a variable not on the current time but on time-\\(\\tau\\). In the previous model we simply memorized the entire history of both variables. Here instead we only store the variable state for the length of the time delay \\(\\tau\\). This saves memory and hence runtime. This will become especially important when simulating a larger number of individual cells each having their own clocks. In the plotting we only plotted stored values, hence you only see what happened between current_time-\\(\\tau\\) and current_time instead of what happened from start of simulation till current time. Of course if you want to dynamically change \\(\\tau\\) you need to make sure you store values over an interval corresponding to the largest possible value of \\(\\tau\\) and need to know this beforehand.\n\n\nExercise 4.6 Next, we will combine the clock and FGF wavefront into a single model.\nRead and interpret existing code: Read the code of clockplusfgf.py and try to understand the assumptions from this model implementation by answering the following questions:\n\nEach cell has its own clock. What clock states is the tissue initialized with? And what clock states do newly divided cells get? 2. The FGF wavefront affects \\(\\tau\\) : what function is used for that? What are your expectations for the effect of the FGF wavefront on the cells‚Äô clocks? 3. Does growth affect the clock state? 4. What is your opinion on the assumptions discussed in the three questions?\n\n\n\nAnswer\n\nGoing to code rolling_clock.py we see that clocks are initialized with the init function of the Class Clock, which gets as arguments \\(m0\\) and \\(p0\\), this initialization function is called when in Class Cell, in its respective __init__ function self.clock=Clock(..) is called. It uses default values of \\(m=0\\) and \\(p=0\\) to pass to the Clock. (In turn, the initialization function of Class Cell is called from the initialize_regular_tissue function of Class Tissue when the first cell is created). Newly divided cells inherit clock state and and FGF value from mother cell.\nThe function is: \\[\\tau_\\text{cell}=\\tau_\\text{model}(1+0.5\\frac{100-\\text{FGF}\\_\\text{cell}}{100}),\\] So if localfgf=max=100, \\(\\tau{local}\\)=\\(\\tau{global}\\), and if localfgf=min=0 \\(\\tau{local}\\)=1.5*\\(\\tau{global}\\) So for lower FGF \\(\\tau\\) increases slowing down the oscillations.\nNot directly, growth of cells, leading to volume increase and potential dilution effects is not affecting m or p levels, however since it is affecting FGF levels it is indirectly affecting the clock.\nInitialization is somewhat arbitrary, inheritance from mother cells is logical although in reality it is of course not perfect but noisy, FGF effect is also reasonable to get the observed slowing and waves, but it would be more logical for clock state like fgf to also be diluted if cells grow their volume\n\n\n\nExercise 4.7 (Biology) Describe how the model behaves and why. Do we get stable somites?\n\n\nAnswer First there is a startup period where we have synchronized oscillations in the first formed cells that all still have high FGF. Then as the tissue grows and the FGF gradient forms we obtain waves of oscillations moving from right to left with increasing amplitudes. The right to left movement arises because oscillations are faster on the right than on the left, this difference in oscillation frequency arises from the impact of FGF on oscillator frequency. However, the oscillations do not become halted and transformed into a stable spatial pattern because there is no memory mechanism in place.\n\n\nExercise 4.8 (Biology & Programming) What is still missing is a means to transform the temporal oscillations in the posterior of the tissue into a spatial pattern in the anterior. In the French Flag morphogen gradient lecture we discussed that memory mechanisms are important to stabilize spatial patterns once the start up signal that broke the symmetry and initialized patterning has gone. It turns out that such a memorization/stabilization mechanism is also essential to convert oscillations to stripes. Add a memorization mechanism to the model to achieve this. Hint: Make use of an extra ‚Äòmemory‚Äô molecule \\(M\\) in each cell and perform its updating inside the function run_clocks. To see what you are doing, visualize the spatial pattern of \\(M\\)over time following the same procedure as for visualizing thefgf` and \\(p\\) values. Think about where, when and what should be memorized to design how \\(M\\) is regulated and what \\(M\\) itself affects and take a stepwise approach.\n\nAdd an extra ‚Äòmemory‚Äô molecule \\(M\\) as a new attribute to the Class Cell, which value is inherited from the mother cell upon division. To create memory we need bistability, which can be easily achieved by having the memory molecule having a non-linear saturating positive feedback on itself. However, doing only this it will depend on the initial value of \\(M\\) -below or above a threshold- whether \\(M\\) will autoactivate or not. Additionally, if it happens it will occur across the entire tissue.\nThus, we need to make \\(M\\) dependent on the clock. To achieve this, we can start with low values of \\(M\\) that prevent autoactivation, and then have \\(M\\) activated by either \\(p\\) (part of the clock) or \\(M\\) itself using the following function: \\[\\frac{dM}{dt}=c\\max\\left(\\frac{p^4}{h_p^4+p^4}, \\frac{M^4}{h_M^4+M^4}\\right)-\\delta M.\\]\nHowever, now we simply always get activation of \\(M\\) everywhere.\nSince oscillations occur in the PSM and stripe formation occurs only more anteriorly we should constrain memorization to occur only below a certain FGF value. As a consequence, high levels of \\(M\\) now only arise anteriorly, but there is no pattern yet and oscillations keep occurring. This is because cells cycle through high and low p values so all cells at some point have high \\(p\\) values and can induce \\(M\\).\nSo in addition to have memorization occur only below a certain FGF value it should also occur above another, lower FGF value, constraining it to occur in a limited temporal window that enables cells passing through there with a high \\(p\\) state to induce a high \\(M\\) state while cells pasing through with a low \\(p\\) state to not induce a high \\(M\\) state. You should get somite patterns that are stable, but not necessarily regular. Still \\(m\\) and \\(p\\) oscillations continue.\nAs a final step, beyond the FGF window where we memorize \\(M\\) we no longer update either \\(M\\) or the clock.\n\n\n\nAnswer See code 05_answers_clockplusfgf_memory.py\n\n\nExercise 4.9 (Biology) Both zebrafish and mice are common model organisms, so we know a lot about the biological parameters of their somitogenesis. See the following list:\n\n\n\n\n\n\n\n\n\n\nParameter\nZebrafish\nReference\nMouse\nReference\n\n\n\n\nDuration of somitogenesis\n18 h\n(a)\n5 days\n(b)\n\n\nNumber of somites\n~30\n(a)\n65\n(c)\n\n\nSomite size\n50 \\(\\mu\\) / 30 \\(\\mu\\)\n(d) (a)\n120 \\(\\mu\\)\n(c)\n\n\nCells per somite\n~5 cell in diameter\n(a)\n5-11 (estimated from total cell size in 3D, ranging 1 order of magnitude)\n(c)\n\n\nClock period\n25 min /30 min\n(d) / (e) / (f)\n2-3 h\n(e)\n\n\n\nFrom these parameters, you can derive a number of desired model inputs/outcomes:\n\nThe total size of the tissue at the end of somitogenesis\nThe size of a cell\nSpeed of division\n\nUse the model and try to find suitable model parameters to recreate the development of both zebrafish and mice: is this model able to describe both of these processes? I.e., is this model able to deal with the scale differences between zebrafish and mice?\nA couple of notes:\n\nYou don‚Äôt have to exactly recreate the biological parameter with 100% precision, except for the number of segments/somites, although it can be a fun challenge to get a complete match.\nYou might want to adapt your plotting timestep to have sufficient but not too many plot updates in one simulation.\nYou can test the clock parameters separately with the clock.py/rolling_clock.py script. Don‚Äôt forget that FGF has an effect on \\(\\tau\\) !\n\n\n\nAnswer See codes 06_answers_clockplusfgf_memory_mouse.py and 06_answers_clockplusfgf_memory_zebrafish.py.\nZebrafish:\nTotaltime: 18 hours for duration of somitogenesis + 10 hours startup phase.\nCell width= 30-50 \\(\\mu\\) sized somite/5 cells per somite = 6-10 so 8\nGrowthrate: 30 somites x 5 cells x 8\\(\\mu\\) / 18 hours = \\(1200\\)\\(/18*3600\\)s\nFor clock frequency:\n\\(\\tau\\) times 0.25\nFac = 6 (multiplies alpha, beta, mu, v)\nThis gives 32/31 somites with on average 4.9 cells, so perfect.\nMouse:\nSimply doing the computations would give you:\nTotaltime: 5 days for duration of somitogenesis + 15 hours startup phase\nCell width = 120\\(\\mu\\) sized somite/5-11 cells = \\(120/8=15\\)\nGrowthrate: 65 somites x 8 cells x 15\\(\\mu\\) / 5 days = \\(7800/5* 24 *3600\\)s\nFor clock frequency:\n\\(\\tau\\) times 1.4-1.6 is sufficient (Fac=1 (multiplies alpha, beta, mu, v))\nSomehow this does not fully work:\nToo few cells (not \\(65*8=520\\)) too short tissue (not \\(65*8*15=7800\\)) and only 24 segments. First, remember \\(\\tau_{local}=(1+0.5 *(100-localfgf)/100))*\\tau_{global}\\). So \\(\\tau_{local}\\) max 1.5 times \\(\\tau_{global}\\) so the also increasing \\(\\tau_{global}\\) 1.4-1.6 times 1.5 makes it really slow. Indeed \\(\\tau\\) times 1.2 gives 31 segments, \\(\\tau\\) times 1 gives &gt; 36 segments, \\(\\tau\\) times 0.7 gives 49 segments\nGood to realize that the numbers in the table do not completely add up:\nIf it takes 5 days =\\(5*24=120\\) hours, and the period is 2-3 hours this would result in 40-60 segments but never 65. For a duration of 5.5 days and \\(\\tau\\) times 0.6 we get to 62 somites of size 9, so quite close.\n\n\nExercise 4.10 (Questions for master students) The model currently has a number of assumptions that we can question. Feel free to explore any of these further open questions and study how it effects the outcome of the model:\n\nWhat if the relationship between FGF and \\(\\tau\\) is shaped differently? For instance, if the clock runs faster on the left/anterior than on the right/posterior?\nWhat if the clock state of a daughter cell is started fresh rather than being a copy from the mother cell?\nWhat if the clock \\(m\\) and \\(p\\) are diluted by growth?\nWhat if \\(\\tau\\) is unaffected by FGF: can we still get somites fixed in place?\n\n\n\nAnswer These are open questions and do not have a single correct answer. Feel free to discuss with us if you find something interesting or have an interesting answer.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html#relevant-literature",
    "href": "pattern_practical_3.html#relevant-literature",
    "title": "4¬† Clock-and-wavefront patterning",
    "section": "4.5 Relevant literature",
    "text": "4.5 Relevant literature\nIf you want to know more about the model system and previous models, have a look at the following (after the tutorial):\nLewis (2003) Autoinhibition with transcriptional delay: a simple mechanism for the zebrafish somitogenesis oscillator.\nHester (2012) A Multi-cell, Multi-scale Model of Vertebrate Segmentation and Somite Formation.\nHerrgen et al. (2010) Intercellular coupling regulates the period of the segmentation clock.\nSoroldoni et al. (2014) Genetic oscillations. A Doppler effect in embryonic pattern formation.\nSonnen et al. (2018) Modulation of Phase Shift between Wnt and Notch Signaling Oscillations Controls Mesoderm Segmentation.\nBulusu et al. (2017) Spatiotemporal Analysis of a Glycolytic Activity Gradient Linked to Mouse Embryo Mesoderm Development.\nOostrom et al. (2025) Coupling of cell proliferation to the segmentation clock ensures robust somite scaling.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "pattern_practical_3.html#references-for-biological-parameters",
    "href": "pattern_practical_3.html#references-for-biological-parameters",
    "title": "4¬† Clock-and-wavefront patterning",
    "section": "4.6 References for biological parameters:",
    "text": "4.6 References for biological parameters:\n\nStickney, Barresi, and Devoto (2000)\nSaga (2012)\nTam (1981)\nIshimatsu et al. (2018)\nCarraco, Martins-Jesus, and Andrade (2022)\nTomka, Iber, and Boareto (2018)\n\n\n\n\n\nBulusu, Vinay, Nicole Prior, Marteinn T Snaebjornsson, Andreas Kuehne, Katharina F Sonnen, Jana Kress, Frank Stein, Carsten Schultz, Uwe Sauer, and Alexander Aulehla. 2017. ‚ÄúSpatiotemporal Analysis of a Glycolytic Activity Gradient Linked to Mouse Embryo Mesoderm Development.‚Äù Developmental Cell 40 (4): 331‚Äì41.\n\n\nCarraco, Gil, Ana P Martins-Jesus, and Raquel P Andrade. 2022. ‚ÄúThe Vertebrate Embryo Clock: Common Players Dancing to a Different Beat.‚Äù Frontiers in Cell and Developmental Biology 10: 944016.\n\n\nHerrgen, Leah, Sa√∫l Ares, Luis G Morelli, Christian Schr√∂ter, Frank J√ºlicher, and Andrew C Oates. 2010. ‚ÄúIntercellular Coupling Regulates the Period of the Segmentation Clock.‚Äù Current Biology 20 (14): 1244‚Äì53.\n\n\nHester, Susan D. 2012. ‚ÄúMulti-Scale Cell-Based Computational Models of Vertebrate Segmentation and Somitogenesis Illuminate Coordination of Developmental Mechanisms Across Scales.‚Äù PhD thesis, Indiana University.\n\n\nIshimatsu, Kana, Tom W Hiscock, Zach M Collins, Dini Wahyu Kartika Sari, Kenny Lischer, David L Richmond, Yasumasa Bessho, Takaaki Matsui, and Sean G Megason. 2018. ‚ÄúSize-Reduced Embryos Reveal a Gradient Scaling-Based Mechanism for Zebrafish Somite Formation.‚Äù Development 145 (11): dev161257.\n\n\nLewis, Julian. 2003. ‚ÄúAutoinhibition with Transcriptional Delay: A Simple Mechanism for the Zebrafish Somitogenesis Oscillator.‚Äù Current Biology 13 (16): 1398‚Äì408.\n\n\nOostrom, Marek J van, Yuting I Li, Wilke HM Meijer, Tomas EJC Noordzij, Charis Fountas, Erika Timmers, Jeroen Korving, Wouter M Thomas, Benjamin D Simons, and Katharina F Sonnen. 2025. ‚ÄúScaling of Mouse Somitogenesis by Coupling of Cell Cycle to Segmentation Clock Oscillations.‚Äù bioRxiv, 2025‚Äì01.\n\n\nSaga, Yumiko. 2012. ‚ÄúThe Mechanism of Somite Formation in Mice.‚Äù Current Opinion in Genetics & Development 22 (4): 331‚Äì38.\n\n\nSonnen, Katharina F, Volker M Lauschke, Julia Uraji, Henning J Falk, Yvonne Petersen, Maja C Funk, Mathias Beaupeux, Paul Fran√ßois, Christoph A Merten, and Alexander Aulehla. 2018. ‚ÄúModulation of Phase Shift Between Wnt and Notch Signaling Oscillations Controls Mesoderm Segmentation.‚Äù Cell 172 (5): 1079‚Äì90.\n\n\nSoroldoni, Daniele, David J J√∂rg, Luis G Morelli, David L Richmond, Johannes Schindelin, Frank J√ºlicher, and Andrew C Oates. 2014. ‚ÄúA Doppler Effect in Embryonic Pattern Formation.‚Äù Science 345 (6193): 222‚Äì25.\n\n\nStickney, Heather L, Michael JF Barresi, and Stephen H Devoto. 2000. ‚ÄúSomite Development in Zebrafish.‚Äù Developmental Dynamics: An Official Publication of the American Association of Anatomists 219 (3): 287‚Äì303.\n\n\nTam, PPL. 1981. ‚ÄúThe Control of Somitogenesis in Mouse Embryos.‚Äù Development 65 (Supplement): 103‚Äì28.\n\n\nTomka, Tomas, Dagmar Iber, and Marcelo Boareto. 2018. ‚ÄúTravelling Waves in Somitogenesis: Collective Cellular Properties Emerge from Time-Delayed Juxtacrine Oscillation Coupling.‚Äù Progress in Biophysics and Molecular Biology 137: 76‚Äì87.",
    "crumbs": [
      "I) Pattern formation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Clock-and-wavefront patterning</span>"
    ]
  },
  {
    "objectID": "morpho_intro_text.html",
    "href": "morpho_intro_text.html",
    "title": "5¬† Introduction to Morphogenesis",
    "section": "",
    "text": "Cells collectively organize into higher-order structures, forming tissues and organs. The process by which tissues are given their shape is called morphogenesis, which is Greek for the generation of form. Gene regulation, (sub-)cellular effectors, and tissue-scale mechanics are the three ingredients of morphogenesis (Collinet and Lecuit (2021) Gilmour, Rembold, and Leptin (2017)). Regulation between these players goes both ways: Genes may determine expression of cytoskeletal regulators, which in turn exert forces resulting in tissue reshaping. Vice-versa, both tissue shape and forces can feed back on cytoskeletal proteins, which feed back to gene expression. Earlier in this course, you learned about patterning. In this part we‚Äôll pick up from there to learn how cells and tissue mechanics interplay to generate ordered structures.\nDespite their great internal molecular complexity, cells do a few basic things: Grow, divide, die, move around, change their shape, adhere to each other, and secrete or absorb material. This repertoire of cell behaviors is used to build higher-order structures. Additionally, non-cellular components which are generated and regulated by cells also contribute to shaping tissues and organs: Osmotic pressure and extracellular matrix proteins. For instance, bones are shaped by a specialized, stiff extracellular matrix, while the inside of eyes is filled with an osmotically active, jelly-like fluid that exerts pressure that maintains the shape of light-sensitive tissues, much like an inflated balloon.\nThe relatively small number of processes occurring at the cellular level has inspired the development of various computational models that treat cells as the central unit. In this chapter of the course, you‚Äôll learn about these cell-based models.\n\n\n\n\nCollinet, Claudio, and Thomas Lecuit. 2021. ‚ÄúProgrammed and Self-Organized Flow of Information During Morphogenesis.‚Äù Nature Reviews Molecular Cell Biology 22 (4): 245‚Äì65.\n\n\nGilmour, Darren, Martina Rembold, and Maria Leptin. 2017. ‚ÄúFrom Morphogen to Morphogenesis and Back.‚Äù Nature 541 (7637): 311‚Äì20.",
    "crumbs": [
      "II) Morphogenesis",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Morphogenesis</span>"
    ]
  },
  {
    "objectID": "morpho_cpm_practical.html",
    "href": "morpho_cpm_practical.html",
    "title": "6¬† Practical 4",
    "section": "",
    "text": "6.1 Morphogenesis\nMorphogenesis is the culmination of gene expression, biochemical signaling, and biophysical forces at the cell and tissue levels. These processes are complex and not completely understood, so we don‚Äôt yet have a ‚Äúperfect‚Äù model of how we should simulate morphogenesis. For this reason, several research groups independently developed simulation approaches that fit their needs. In this practical, you will get acquainted with one such approach, the cellular Potts model.",
    "crumbs": [
      "II) Morphogenesis",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Practical 4</span>"
    ]
  },
  {
    "objectID": "morpho_cpm_practical.html#biological-background-cell-sorting-by-differential-adhesion",
    "href": "morpho_cpm_practical.html#biological-background-cell-sorting-by-differential-adhesion",
    "title": "6¬† Practical 4",
    "section": "6.2 Biological background: Cell sorting by differential adhesion",
    "text": "6.2 Biological background: Cell sorting by differential adhesion\nClassical experiments in the 1950s dissociated animal embryonic tissues and recombined them in vitro to try to understand the principles of self-organization. Instead of staying mixed, each cell type sorts out into its own region, resembling the organization in the embryo.\n\n\n\nSorting of aggregates of embryonic cells, based on experiments by Townes and Holftreter in 1955. Image adapted from Gilbert ‚ÄúDevelopmental Biology‚Äù 11th edition.\n\n\nAccording to the ‚Äúdifferential adhesion hypothesis‚Äù, unlike expression levels of cell-cell adhesion molecules is the mechanistic driver of cell sorting. Biophysically, cells form aggregates that minimize the contact energy at interfaces, rearranging into the most thermodynamically stable pattern. In 1992, the cellular Potts model was developed to test this hypothesis using simulations (Graner and Glazier (1992)).",
    "crumbs": [
      "II) Morphogenesis",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Practical 4</span>"
    ]
  },
  {
    "objectID": "morpho_cpm_practical.html#the-cellular-potts-glazier-graner-hogeweg-model",
    "href": "morpho_cpm_practical.html#the-cellular-potts-glazier-graner-hogeweg-model",
    "title": "6¬† Practical 4",
    "section": "6.3 The cellular Potts / Glazier-Graner-Hogeweg model",
    "text": "6.3 The cellular Potts / Glazier-Graner-Hogeweg model\nThe cellular Potts Model (CPM), also known as Glazier-Graner-Hogeweg (GGH) model after its major developers and popularizers, simulates stochastic dynamics of cell shapes on a grid (=a lattice). The model has its historical roots in physical models of magnetization of metals and formation of foam bubbles, so some of the terminology harkens back to that. Each grid site on the lattice has a spin value œÉ (Greek small letter sigma), usually an integer number. A biological cell is represented by all lattice sites with equal value œÉ.\n\n\n\nExample of a œÉ field with four unique integer values. The value 0 is reserved for the medium, while the non-zero values represent three cells.\n\n\nCell shapes change by elementary changes called copy attempts, where one site of the lattice (the source site xS) attempts to copy its œÉ value to a neighboring lattice site (the target site xT). Copy attempts can succeed (be accepted) or fail (be rejected).\n\n\n\nEvery copy attempt affects a pair of neighboring lattice sites.\n\n\nChoosing and accepting copy attempts occurs via a stochastic Monte Carlo method. More specifically, a modified Metropolis-Hastings algorithm, which is an algorithm that minimizes an energy function H calculated based on the lattice configuration. The probability that a copy attempt is accepted P(accept) depends on the change in energy ŒîH between the configuration before and after a copy attempt:\n\n\n\nEquation 1.\n\n\nIn the above equation, T is a parameter that determines how likely it is that an unfavorable copy attempt that increases the energy is accepted, while H0 is a parameter representing the yield, that is, how easily the cell membrane changes shape (often, this parameter is set to H0 = 0). What this equation says is: Configurational changes that lower the energy below at least H0 are always accepted (probability of 1), while configurational changes that increase the energy are accepted with a probability that decreases exponentially with the energy cost. In a biological sense, cells consume energy from ATP to create configurations that are physically unfavorable (ŒîH ‚â• -H0). Therefore, the parameter T can be interpreted as intrinsic cell activity.",
    "crumbs": [
      "II) Morphogenesis",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Practical 4</span>"
    ]
  },
  {
    "objectID": "morpho_cpm_practical.html#exercises",
    "href": "morpho_cpm_practical.html#exercises",
    "title": "6¬† Practical 4",
    "section": "6.4 Exercises",
    "text": "6.4 Exercises\n\nExercise 6.1 (Algorithmic thinking - Anatomy of a cellular Potts model) A minimal cellular Potts model simulation of cell sorting contains the following ingredients:\n\ndefinition of the ‚Äúsigma field‚Äù\ndefinition of the ‚Äútau field‚Äù\nthe initial condition of sigma and tau fields\nthe neighborhood of a grid site\na Monte Carlo stepping procedure\ndefinition and calculation of the energy function H\n\nStudy the code provided in the script 01_cpm.py. Then, answer the following questions:\n\nWhich functions in the script correspond to the above ingredients?\nWhat is the default initial condition?\nWhat is the default neighborhood?\nRun the script and observe the plots that appear. Explain in your own words what is the sigma field and what is the tau field. Do you understand why two different fields are needed?\n\n\n\nAnswer TBD\n\n\nExercise 6.2 (Conceptual thinking - Interpreting the Hamiltonian energy function) The Hamiltonian energy function H is at the heart of a CPM/GGH simulation. H reflects our assumptions about the system and how forces affect the cell (the derivative of energy with respect to position is force).\nIn the script 01_cpm.py, the following Hamiltonian is used.\n\n\n\nEquation 2.\n\n\nwhere J is a symmetric matrix of interfacial energies for all cell type pairs œÑ (small Greek letter tau), A(œÉ) is the area sum of lattice sites belonging to spin œÉ, A0,œÑ(œÉ) is the ‚Äútarget‚Äù area of the corresponding cell type œÑ, and ŒªA(œÑ(œÉ)) is a weighting factor that scales the importance of this term (also known as Lagrange multiplier). Each term represents a constraint.\nPerhaps a more intuitive way to understand the terms is the following pseudo-code:\n\nCompare the pseudo-code to the implementation in the script. Then answer the following questions:\n\nWhat are the biological processes modelled with this specific energy function?\nTake a closer look at the summation terms. Over which domain is the first sum calculated? What about the second summation term?\n\n\n\nAnswer TBD\n\n\nExercise 6.3 (Conceptual thinking - How parameters affect the Hamiltonian energy function) The modified Metropolis algorithm used in the Monte Carlo stepping procedure ensures that over time, on average, the sum of the energy of the entire sigma field will reach a minimum. However, the different Hamiltonian terms may work ‚Äúagainst‚Äù each other: minimizing one term may lead to maximizing the value of the other term.\nLet‚Äôs vary the parameters that affect the Hamiltonian terms to get an intuition for how they work. In the following, you may want to take screenshots of the final simulation state to put in your notes, so that it‚Äôs easier to compare the outputs.\n\nChange the default neighborhood to ‚Äú8‚Äù and run the simulation again. What happens?\nKeep the neighborhood at ‚Äú8‚Äù, and increase the parameter for ‚Äúvolume_weight‚Äù from [1, 1] to [2, 1]. Then, run the simulation again. What happens?\nChange the initialization mode (‚Äúinit_mode‚Äù) from ‚Äúrandom_pixel‚Äù to ‚Äúsquare_grid‚Äù, and reset the parameter ‚Äúvolume_weight‚Äù to [1, 1]. What happens now if you simulate?\nExplain why these settings and parameters that you just changed affect the simulation outcome.\n\n\n\nAnswer TBD\n\n\nExercise 6.4 (Algorithmic thinking - The Monte Carlo update algorithm) In addition to showing a visualization of the œÉ and œÑ fields, the script 01_cpm.py also prints some information to the terminal.\nReset all parameters to their default values, then, based on the printed information, answer the following questions:\n\nHow many copy attempts are made in total in one Monte Carlo Step?\nHow many of these copy attempts are accepted on average?\n\nCalculate the average for 5 Monte Carlo Steps across 3 simulation runs.\n\nHow many seconds does it take to run one simulation?\n\nCalculate the average run time for 3 runs.\n\n\nThe script 01_cpm.py uses the traditional update algorithm. In this algorithm, every copy attempt picks a target site randomly among all lattice sites, then subsequently picks a source site randomly among the neighbors of the target site. This leads to many invalid lattice pairs that are always rejected. The edge list algorithm is a clever way of speeding up this procedure by restricting copy attempts only to those interfaces where there is a possibility of acceptance.\n\n\n\nSince a copy attempt affects a pair of neighboring lattice sites, we can keep track of valid pairs (productive site pairs). These can be represented by an edge, i.e.¬†an arrow pointing from one site to the other. Edges have a directionality, so two neighboring sites are always joined by two edges with opposing direction.\n\n\nThe script 02_cpm_edgelist.py implements the edge list algorithm. It‚Äôs quite complicated, so don‚Äôt worry about understanding all the technical details. Using this script, answer the above questions again and compare to the results you got with the traditional update algorithm.\nExplain: Why does the edgelist algorithm reduce computation time? In which situations would you expect the difference in computation time between algorithms to be large?\n\n\nAnswer TBD\n\n\nExercise 6.5 (Biology - Expanding the Hamiltonian) Suppose you want to add more mechanisms for cell shape changes to the model. This would involve the following steps:\n\nComing up with a hypothesis on how forces act on the cell shape.\nDeriving a mathematical function that establishes the energy balance.\nAdd an additional function to the code that calculates the energy based on the cell configuration.\nCall this function in the code that calculates the energy differential.\n\nLet‚Äôs walk through these steps together to create a perimeter constraint.\n\nAnimal cells possess a contractile actomyosin cortex connected to the cell membrane via actin-membrane linker proteins. Cortex contraction is regulated to maintain homeostasis of the membrane tension. Simply put, if the cell membrane gets ‚Äúfloppy‚Äù, the cortex contracts to pull the membrane in. Vice-versa, if the membrane is too tense, cortex contractility is reduced to allow the membrane to relax. We thus could advance the hypothesis that forces from the actin cortex strive to maintain a constant cell surface area.\nWe want cell surface area to reach a target homeostatic value S0. Let‚Äôs call the actual cell surface area S. We need a function that has a minimum where S0 = S and increases if S is larger or smaller than S0. A simple function that does the trick is the parabola (S- S0)2. We may also want to tune how much this term influences the entire Hamiltonian, so we introduce the weight factor ŒªS. We also consider that both S0 and ŒªS may depend on cell type œÑ.\n\nThus, we write:\n\n\n\nEquation 3.\n\n\n\nFor the implementation of steps 3 and 4, consult the script 03_cpm_perimeter.py.\nStudy the new additions to the script (search for the string: ### NEW ### to find them quickly), then run the simulation. Vary the parameters to gain an intuition of how this new Hamiltonian term affects the simulation outcome.\nYou may have noticed the simulation is running more slowly than before. Review the new code additions again. Can you identify any potential reasons for this decrease in computation speed?\n\n\n\nAnswer TBD\n\n\nExercise 6.6 (Biology - Imposing cell connectivity**) In the previous simulations you ran, you may have noticed that cells sometimes break up into disjointed fragments. There are two alternative ways to prevent this: 1. Add a new Hamiltonian term that penalizes copy attempts that would break up the cell. 2. Modify the update algorithm to always reject copies that would break cells apart.\nScript 04_cpm_connectivity.py implements the second option using a flood fill algorithm to count the number of contiguous pixels of the non-medium target cell supposing that a copy succeeded.\nTry out the simulation!\n\nCan you think of advantages and disadvantages to using the first or second option?\nOnce again, the simulation is running more slowly than before. What parts of the new code do you suspect may be causing the slowdown?\n\n\n\nAnswer TBD\n\n\nExercise 6.7 (Biology - Exploring differential adhesion and cell sorting**) Now let‚Äôs explore how changing adhesion affinities leads to different sorted cell configurations. To do that, you will need to run simulations for a longer time. To speed up computations, you will be using an optimized package (many scripts working together), which is contained in the folder ‚ÄúcellularPotts‚Äù. To make visualizations super-fast, this package uses the PyQt5 module, which should be installed by default with your Anaconda distribution.\nNote: If for some reason you cannot get PyQt5 to work, there is an alternative package that uses matplotlib ‚ÄúcellularPotts_MPL‚Äù. However, this runs much slower.\nTo launch the simulation, run the main.py file in the cellularPotts folder. You can change the parameters by adjusting values in the parameters.py file.\nAdjust the values of the adhesion table for the two cell types:\n\nSet equal adhesion affinities for all cell-cell interactions. Leave affinity to the medium at a low value.\nSet the adhesion affinity of cell type 1 to itself to a larger value.\nSet the affinities such that cell-medium interactions are more favorable than any cell-cell interactions.\nExplore how other parameters such as volume and surface constraints affect the cell sorting simulations.\n\n(Master students) Add a third cell type to the simulation by editing the parameter.py file. Can you get a sorted configuration where cell type 1 envelops cell type 2, which itself envelops cell type 3?\n\n\nAnswer TBD\n\n\nReferences\n[1] Gilbert and Barresi (2016) (https://utrechtuniversity.on.worldcat.org/oclc/1035852599)\n[2] Graner and Glazier (1992) (https://doi.org/10.1103/PhysRevLett.69.2013)\n\n\n\n\nGilbert, SCOTT F, and MJF Barresi. 2016. ‚ÄúDevelopmental Biology. 11th Edn, 372.‚Äù Sinauer Associates.\n\n\nGraner, Fran√ßois, and James A Glazier. 1992. ‚ÄúSimulation of Biological Cell Sorting Using a Two-Dimensional Extended Potts Model.‚Äù Physical Review Letters 69 (13): 2013.",
    "crumbs": [
      "II) Morphogenesis",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Practical 4</span>"
    ]
  },
  {
    "objectID": "differentiation_intro_text.html",
    "href": "differentiation_intro_text.html",
    "title": "7¬† Cell differentiation introduction",
    "section": "",
    "text": "Gene regulation in a bacteria: from genes to networks. Credit: RegulonDB, and DOI: 10.1371/journal.pone.0106479\n\n\nCell differentiation is the process in which cells acquire specialized functions, giving rise to a large array of different cell types. This happens in both single cell organisms (bacterial cells being in exponential growth or stationary phase), and multicellular organisms with different cell types organized in tissues and organs (think of blood cells, melanocytes, muscle cells, neurons, etc., in our bodies). This cell specialization results from regulated gene expression: genes are turned on and off in precise combinations, yielding distinct expression profiles. Therefore, cells with an identical genome can be phenotypically different if they express different sets of genes. Yet, how exactly is this differential gene expression regulated?\nTranscription factors are DNA-binding proteins that regulate gene transcription by binding to gene promoters to regulate expression, therefore acting as master switches of gene expression. In E. coli, plants and animals it is estimated that 5-10% of the genes are transcription factors, and then this relative small portion of the genome controls the expression of the rest of the genes. Numerous transcription factors together with signaling pathways (‚Ä¶ and epigenetic modifications) form gene regulatory networks that govern when and where genes are expressed. Computational models are useful to study how transcription factor activity is regulated, and describe the mechanisms underlying cell identity and behavior. For example, they can help us decipher how gene regulatory networks produce the gene expression patterns observed in normal and abnormal development (where cells do not differentiate normally), or identify information processing motifs in the network that help cells make decisions in response to intrinsic cues and environmental signals.\nIn this part of the course, you will learn how to use computational models to simulate gene regulatory networks and to understand how their dynamics generate gene expression patterns that establish and maintain different cell types.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Cell differentiation introduction</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_1.html",
    "href": "differentiation_practical_1.html",
    "title": "8¬† Gene regulation in time",
    "section": "",
    "text": "8.1 Introduction\nIn this practical you will get hands on experience into making gene regulatory networks models. First you will practice how to encode regulatory interactions into ordinary differential equations and logical rules. Next you will study how many interactions produce self-sustained activity configurations (attractors) in a Boolean network model. Lastly, you will learn how to predict the genes that cause cell fate transitions (attractor changes).\nToday we will use the following python file: circuitsfunctions.py. In it you will find the code of the functions we use throughout the practical, have a quick look at it.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Gene regulation in time</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_1.html#regulatory-interactions",
    "href": "differentiation_practical_1.html#regulatory-interactions",
    "title": "8¬† Gene regulation in time",
    "section": "8.2 Regulatory interactions",
    "text": "8.2 Regulatory interactions\nOpen Boolean_practical.py and familiarize yourself with the functions ODEgeneRegulation() and logicalRule(). Look at the name of the functions, inputs and outputs.\nBoth functions model an AND gate where nodeA and nodeB positively regulate nodeC. This could represent that A and B are transcription factors that form a protein complex, and that it is via this complex that they can regulate the expression of C.\nWhile ODEgeneRegulation() encodes this regulation with an ordinary differential equation that requires several parameters, logicalRule() only needs the logical operator relating the input genes.\nODE implementation\n\\(\\frac{dC}{dt}=p\\frac{A^n}{K^n+A^n}\\frac{B^n}{K^n+B^n}-dC\\)\nWhere \\(p\\) is the production rate, \\(K\\) is the half saturation constant (represents the affinity of a TF to the promoter), \\(n\\) is the cooperativity of transcription factor binding, and is the same for \\(A\\) and \\(B\\), \\(d\\) is the decay rate.\nLogical rule implementation\n\\(C_{t+1}=A_t\\) & \\(B_t\\)\nPython code for ODE and logical rule\ndef ODEgeneRegulation(a,t,parameters): \n    prod=parameters['prod']\n    decay=parameters['decay']\n    Ksat=parameters['Ksat']\n    nodeA=parameters['nodeA']\n    nodeB=parameters['nodeB']\n    n=parameters['n']\n    outputC=a[0]\n    doutputC=prod*nodeA**n/(Ksat**n+nodeA**n)*nodeB**n/(Ksat**n+nodeB**n)-decay*outputC #Tip: Change this in exercise 8.3\n    return(doutputC)\n\ndef logicalRule(nodeA,nodeB):\n    return(nodeA and nodeB)\n\nExercise 8.1 (Mathematical thinking) What is the minimum information you need to encode a regulatory interaction with either function? In what cases would you prefer to use an ODE or a logical rule for a model?\n\n\nAnswer To model a regulatory interaction with a logical function we need the information of who is regulating who (direction of interactions), and how (effect of interaction); no parameters needed. To model the same with an ODE we also need several parameters: production rate, decay rate, K, n.¬†\n\nLet‚Äôs run the model to simulate what happens if A and B are both active/expressed. Do this using the ODEgeneRegulation() and logicalRule() functions. Create a new python file, import circuitfunctions.py and copy the following lines of code inside the main().\n# ODErun has three arguments: model, geneA, geneB\nA=10\nB=10\nODErun(ODEgeneRegulation,A,B) \n\n# Boolean model\n# look at the text in the terminal for the result:\nA=1\nB=1\nprint(\"the boolean operation of nodeA \",A,\" AND nodeB\",B,\" is:\", logicalRule(A,B))\nBoth implementations of this regulatory interaction produce the same effect, i.e.¬†C is active when both A and B are active. Yet, the ODE version recovers quantitative differences in C activity with values higher than 1.\nNow, let‚Äôs explore the output of each function using different values of A and B We are going to use the code below to plot the output of the ODE and the Boolean model in a 2D heatmap. Notice that the exploration values for the ODE model is 11 values (0,1,2,3,4,5,6,7,8,9,10), while for a Boolean model is 2 (0 or 1).\nODE model simulation - AND gate:\nexplorationvalue = 11 # an ODE model allows us to explore more values than a Boolean model\node_output = np.zeros((explorationvalue, explorationvalue))\nfor nodeA in range(0, explorationvalue):\n    for nodeB in range(0, explorationvalue):\n        parameters = {'prod': 0.01, 'decay': 0.001,'Ksat': 4, 'n': 2,'nodeA':nodeA,'nodeB':nodeB} # prod, decay, Ksat, n, and initial values for A and B\n        cells = odeint(ODEgeneRegulation, 0, np.arange(0, 1000.1 , 0.1), args=(parameters,)) \n        ode_output[nodeA, nodeB] = cells[-1, 0]\nBoolean network simulation - AND gate:\nexplorationvalue=2 # a Boolean model assumes there is only 2 possible states: 0 or 1\nbool_output = np.zeros((explorationvalue, explorationvalue))\nfor nodeA in range(0, explorationvalue):\n    for nodeB in range(0, explorationvalue):\n        bool_output[nodeA, nodeB] = nodeA and nodeB #AND #Tip: Change this in exercise 8.3\nAfter running each, this function plots the ODE and Boolean results side by side:\nODEBooleanPlot(ode_output, bool_output)\n\nExercise 8.2 (Biology) What similarties and differences in the activity of \\(C\\) do you see using each approach? Why is the activity of \\(C\\) higher in the ODE output than the Boolean one?\n\n\nAnswer In both cases \\(C\\) has maximum activity when both \\(A\\) and \\(B\\) are active/expressed. Whereas in a Boolean model the activity of the variables can be either 0 or 1, in an ODE the variables can have &gt; continuous values with the maximum level determined by the production and the decay rate parameters (\\(p/d\\)). This is the reason \\(C\\) has values close to 5 in the ODE output. \n\n\nExercise 8.3 (Altorithmic thinking:) Now let‚Äôs compare how ODE and Boolean models represent different regulatory interactions. You can use the same code in the previous box. Remember to modify the logical operator in the Boolean section, and to modify the \\(doutputC\\) equation in the ODEgeneRegulation function:\n\nEither A or B can activate C.\nA represses C.\nA and B together repress C.\nA represses C and B activates it.\nA or B can activate C, but not when both are active at the same time.\nany other biological scenario yon can think of!\n\n\nSee if you can modify the code to include an extra input node, and make a 3-node logical gate.\n\n\n\nAnswer See Boolean_answered-8.3.py to see an implementation of the regulatory interactions listed above.\n\n\\(OR\\) gate: A \\(OR\\) B \n\\(NOT\\) gate: \\(NOT\\) A \ncombination of \\(NOT\\) and \\(AND\\) gate: \\(NOT\\) A \\(AND\\) \\(NOT\\) B \n\nlooks quite different if it is an \\(OR\\) gate: \\(NOT\\) A \\(OR\\) \\(NOT\\) B\n\n\nThere are two possibilities because it is not specified which interaction is ‚Äústronger‚Äù. Here we show when A is stronger. Option 1: \\(NOT\\) A \\(AND\\) B.\n\n\n\n\\(XOR\\) gate (A \\(XOR\\) B), meaning either but not both at the same time. \n\n\n\nExercise 8.4 (Biology and Algorithmic thinking) Is there a logical gate that can be better represented with an ODE than with Boolean logic? Give an\nexamples of biological regulatory interactions that can be described with each logical gate?\n\n\nAnswer All logical gates can be modelled correctly with either approach. With Boolean models we look quantitative resolution, with ODE we can model many more values than 0 and 1.\nBiological examples that can be modelled with the logical gates:\n\n\\(AND\\) gate can be that both genes code for transcription factors that form a protein complex that binds a promoter to regulate gene expression of \\(X\\). It can also represent that one regulator promotes the expression of \\(X\\), while the other phosphorylates protein \\(X\\). As the phosphorylated \\(X\\) is the active form, both regulatory interactions are needed (\\(AND\\)).\n\\(OR\\) is that either TF can bind promoter and activate transcription.\n\\(NOT\\) is that a TF blocks the promoter so that activating TF cannot bind.\n\\(NOT\\) A \\(AND\\) \\(NOT\\) B is that both TFs are repressors of the same gene.\n\\(NOT\\) A \\(AND\\) B is that A is a repressor and B is an activator.\n\\(XOR\\) gate can be a promoter where either A or B can bind, the complex AB cannot bind.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Gene regulation in time</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_1.html#gene-regulatory-network",
    "href": "differentiation_practical_1.html#gene-regulatory-network",
    "title": "8¬† Gene regulation in time",
    "section": "8.3 Gene regulatory network",
    "text": "8.3 Gene regulatory network\nNow let‚Äôs move to a network model made of many individual regulatory interactions (Garcƒ±ÃÅa-G√≥mez et al. (2020)). This model includes regulatory interactions discovered for the cells of plant roots in years of experimental research. Here, all this knowledge is encoded as logical rules.\nThe model consists of 18 variables representing transcription factors, hormones, peptides. Look at the rootNetwork() function in the circuitFunctions.py file. This function defines the state of 18 variables (CK, ARR1, SHY2, AUXIAAR, etc.) based on an input stored in parameters, then it updates the state of each variable using the logical operators \\(AND\\), \\(OR\\) and \\(NOT\\); the result is stored in w_variable. The function returns the update state w_variable for each of the 18 variables.\n\nInitial condition timecourse\nLet us define a random initial condition for each of these 18 variables , and the total number of timesteps we will solve the network using the logical functions. First, we need to input the initial condition of the variables to rootNetwork() using parameters. Next, we save the output of this function as the new current state. We repeat this procedure iteratively for as many timesteps as we defined, saving in a matrix the timecourse of the simulation. This can be done using a for loop as shown below:\ntimesteps=20\nvariables=18\nmatrix = np.zeros((timesteps+1, variables), dtype=int) \nmatrix[0,:] = np.random.randint(0, 2, size=variables) #Random initial condition\nfor i in range(timesteps):\n    parameters= {'CK': matrix[i,0], 'ARR1': matrix[i,1], 'SHY2': matrix[i,2], 'AUXIAAR': matrix[i,3], 'ARFR': matrix[i,4], 'ARF10': matrix[i,5], 'ARF5': matrix[i,6], 'XAL1': matrix[i,7], 'PLT': matrix[i,8], 'AUX': matrix[i,9], 'SCR': matrix[i,10], 'SHR': matrix[i,11], 'MIR165': matrix[i,12], 'PHB': matrix[i,13], 'JKD': matrix[i,14], 'MGP': matrix[i,15], 'WOX5': matrix[i,16], 'CLE40': matrix[i,17]}\n    \n    matrix[i+1, :] = rootNetwork(parameters) # we save the output in i+1\n\nplotBooleanTimecourse(matrix,timesteps)\nAt the end the function plotBooleanTimecourse() will make a plot to see how each variable in the network changes ts activity throughout the simulated timesteps. 100 timesteps is enough to reach an attractor for this network.\n\n\n\n\nTimecourse simulations\n\n\n\n\nFor the initial condition you can also use that of one of the attractors reported in the paper and check what happens when you update them.\n\n\nExercise 8.5 (Algorithmic thinking) Try this initial condition:\nmatrix[0,:]=[0,1,1,0,1,1,0,0,1,0,1,1,1,1,1,0,1,0]\nSolve this initial condition using rootNetwork() and rootNetworkAsynchronous(). Compare the output in each case. What happens and why? What does an asynchronous update do?\n\n\nAnswer The asynchronous updating function introduces a probability of update (95%) to each node in the simulation. This means the nodes might not be all updated each timestep. The initial condition converges to a cyclic attractor in the synchronous updating function (rootNetwork()), and to a fixed point attractor in the asynchronous function (rootNetworkAsynchronous()). This means the cyclic behaviour recovered with the synchronous updating scheme was an artifact of updating all the nodes at the same time.\n\nThis network has 18 variables, and then \\(2^{18}\\) = 262,144 possible states. We can solve each of these conditions, or instead explore just a subset of them.\n\nExercise 8.6 (Algorithmic thinking) Using the previous code, add for loop to solve 100 random initial conditions, and then save the final activity configurations (attractors) in the attractors matrix. The attractors matrix should have the dimensions\n(initial_conditions, variables) so that you can plot the results using the plotBooleanAttractors() function.\n\nNotice here you should save the final state for each initial condition you explore.\ntip: 100 initial conditions is a good number to explore.\n\nICs=100\nattractors = np.zeros((ICs, len(parameters))) \n\n# your code\n\nplotBooleanAttractors(attractors) # it takes as argument a matrix of attractors\n\n\nAnswer See Boolean_answered-8.6.py to see an implementation of this for loop.\n\n\n\nAttractors reached by 100 random initial conditions\n\n\n\nYou probably found many different attractors. First, let‚Äôs use a multidimensional reduction technique to see how these attractors relate to each other.\nUMAPBoolean(attractors)\n\nExercise 8.7 (Biology / Conceptual thinking) Q5 Why do you see clearly defined groups and not a continuous distribution of attractors?\n\n\nAnswer We see clearly defined groups because we are doing the UMAP with the cells that have already reached an attractor, and not while they are in their trajectory towards the attractor.\n\n\n\nTimecourse simulation: state of the variables (columns) through simulated time (rows).\n\n\n\nNow let‚Äôs analyse the attractors you found. First, let‚Äôs group similar attractors using the sorted() function:\nattractors_sorted = np.array(sorted(attractors.tolist()))\nplotBooleanAttractors(attractors_sorted) \n\n\n\n\nAttractors reached by 100 random initial conditions - clustered\n\n\n\n\nExercise 8.8 (Biology / Conceptual thinking) Q6 Do some attractors occur more frequently than others. Why is this?\n\n\nAnswer Yes, some attractors occur more often than others, and this is a consequence of the regulatory interactions of the model. The number of initial conditions that reached a specific attractor is a reflection of the size of their basin of attraction. The basins of attraction (all variable configurations that converge to an attractor) are shaped by the AND, OR, and NOT gates in each of the modelled variables. Imagine cells are like marbles moving through a landscape, such that the regulatory interactions define the ‚Äúrolling‚Äù direction, until eventually they land in different basins of attraction as they acquire different cell states. Here the landscape is multidimensional (as many dimensions as variables in the model, so 18).\n\n\nExercise 8.9 (Biology) Some variables are active (1) in the majority of the attractors, others in half, and others in very few. In how many attractors is the node SHR active? What does this suggest about its regulation?\n\n\nAnswer How often a variable is active in the attractors, is defined by their logical rules. For example, SHR logical rule is SHR = SHR. This means that SHR is an input node, that is not regulated by any other node. This splits the state space perfectly in two halves: one with SHR active and another with SHR inactive. You can implement this in Boolean_answered-8.3.py to visualize this split.\n\n\n\nSHR activity regulated by itself (variable A)\n\n\n\nNow let‚Äôs remove the repeated rows (duplicate attractor states) to see unique attractors using the np.unique() function:\n_, unique_indices = np.unique(attractors, axis=0, return_index=True)\nattractors_unique = attractors[np.sort(unique_indices)]\nplotBooleanAttractors(attractors_unique) \n\n\n\n\nUnique attractors reached by random 100 initial conditions\n\n\n\n\nExercise 8.10 (Biology / conceptual thinking) How many unique attractors did you find? Compare them with the ones reported in the paper. Are they all fixed points?\n\n\nAnswer This depends on how many initial conditions you test. The more initial conditions, the more likely you will find the maximum of 6 fixed point attractors this model can produce.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Gene regulation in time</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_1.html#cell-differentiation-jumping-from-one-attractor-to-another",
    "href": "differentiation_practical_1.html#cell-differentiation-jumping-from-one-attractor-to-another",
    "title": "8¬† Gene regulation in time",
    "section": "8.4 Cell differentiation ‚Äì jumping from one attractor to another",
    "text": "8.4 Cell differentiation ‚Äì jumping from one attractor to another\nDepending on what we want to answer a continuous or discrete model may be more appropriate for a model. To study the role of many genes in cell differentiation, a Boolean model might be better particularly if we lack details of the parameters underlying each reaction. If we want to study how cells jump from one state to another, a continuous model might be more appropriate.\nHere we will see how we can convert the Boolean model to a continuous one and use it to predict which regulators are able of causing a change in the state of the system (changes in cell fate!)\nCompare the code of the functions rootNetwork() and rootNetworkODE(). Notice how in rootNetworkODE() the logical rules are represented with min and max functions, and then used in a sigmoidal function to create a continuous ODE model. * AND operator is a min function, OR operator is a max function, and NOT operator is 1-x. Use the code below to run a random initial condition for the 18 variables (IC) and see how the system behaves.\ntimerunning=10.1 \ntimes = np.arange(0, timerunning, 0.1)\n\nIC = np.random.randint(0, 2, size=18).tolist() #random initial condition\nparameters = {'decayrate': 1, 'h': 50} \ncells = odeint(rootNetworkODE, IC, times, args=(parameters,)) \n\nplotODEroot(cells,times)\n\nExercise 8.11 (Modeling choices) Do the attractors match those recovered with the Boolean network?\n\n\nAnswer Yes. The ODE model is built with the same regulatory interactions from the Boolean model. The difference is that this model allows us to describe quantitative differences in the activity of the nodes.\n\nNow let‚Äôs use the model to study cell differentiation. Let‚Äôs start in the following initial condition (IC vector), and find a change in a node that produce a jump to another attractor (end). For this you can simply flip the activity of a gene in the initial condition (from 0 -&gt;1 or 1-&gt;0) and see the final attractor matches the desired end state.\n# We start here: \nIC=[0,0,0,0,1,0,1,1,1,1,1,1,1,0,1,0,1,0]\n# We want to end here. \nend=[1,1,0,0,1,1,1,1,1,1,0,0,1,0,0,0,0,1]\n# node order\n# CK, ARR1, SHY2, AUXIAAR, ARFR, ARF10, ARF5, XAL1, PLT, AUX, SCR, SHR, MIR165, PHB, JKD, MGP, WOX5, CLE40\n\n# your code \n\nplotODErootTransition(cells,times) # use this function to plot your result\n\nExercise 8.12 (Algorithmic thinking / biology) What regulator causes the transition between these attractors? How many variables change their activity between the initial and final state? what could be the biological meaning of this switch? how would you test this experimentally?\n\n\nAnswer SHR OFF (0) causes the transition between the specified activity configurations. We can interpret this change between attractors as a cell fate change, in which the decrease in SHR activity triggers changes in other nodes of the network resulting in the differentiation of the cells. SHR could be a cell differentiation signal. To test this experimentally we can artificially repress SHR in the cells.\n\n\nExercise 8.13 (Biology) Finally, use the model to predict the rest of the attractor transitions because of single node changes. Are all attractor transitions possible, or are there preferred differentiation routes?\n\n\nAnswer Not all attractor transitions are possible by single node changes. This is because the regulatory interactions in the model constrain how this perturbation is interpreted, and with it the differentiation trajectories.\n\n\n\n\n\nGarcƒ±ÃÅa-G√≥mez, M√≥nica L, Diego Ornelas-Ayala, Adriana Garay-Arroyo, Berenice Garcƒ±ÃÅa-Ponce, Marƒ±ÃÅa de la Paz S√°nchez, and Elena R Alvarez-Buylla. 2020. ‚ÄúA System-Level Mechanistic Explanation for Asymmetric Stem Cell Fates: Arabidopsis Thaliana Root Niche as a Study System.‚Äù Scientific Reports 10 (1): 3525.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Gene regulation in time</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#section",
    "href": "differentiation_practical_2.html#section",
    "title": "9¬† Gene regulation in space",
    "section": "",
    "text": "How wonderful that we have met with a paradox.  Now we have some hope of making progress ‚Äì Niehls Bohr",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#auxin-does-everything",
    "href": "differentiation_practical_2.html#auxin-does-everything",
    "title": "9¬† Gene regulation in space",
    "section": "9.1 Auxin does everything",
    "text": "9.1 Auxin does everything\nIn this practical you will study the effects of auxin, a plant hormone, on gene regulation in the root. Auxin is a hormone that is virtually involved in all developmental processes in plants, from how the plant embryo is patterned, how the organs in flowers are determined, the spatial disposition of leaves, among many many others. In each context, auxin triggers specific responses, posing the question of how is it that the same molecule can trigger so many different responses. This specificity of auxin responses can be explained by the underlying regulatory networks. The auxin signalling pathway activates the auxin response factors, ARFs, which are transcription factors that regulate gene expression in response to auxin. Arabidopsis has 23 genes encoding for ARFs, some regulate gene expression positively and others negatively, and they are expressed in different tissues. It is this diversity in transcription factors activity which in part explains how auxin can regulate so many different things.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#auxin-does-opposite-things-in-the-root",
    "href": "differentiation_practical_2.html#auxin-does-opposite-things-in-the-root",
    "title": "9¬† Gene regulation in space",
    "section": "9.2 Auxin does opposite things in the root",
    "text": "9.2 Auxin does opposite things in the root\nPlant roots grow thanks to the presence of stem cells housed in a niche at the root tip. A subset of these cells express WOX5, maintaining the surrounding cells undifferentiated. WOX5 is regulated by several factors, and network models help us understand how. Researchers have found paradoxical results regarding how auxin regulates WOX5: some experiments show that auxin promotes its expression, while others found it is repressed (Figure 1). Notably, auxin does this opposite reglation through different ARFs (Figure 2) posing the possibility that accounting for how the ARFs themselves are regulated can reconcile the results. Here we will explore this possibility using an in silico root model.\n\n\n\nFigure 1. The right panel shows evidence of the positive effect of auxin on WOX5 expression (white arrows). The left panel shows evidence of auxin repression of WOX5 in the root tip (white rectangle), also shown with qPCR measurements of WOX5 mRNA in root-tip cells.\n\n\nThe Boolean network we used last week describes the gene and hormonal activity configurations of the cells in the root tip (Figure 3 left). This model recovers an attractor representing the columella, the QC, the endodermis, and three types of vascular cells. attractorswe studied describe the columella, In this model WOX5 is regulated positively by ARF5, and negatively by ARF10 and also by CLE40. Both ARF5 and ARF10 are auxin response factors, activated by auxin. Moreover, other root transcription factors control ARF5 and ARF10 expression causing them to be expressed in specific cells of the root (see Figure 3). This regulation is included in the Boolean model we studied last week. Here you will explore whether this is relevant to understanding the auxin‚ÄìWOX5 paradox. For this you will use an in silico root model in which each cell carries a copy of the Boolean network initialized in a particular attractor, which allows us to test hypotheses and find an explanation.\n\n\n\nFigure 2 Auxin regulates WOX5 expression through different auxin response factors. These are expressed in specific cell types of the root\n\n\n\nBiology / conceptual thinking\nNotice that the QC is the only attractor with WOX5 activity. This makes sense given that its logical rule is:\n\\(WOX5_{t+1} =NOT\\) \\(ARF10\\) \\(AND\\) \\(ARF5\\) \\(AND\\) \\(NOT\\) \\(CLE40\\)\nNotice how the other attractors have either ARF10 and/or CLE40 activity (repressors of WOX5), or lack ARF5 activity (activator of WOX5). The QC attractor is the only one with the following combination: ARF5 active, ARF10 inactive, CLE40 inactive; therefore allowing for WOX5 activity.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#the-model",
    "href": "differentiation_practical_2.html#the-model",
    "title": "9¬† Gene regulation in space",
    "section": "9.3 The model",
    "text": "9.3 The model\nToday you will use a multicellular model that simulates the cells of a root tip (Figure 3, right). The tissue is modelled as a grid, where the x position defines the radial direction and the y the longitudinal position (0 being the cells at the bottom). The cells are organized in cell layers (8 layers, so x is 8), each with a gene regulatory network initialized in a different attractor. The model simulates the transport of auxin from cell to cell, where the cell type (the attractor) defines the direction of polar auxin transport. In figure 3 (right) you can see the different properties that the cells have: a cell type, auxin levels, gene expression (in the figure we see only ARF10, ARF5, MGP and WOX5).\n\n\n\nFigure 3. Ingredients of the in silico root model. 1) gene regulatory network, 2) multi-cellular tissue layout with polar auxin transport in the root tip\n\n\n\nFiles\n1.- Rootfunctions.py ‚Äì File storing the functions used in Root-model-Auxin.py. For this practical you only need to modify rootNetwork. Yet, have a look at all of them and try to understand what they do.\nThe file contains the following functions:\n\nfindNeighbors - retrieves the x and y coordinate of the neighbors of a given cell.\nauxinTransport - executes the passive and the polar auxin transport using the levels of auxin in each cell stored in auxinGrid. The direction at which a cell transports auxin is determined by its cell type.\nrootNetwork- stores the interactions of the root network. This file is similar to the one we used in the last practical. A key difference is that the auxin levels are an input that can modify gene expression (auxininput=parameters['auxininput']). You will modify this function in question 9.5.\ninitialCondition - Initializes the grids of the model. We have many cells in this simulation, and we have a grid per cell property: cell type (cellgrid), auxin levels (auxinGrid), and a grid per node of the network.\nnodeUpdate - function to take the output of the ODE and stored it in the grids.\nplotGrids - plots 6 grids you provide. The labelling assumes you give cellgrid, auxingrid, arf10grid, arf5grid, mgpgrid, wox5grid.\n\n2.- Root-model-Auxin.py ‚Äì code to simulate the in silico root where we use the functions in a specific order. In the main we define the dimensions of the grid, the parameters, initialize the grid, and then use a for loop to run the transport and network update tor a defined number of timesteps. In this loop, the output of the network is plotted every 10 steps. Tip: if the simulation runs too slow, increase this plotting frequency.\n3.- auxin_grid.npy ‚Äì initial auxin levels in each cell with the auxin gradient. This is used to initialize auxinGrid in Root-model-Auxin.py.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#questions",
    "href": "differentiation_practical_2.html#questions",
    "title": "9¬† Gene regulation in space",
    "section": "9.4 Questions",
    "text": "9.4 Questions\n\nExercise 9.1 (Algorithmic thinking) Familiarize yourself with the model. Examine Rootfunctions.py. What are the algorithmic steps you would take if you want to make a model of the root where cells transport auxin every timestep, and update their networks every 10 steps? Keep in mind that the cell type of the cells define the direction at which they transport auxin to their neighbors. Make a plain-language description.\nTip: have a look at the code in the Root-model-Auxin.py.\n\n\nAnswer To simulate the auxin transport first we need to find the neighbors of a given cell (findNeighbors), and then calculate how much auxin should they exchange based on how much auxin they have (auxinGrid) and what are their cell types (cellGrid). The function auxinTransport does this; this function is called every timestep.\nNext, to update the gene regulatory network we iterate over each cell of the simulation (for x, for y, as these are the dimensions of the grid) and store the value each element of the network (e.g.¬†WOX5grid, CKgrid, etc.) into a cellState vector. cellState is used to solve the network (rootNetwork), whose output needs to be filled in again in the grids. We need to perform this every 10 timesteps, so we should make this inside a conditional statement (if).\nUsing a for loop for as many timesteps as we want to simulate we can iterate the above steps.\n\n\nExercise 9.2 (Biology) What happens if you update the network more often than the auxin transport? How would you implement this modification in the code?\n\n\nAnswer It is possible to make this change in the code, and the model will still run (try). Nevertheless, it does not make biological sense because the transport of molecules from cell to cell ocurr in the range of seconds, while gene expression changes can take from minutes to hours. This change can be implemented by putting both auxinTransport and the network update inside conditionals with different values (if step % x == 0:) where x is larger for auxin transport than for gene regulation.\n\n\nExercise 9.3 (Biology & algorithmic thinking) Now let‚Äôs use the model to simulate the auxin treatments from the experiments in Figure 1. To simulate an auxin treatment change the value of AuxinTreatment in Root-model-Auxin.py. Try values of 10 and 750. This increases auxin levels in all cells by the given amount.\nHow does auxin treatment affect gene expression in the root? Does the model output correspond to the transcriptional reporter of WOX5 shown in Figure 1? Do you see activation or repression as reported experimentally?(You can visualize different genes by changing the grid provided to plotGrids().)\n\n\nAnswer The levels of auxin increase in the cells of the root model (as visualized with the plotting function), but the expression of WOX5 remains the same. This means, the model in its current form, cannot explain the observations from the experiments. Yet, we can use this model to test hypothesis and aim to recover such behaviours (Figure 1).\n\n\n\n\nFigure 3. Ingredients of the in silico root model. 1) gene regulatory network, 2) multi-cellular tissue layout with polar auxin transport in the root tip\n\n\n\nExercise 9.4 (Biology & algorithmic/mathematical thinking) The model does not yet reproduce the experiments. Yet, we can use this model to test hypothesis and aim to recover such behaviours (Figure 1). To account for quantitative effects of auxin on gene regulation, let‚Äô implement the following two hypotheses based on the literature:\n\n\n\nFigure 4. Auxin represses MGP expression. Extremely high auxin levels (unrealistic in vivo) repress WOX5 expression.\n\n\nModify the equations for MGP and WOX5 in rootNetwork accordingly.\n\nFor MGP, retain its logical rule but multiply its production term by a negative regulatory term of auxin (Km = 45):  \\(\\frac{45}{45+auxininput}\\) \nSimilarly, add a negative regulation by auxin to the WOX5 equation (Km = 1000).\n\nRun control and auxin treatments with (i) updated MGP, (ii) updated WOX5, and (iii) both updates.\nDo you now see the differences reported by experimentalists?\n\n\nAnswer Yes, the model with both hypothesis can reproduce the experimental observations. It reveals that the effects of auxin on WOX5 expression depend on the cell type and on the time of the treatment. If we focus on the QC we can see that the expression of WOX5 is reduced gradually until it is not expressed at all. In contrast, in the endodermis gradually is gradually induced from the base of the tip to the rest of the cells. In the long run, this expression also disappears. This matches the experiments in Figure 1. Indeed in the left panel we can see WOX5 expressed in the endodermis, and already reduced in the QC. The hypothesis regarding MGP regulation is needed to allow for WOX5 expression in the endodermis. The second hypothesis is to shut down expression in the QC cell, and gradually in the rest of the cells. \n\n\nExercise 9.5 (Biological interpretation) You should observe that WOX5 expression gradually expands into the endodermis and disappears from QC cells when both equations are updated. This matches experimental microscopy.\nWhy do different parts of the root respond differently to auxin? Why does induction occur mainly in upper cells and repression in basal cells?\n\n\nAnswer Because of the differences in gene expression (whether they are expressing ARF10 and ARF5 or not), and because auxin is distributed in a gradient so the cells at the tip will reach the repressive levels before the distal cells.\n\n\nExercise 9.6 (Algorithmic thinking) Experimental repression of WOX5 was detected by RT-PCR at the very tip. Reproduce this in silico by saving the average WOX5 levels of cells at the tip (y &lt; 40) at the end of the simulation for all three treatments. For comparison, also calculate the average for the whole root. Then plot this average WOX5 expression levels for auxinTreatment= 0, 10 and 750.\nTip: WOX5Grid stores WOX5 levels, and y controls the position of the cells.\nWhat is the root tip response to auxin treatments? Considering that the root contains mixed cell types, which cells best represent your plotted values?\n\n\nAnswer What portion of the root we sample changes completely our perception of how auxin regulates WOX5. If we sample the whole root we get the impression that WOX5 is induced and it is true for the endodermis cells. Yet, if we sample the very tip of the root, we are sampling a smaller proportion of endodermal cells, allowing us to detect that auxin is also repressing WOX5 expression. The latter is similar to the qPCR experiment in Figure 1 (right panel). \n\n\nExercise 9.7 (Biology & algorithmic thinking) Now, let‚Äôs compare WOX5 regulation across tissues. Repeat the qPCR in silico analysis but include the cells in the QC position, and endodermis cells. Tip: cell types ar stored in cellGrid.\nDo you see a difference between your observations and the simulated RT-PCR plot? Why?\n\n\nAnswer The repression of WOX5 expression is much more clear if we sample individual QC cells, while sampling only endodermal cells only shows activation. In a way both results are correct, and reveal how dependent on spatial context the effects of auxin are. Moreover it shows how difficult it is to draw conclusions from this sort of experiments, as it is actually very diffiult to only sample individual cell types. Sampling the whole tissue is the best option, but we have to be aware that tissues are composed of many cell types, and each may respond differently!  :::{#exr-tur}",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#biology-algorithmic-thinking-2",
    "href": "differentiation_practical_2.html#biology-algorithmic-thinking-2",
    "title": "9¬† Gene regulation in space",
    "section": "9.5 Biology & algorithmic thinking",
    "text": "9.5 Biology & algorithmic thinking\nChanges in WOX5 expression depend on tissue context and auxin dosage. Simulate a range of auxin treatments from 0 to 750 (choose your step size).\nCan you now explain the auxin‚ÄìWOX5 paradox? What mechanism do you propose?\n\nAnswer It is clearly dependent on levels of auxin and cell type specificity.\n\n\nExercise 9.8 (Biology & conceptual thinking) Perform an in silico intervention to alter WOX5 dosage responsiveness. For example, enforce continuous induction or block its induction in the endodermis. How would you test these model predictions experimentally?\n\n\nAnswer This is an open question. Feel free to test your ideas with the model and discuss with us your results :)‚àÇ\n\n\nExercise 9.9 (Biological thinking) All models are wrong, but some are useful. The model we studied today offered mechanistic insight into the paradoxical auxin responses. Yet, to make things more fun, the original experiments used different auxin analogs: NAA (that is transported independently of PINs), IAA (that is PIN-transported), and NPA (that blocks PINs, increasing auxin in the tip). How does this information change your conclusions? What extensions to the model would account for these differences?\n\n\nAnswer Open question. This will be a mini project if you are interested!",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "differentiation_practical_2.html#references",
    "href": "differentiation_practical_2.html#references",
    "title": "9¬† Gene regulation in space",
    "section": "9.6 References",
    "text": "9.6 References\n\nM√§h√∂nen et al. (2014)\nDing and Friml (2010)\n\n\n\n\n\nDing, Zhaojun, and Ji≈ôƒ±ÃÅ Friml. 2010. ‚ÄúAuxin Regulates Distal Stem Cell Differentiation in Arabidopsis Roots.‚Äù Proceedings of the National Academy of Sciences 107 (26): 12046‚Äì51.\n\n\nM√§h√∂nen, Ari Pekka, Kirsten ten Tusscher, Riccardo Siligato, Ond≈ôej Smetana, Sara Dƒ±ÃÅaz-Trivi√±o, Jarkko Saloj√§rvi, Guy Wachsman, Kalika Prasad, Renze Heidstra, and Ben Scheres. 2014. ‚ÄúPLETHORA Gradient Formation Mechanism Separates Auxin Responses.‚Äù Nature 515 (7525): 125‚Äì29.",
    "crumbs": [
      "III) Cell differentiation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Gene regulation in space</span>"
    ]
  },
  {
    "objectID": "evdev_intro_text.html",
    "href": "evdev_intro_text.html",
    "title": "10¬† Environmental inputs",
    "section": "",
    "text": "So far we have largely discussed developmental patterning processes as if they are either occurring under constant environmental conditions or as if environmental conditions have no effect. A minor exception to this is the discussion on robustness of patterning under morphogen gradients, where differences in input of maternal resources, which in turn may depend on food conditions, may impact embryo size and hence scaling is required.\nIn contrast to our discussion so far, environmental conditions may have a major impact on developmental processes. This ranges from the effect of temperature on the body sizes of fruitflies and butterflies, and on the gender of hatching sea turtles, nutritional effects on ant size and worker function to the complete reshaping of organs, organ positioning and body plans in plants. Indeed, while for animals symmetry and scaling are essential for mobility, for plants adaptation to the conditions they find themselves in is of key importance, making developmental plasticity more pervasive in plant development. In the practical we are going to investigate temperature induced leaf hyponasty, in which plants develop leaves with longer petioles (stems) and smaller blades (leaf surface itself) that are positioned in a more upright angle, investigating the adaptive value of this developmental plasticity.\nApart from the question whether plasticity of a developmental process is adaptive, a major question is how this plasticity can be united with robustness. That is, how certain aspects of development can be adjusted, while other aspects can be maintained (i.e.¬†the plant still makes a leaf, that has still a top, bottom, stomata veins, etc but other things are allowed to vary). Additionally, a question is how the networks that drive developmental patterning and e.g.¬†cell fate decisions (Part III) are integrated with networks that sense, process and combine various environmental signals to decide how these developmental processes are to be adjusted. This will be discussed in the lecture.\nIn addition to developmental processes, also physiology, behavior and evolution of course depend on environmental conditions. Examples of the latter two will be discussed in Part V.\n\n\n\nEnvironmental cues affect how organisms develop. Credit: WikimediaCommons.",
    "crumbs": [
      "IV) Environment",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Environmental inputs</span>"
    ]
  },
  {
    "objectID": "evdev_practical.html",
    "href": "evdev_practical.html",
    "title": "11¬† Modeling Thermomorphogenesis in Plants",
    "section": "",
    "text": "11.1 The model\nPlants respond to temperature not only through metabolic changes but also by adjusting their morphology. A well known response of plants to increased ambiant temperature is the lifiting of their leaves (hyponasty) and increasing their petiole (stalk between leaf and stem) length to lower the internal leaf temperature. It is thought that this may help plants cope with heat, but it‚Äôs not always clear whether and if so how this actually improves growth. In this practical, we will use and extend a small ODE model of carbon (C) and leaf area (LA) dynamics and explore how temperature and thermomorphogenic responses affect growth. You‚Äôll simulate different temperature conditions and analyze how traits like leaf angle and size affect photosynthesis and carbon respiration rate, and how this affects overall plant growth. Later, you will investigate the adaptive value of this response, as this is still an open question.\nIn code 00_Environment_and_development.py we start with a simple two ODE model where leaf area (\\(L\\)) grows with rate \\(G\\), which depends on carbon concentration (\\(C_c\\)) and leaf area. Carbon (\\(C\\)) is produced by photosynthesis (\\(P\\)) dependent on leaf area, leaf angle (\\(\\alpha\\)), and temperature (\\(T\\)). Carbon is consumed by growth and respiration (\\(R\\)), dependent on temperature and leaf area again:\n\\(\\frac{dC}{dt}=P(L,T,\\alpha)-R(L,T)-G(C_c,L)\\)\n\\(\\frac{dL}{dt}=G(C_c,L)\\)",
    "crumbs": [
      "IV) Environment",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Modeling Thermomorphogenesis in Plants</span>"
    ]
  },
  {
    "objectID": "evdev_practical.html#the-model",
    "href": "evdev_practical.html#the-model",
    "title": "11¬† Modeling Thermomorphogenesis in Plants",
    "section": "",
    "text": "Photosynthesis\nPhotosynthesis is a complex process that can be modeled in many different ways with varying complexity. Here, in the function Photosynthesis_per_m2() we included a semi-detailed photosynthesis model based on the assumption that the protein RubisCo is the limiting step in photosynthesis. This model has 6 parameters and 2 environmental variables, of which all parameters are temperature sensitive. This temperature sensitivity is based on the Arrhenius equation, which describes the temperature dependency of chemical reaction rates.\n\n\nMaintenance Respiration\nMaintenance respiration is a term used in biological systems to describe all energy/carbon consuming processes that a plant must do to survive. Similar to photosynthesis, the rates of these reactions increase with temperature, and as such, carbon costs greatly increase. We model this in the function Respiration_per_m2() using the Q10 equation, that describes how much a rate increases for 10 degrees of temperature increase.",
    "crumbs": [
      "IV) Environment",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Modeling Thermomorphogenesis in Plants</span>"
    ]
  },
  {
    "objectID": "evdev_practical.html#exercises",
    "href": "evdev_practical.html#exercises",
    "title": "11¬† Modeling Thermomorphogenesis in Plants",
    "section": "11.2 Exercises",
    "text": "11.2 Exercises\n\nExercise 11.1 (Algorithmic thinking and Biology - Temperature Effects on Photosynthesis, Respiration and Growth) Let us first investigate how photosynthesis and respiration depend on temperature.\n\nPlot photosynthesis and respiration rate as a function of temperature, using the functions defined above. Explain: What happens to net carbon gain at high temperature? Why is high temperature problematic for growth in this model?\nRun the code and study the output for leaf area and carbon. Why does leaf area increase exponentially while carbon levels saturate? What carbon level are you actually plotting?\nNow run simulations of the ODE model for low (15¬∞C), medium (25¬∞C), and high (35¬∞C)\ntemperatures. Which plant performs best? Which plant worst? Why is this?\n\n\n\nAnswer\n\nSee answer code ‚Äò01_Environment_and_development_answer.py‚Äô You need to do this plotting outside of the plant growth simulation loop by calling the photosynthesis and respiration functions for a range of temperatures (10-40 degrees).\n\\(dLA/dt=Gmax*(C*/(C*+K))*LA\\)\n\nThis gives \\(LA(t)=LA(0)*exp(Gmax*(C/(C+K))*t)\\)\nIn contrast:\n\\(dC/dt=photosynthesis-respiration-growth\\)\n\\(dC/dt=p*LA-d*LA- Gmax*(C*/(C*+K))*LA\\)\nSolving \\(dC/dt=0\\) we get \\(LA=0\\) (no plant) or\n\\(p-d-Gmax*(C*/(C*+K))=0\\)\n\\(p-d=Gmax*(C*/(C*+K))\\)\n\\((p-d)*C*+(p-d)*K=Gmax*C*\\)\n\\((p-d)*K=(Gmax-p+d)*C*\\)\n\\(C*=(p-d)*K/(Gmax-p+d)\\)\n\\(C*=C/LA\\)\nSo carbon concentration equilibrates, not total carbon\n\nSee answer code 01_Environment_and_development_answer.py for running and plotting the 3 different temperatures. At 15 degrees respiration is low but photosynthesis is far below optimal, so at 25 degrees you loose more to respiration but that is overcompensated by the increase in photosynthesis, at 35 degrees photosynthesis is beyond its optimum while respiration is now very high; as a consequence 25 degrees gives optimum growth and 35 the worst.\n\n\n\nExercise 11.2 (Mathematical & Biological thinking - Hyponasty ‚Äì Leaf angle increase) Until now we have investigated the effect of temperature on plants that do not respond to their environment, but plants do respond to their surroundings.Plants display a variety of responses to elevated temperature, and one of these is so-called leaf hyponasty in which leaves are positioned in a more upward orientation that is often accompanied by longer leaf stems (petioles) and smaller leaves (blades). Let us extend the model to first simply include the orientation aspect of this hyponastic response to high temperature.\n\nThe photosynthesis function already includes a term for ‚Äòeffective leaf area (LA_eff)‚Äô, but the function does currently return the total leaf area. Rewrite this function to return effective leaf area as a function of leaf elevation angle, assuming that light comes from above. Plot the effective leaf area as a function of angle. How does hyponasty affect photosynthesis? What other factors could affect effective leaf area?\nNow investigate the effect of the increased leaf angle on plant growth. Compare medium (25¬∞C) and high temperature (35¬∞C) for normal (20¬∞) and increased (40¬∞) angle.\n\n\n\nAnswer\n\nSee answer code ‚Äò02_Environment_and_development_answer.py‚Äô Under the assumption that light comes from above it holds that effective leaf area=the area projected on the horizontal surface which can be found by taking the cosine of the angle of the leaf. So the larger the angle the smaller the effective leaf area. This results in an effective reduction of leaf area and thus photosynthesis. Shading, chloroplast density, leaf size could all affect effective leaf area.\nAs can be expected from the above, the temperature increase combined with the angle increase now both reduce photosynthesis and hence carbon and leaf area increase, so it is basically adding insult to injury rather than helping.\n\n\n\nExercise 11.3 (Biology - Hyponasty‚Äì Cooling Benefit) Leaf hyponasty is shown to lower leaf temperature with a few degrees by improving heat dissipation and decreasing the area in which direct sunlight hits the leaves.\n\nInvestigate the effect of the leaf cooling. How much cooling is needed for hyponasty to have a net positive effect? Simply assume a certain cooling effect and hence apply a lower temperature than the environmental one.\nHyponasty is shown to lower leaf temperature by only 1-2¬∞C (see the left picture at the beginning of this document), what is the effect of this amount of cooling on plant growth? Would you argue that this hyponastic response is adaptive in the current model (i.e.¬†assuming no other factors play a role)?\nReanalyze the curves describing how photosynthesis rate depends on temperature and how effective leaf area depends on angle and explain your earlier results.\n\n\n\nAnswer\n\nSee answer code ‚Äò03_Environment_and_development_answer.py‚Äô If the 40 degree angle results in slightly more than 3 degrees of cooling, causing internal leaf temperature to be less then 32 degrees instead of the external 35 degrees, final leaf area is larger than for 35 degrees and an angle of 20 degrees. Note that if under high temperature leaf anlge would increase to only 35 degrees, a slightly less than 3 degrees of cooling would already have a positive effect and this positive effect would start already at 32.5 degrees celcius.\nSee answer code 03_Environment_and_development_answer.py No for only 1-2 degrees of cooling final leaf area is smaller under a hyponastic response than if leaf angle were kept at 20 degrees under 35 degrees celcius temperature.\nPhotosynthesis is normalized to 100% at 25 degrees and is 90% at 35 degrees. Lowering leaf temperature with 1-2 degrees by hyponasty recovers this to 95 % while lowering it to 2-3 degrees recovers it to 100%. Still since there is only a 10% loss of photosynthesis to high temperature there is also only a max 10% gain by cooling the leaves. At the same time a hyponastic angle of 40% decreases effective leaf area to 80 percent whereas it is 95% for 20 degrees. So the loss is 15%, which is more than the potential gain.\n\n\n\nExercise 11.4 (Biology -Is hyponasty adaptive?) The cooling down of the leaves through hyponasty is thought to be an important adaptive trait. Interestingly, from the simulations we‚Äôve done so far this adaptive advantage is not very clear. It might therefore very well be that we miss important processes in the current model. In this last question of the practical you will extend the model with additional processes also relevant in plants to see if this may help explain the adaptiveness of hyponasty. This is also an open question in current research, so there is no clear answer and you might actually come up with novel ideas! To help guide your thinking, we have come up with some additional plant processes you can implement and investigate to see how these impact adaptiveness of leaf hyponasty under high temperature. Pick one to work on and investigate if this would render hyponasty more clearly adaptive. If you have extra time, see if you can combinethem. If you have other ideas to work on, this is also great!\n\nHypothesis 1 - Day-Night Rhythm \n\nWe found that during the day, while hyponastic cooling brings photosynthesis closer to its optimum temperature and reduces respiration, this is insufficient to overcompensate the costs of reduced effective leaf area. During the night, cooler leaf temperatures still reduce maintenance respiration, while photosynthesis halts and hence reduction of effective leaf area plays no role, suggesting at night time hyponasty has only advantages. To investigate whether this results in a net adaptive effect of leaf hyponasty incorporate the following processes in your model: - Temperature: Simulate the cooling effect of nighttime. - Photosynthesis: Photosynthesis does not occur without sunlight, so simulate the change of light over the day and its effects. - Respiration: Maintenance respiration persists in absence of photosynthesis. - Hyponasty changes: Leaf angles change over the course of a day, reaching a minimum early during the day and being relatively high during the night. - Growth rate: Counterintuitively in plants often most growth happens during the night, when water loss is minimal and hence turgor pressure is high.\nHint: use sinus functions to describe the daily rhythms and their relative phases. Make sure that light is really zero at night.\na1. Analyze how the absence of photosynthesis at night affects carbon balance. Do you think what you see happening is reasonable? How could you repair this?\na2. Examine the impact of nighttime temperatures on maintenance respiration rates.\na3. Explore how changes in leaf angle influence overall plant growth.\na4. Now combine the two\n\nHypothesis 2 ‚Äì Stomata\n\nStomata play a critical role in regulating gas exchange and water loss in plants. As leaf temperature increases, stomata open to enhance transpiration, which cools the leaf through evaporative cooling. This mechanism is particularly effective in well-watered plants, where increased stomatal opening can significantly reduce leaf temperature up to around a maximum of 9-10¬∞C. However, at very high temperatures, stomata may close to prevent excessive water loss, which can lead to overheating. Additionally, stomatal opening increases while stomatal closing decreases photosynthesis rates by affecting gas exchange efficiency. Thus, incorporating this dependence of stomatal aperture on temperature may enhance the detrimental effects of high temperature, offering more opportunity for leaf cooling effects of hyponasty to matter.\nTo model this, implement the following changes: - Stomatal opening affects photosynthesis: Include in the photosynthesis function a multiplication factor for stomatal aperture. - Stomatal opening depends on temperatures: Include a function that describes how stomatal aperture first increases with temperature, reaches a maximum at around 25 degrees and then declines for higher temperatures. Aperture should be approximately half the maximum value for temperatures of 10 degrees and 35 degrees. - Stomatal opening increases transpiration: Simulate the cooling effect of transpiration on leaf temperature.\nAnalyze how stomatal opening affects leaf cooling and photosynthesis under moderate temperature conditions. Investigate the trade-offs between cooling benefits and photosynthesis efficiency at very high temperatures. Examine how stomatal closure impacts plant growth and carbon balance under heat stress.\n\n\nAnswer\na1. Carbon levels at night go below zero because while growth depends on carbon levels, respiration continues independent of carbon levels in the current model. However, you can not burn energy you do not have so it is best to make respiration also depend on carbon levels, this is constistent with experimental observations: Seki et al. (2017)\na2. The less temperatures drop during the night, the larger the reduction in final leaf area, yet also the closer the final leaf area achieved with hyponasty becomes to that without. As night time temperature is higher, respiration is higher and leaf cooling becomes more relevant. So we increase the night time advantage of hyponasty.\na3. For leaf angle we can play with how much the minimum and maximum angles are and when during the day these are reached. If leaf angle is allowed to drop from 40 to 14 degrees instead of 20 degrees and the lowest point is reached early in the morning, average day time angle is lower and hence photosynthesis loss is lower and hence break even is reached easier. If maximum leaf angle is lowered to 35 degrees it also helps. So here we basically decrease the day time costs of hyponasty.\na4. If we make leaf angle during the day low and keep night time temperature high we are able to make hyponasty adaptive. The results are a bit shaky in that in the final leaf area versus temperature curve the non hyponasty and hyponasty curves intersect multiple times but beyond 32 degrees with hyponasty is always on top but with small difference.\n\nSee answer code 04_Environment_and_development_Stomata_1.py This is a first code with a Gaussian function for how stomatal aperture depends on temperature that results in stomatal opening being maximal at around 25 degrees. Now slightly less than 2 degrees cooling (on top of the stomatal aperture induced cooling) suffices to make leaf hyponasty advantageous. This is because temperature increase harms photosynthesis now way more (leaf area under no hyponasty decreases to 32 instead of 43% while with hyponasty it decreases to 20 instead of 25%, 43-25=18% 32-20=12%). But this approach is a bit odd: we let stomatal aperture affect effective leaf temperature, and separately (and before this) substract the cooling effect of hyponasty.\n\nSee answer code 04_Environment_and_development_Stomata_2.py Instead of doing a manual extra cooling because of leaf hyponastic angle here we made a function for leaf temperature that takes into account both the stomatal aperture (max cooling effect of 10 degrees for full aperture at 35 degrees) and the leaf angle (max cooling effect of 2 degrees for a 40 degree angle). Now final leaf area decreases to 34% for no hyponasty and to 25% for hyponasty so 34-25=9%. (and looking at the lower temperatures done by further manual substraction is non-sensical now). So, although difference becomes less and less, still hyponasty costs more than it delivers. Also intriguing that if we put both effects into leaf temperature directly we can not get it to work while if we put in the 2 degrees cooling from the angle manually and then the stomatal affect it could work. This is because with the 2 degrees manually, stomatal opening for 33 degrees applies which is 70% whereas without this, stomatal opening for 35 degrees applies which is 60%, so the actual cooling achieved was higher.\nSee 04_Environment_and_development_Stomata_3.py However in reality stomatal aperture does not depend on external temperature but on leaf temperature, so let‚Äôs make leaf temperature a variable that affects stomatal aperture and is affected by it. However, we also reduced the stomatal effects on temperature a bit to avoid weird effects: if internal leaf temperature drops too much instead of external temperature 25 being optimal and 35 far below it, we get internal temperatures 17.5 far from optimum and 25 close to optimum, so now under 35 degrees the plants would be doing better. If we make the stomata effect on leaf cooling less strong (factor 0.175 instead of 0.29) internal temperatures are 29 and 20, things are again worse for external temperature of 35, and now for hyponasty situation is only slightly worse and if external temperature is 37.5 with hyponasty plants even perform better. We can further enhance this by boosting angle effect on temperature a bit (factor 0.4 instead of 0.3). If we now additionally assume that stomatal aperture as a function of temperature is usually plotted for external not internal temperature and therefore assume that an external temperature optimum of 24 is approximately an internal leaf temperature of 22 we get a break even for hyponasty at slightly below 35 degrees and plants with hyponasty do actually a bit better at 35 degrees.\nAs optima are around 25 and stomata always provide some cooling actual growth optimum for angle of 20 degrees is around 28.5 degrees and things only really start to go down at 35 degrees, so there is where the big costs are and where hyponasty earns you more by cooling than it costs in terms of effective leaf area. This explains why if you increase further effect of angle at lowering temperature (eg factor 0.8 instead of 0.4), plants do much better at 36/37 degrees than without hyponasty (i.e.¬†at 36/37 degrees difference gets way bigger), but temperature at which plants with hyponasty start to do better hardly shifts. If optimum temperature of stomatal aperture is lowered than temperature at which plant with hyponasty do better becomes lower.\nAlso note it is a bit of a flywheel effect, angle brings more cooling, more cooling brings more stomatal aperture, which brings still more cooling.",
    "crumbs": [
      "IV) Environment",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Modeling Thermomorphogenesis in Plants</span>"
    ]
  },
  {
    "objectID": "evdev_practical.html#references",
    "href": "evdev_practical.html#references",
    "title": "11¬† Modeling Thermomorphogenesis in Plants",
    "section": "11.3 References",
    "text": "11.3 References\nLook at this: hyponasty angle quite high during night, dips early in the day Praat et al. (2024)\nNormal and shade avoidance hyponasty is still large in the dark, and dips around dawn for long day conditions: Michaud et al. (2017)\nFor short day conditions it even peaks during the dark and is lower during day: Oskam et al. (2023) and Dornbusch et al. (2012)\n\n\n\n\nDornbusch, Tino, S√©verine Lorrain, Dmitry Kuznetsov, Arnaud Fortier, Robin Liechti, Ioannis Xenarios, and Christian Fankhauser. 2012. ‚ÄúMeasuring the Diurnal Pattern of Leaf Hyponasty and Growth in Arabidopsis‚Äìa Novel Phenotyping Approach Using Laser Scanning.‚Äù Functional Plant Biology 39 (11): 860‚Äì69.\n\n\nMichaud, Olivier, Anne-Sophie Fiorucci, Ioannis Xenarios, and Christian Fankhauser. 2017. ‚ÄúLocal Auxin Production Underlies a Spatially Restricted Neighbor-Detection Response in Arabidopsis.‚Äù Proceedings of the National Academy of Sciences 114 (28): 7444‚Äì49.\n\n\nOskam, Lisa, Basten L Snoek, Chrysoula K Pantazopoulou, Hans van Veen, Sanne EA Matton, Rens Dijkhuizen, and Ronald Pierik. 2023. ‚ÄúA Low-Cost and Open-Source Imaging Platform Reveals Spatiotemporal Insight into Arabidopsis Leaf Elongation and Movement.‚Äù BioRxiv, 2023‚Äì08.\n\n\nPraat, Myrthe, Zhang Jiang, Joe Earle, Sjef Smeekens, and Martijn van Zanten. 2024. ‚ÄúUsing a Thermal Gradient Table to Study Plant Temperature Signalling and Response Across a Temperature Spectrum.‚Äù Plant Methods 20 (1): 114.\n\n\nSeki, Motohide, Takayuki Ohara, Timothy J Hearn, Alexander Frank, Viviane CH Da Silva, Camila Caldana, Alex AR Webb, and Akiko Satake. 2017. ‚ÄúAdjustment of the Arabidopsis Circadian Oscillator by Sugar Signalling Dictates the Regulation of Starch Metabolism.‚Äù Scientific Reports 7 (1): 8305.",
    "crumbs": [
      "IV) Environment",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Modeling Thermomorphogenesis in Plants</span>"
    ]
  },
  {
    "objectID": "evo_intro_text.html",
    "href": "evo_intro_text.html",
    "title": "12¬† Introduction to evolution",
    "section": "",
    "text": "12.1 Evolution: Life‚Äôs most clever algorithm\nEvolution is the process by which populations change over generations through variation, inheritance, and differential survival. This idea, famously championed by Darwin and Wallace, explains the diversity of life on Earth. It describes how species adapt to their environments, how new species arise, and how complex traits evolve. Today, the concept of evolution has expanded beyond biology, it‚Äôs recognised as a powerful algorithm that drives adaptation in systems ranging from bacteria (genes) to ideas (memes), from DNA (nucleotides) to computer code (bits).\nIn this part of the course, we‚Äôll bring these ingredients to life by writing our own simulations and watching evolution unfold on the screen. And while our digital creatures aren‚Äôt made of flesh and blood, the evolutionary battles they fight, the strategies they discover, and the adaptations they evolve are as real, and often as surprising, as anything found in nature itself.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Introduction to evolution</span>"
    ]
  },
  {
    "objectID": "evo_intro_text.html#three-ingredients",
    "href": "evo_intro_text.html#three-ingredients",
    "title": "12¬† Introduction to evolution",
    "section": "12.2 Three ingredients",
    "text": "12.2 Three ingredients\nAs briefly mentioned above, we just need three ingredients to have evolution by means of natural selection:\n\nvariation (differences between individuals),\ninheritance (the passing on of traits),\nselection (some variants performing better than others).\n\nThe last ingredient is self-evident. Evolution by means of natural selection requires selection. It is especially the first two that are a little more tricky to really understand, as they are not always as obvious as they seem.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Introduction to evolution</span>"
    ]
  },
  {
    "objectID": "evo_intro_text.html#balancing-change-and-stability",
    "href": "evo_intro_text.html#balancing-change-and-stability",
    "title": "12¬† Introduction to evolution",
    "section": "12.3 Balancing change and stability",
    "text": "12.3 Balancing change and stability\nTo evolve, a system needs enough variation ‚Äì if everyone is the same, there‚Äôs nothing for selection to act on. But this variation can‚Äôt just be noise; it needs to be passed on. That means inheritance can‚Äôt be perfect ‚Äì there must be room for change, such as through mutations ‚Äì but it also can‚Äôt be too sloppy. If traits aren‚Äôt reliably transmitted to the next generation, then even the best adaptations will vanish before they can take hold. Evolution lives in the sweet spot: not too rigid, not too chaotic, just enough memory and just enough change. To make this a little more tangible, let us make our very first simulation.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Introduction to evolution</span>"
    ]
  },
  {
    "objectID": "evo_intro_text.html#a-simple-evolutionary-algorithm",
    "href": "evo_intro_text.html#a-simple-evolutionary-algorithm",
    "title": "12¬† Introduction to evolution",
    "section": "12.4 A simple evolutionary algorithm",
    "text": "12.4 A simple evolutionary algorithm\nOne simple way to simulate evolution is with a Moran process, a classic model from population genetics. Imagine a population of 100 individuals, each with a single gene that determines its fitness. This gene can have all values from 0 to 1 (let‚Äôs call this value \\(\\phi\\)). At each time step, one individual is chosen to reproduce with a probability proportional to \\(\\phi\\), producing 1 offspring. This offspring inherits their parents gene (so the same \\(\\phi\\)), but with a probability \\(\\mu\\), the value changes by a small amount (a mutation). The population size will now be 101, which could be interesting if we want to study population growth. However, in a Moran process we keep it simple: one random individual is removed by the new offspring, so the population size is constant while still allowing fitter individuals to spread over time.\nHere‚Äôs a minimal Python example:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(5)\n\nN = 100 # Population size \nfitnesses = np.full(N, 0.05)\nmu = 0.01\n# Updated parameters\nsteps = 50000\navg_fitness = []\n\n# Moran process with mutation (logging every 10 steps)\nfor step in range(steps):\n    probs = fitnesses / fitnesses.sum()\n    parent = np.random.choice(N, p=probs)\n    dead = np.random.choice(N)\n\n    # Copy with mutation\n    new_fit = fitnesses[parent]\n    if np.random.rand() &lt; mu:\n        new_fit = np.clip(new_fit + np.random.normal(0, 0.1), 0, 1)\n            \n    fitnesses[dead] = new_fit\n\n    # Save average fitness every 10 steps\n    if step % 10 == 0:\n        avg_fitness.append(fitnesses.mean())\n\n# Plotting\nplt.plot(np.arange(0, steps, 10), avg_fitness)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Average fitness\")\nplt.title(\"Evolution of Fitness in a Moran Process\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nExercise 12.1 (Moran process simulation) \nStudy the Python code for the evolutionary algorithm given above. Answer the following questions:\n\nHow ‚Äúwell adapted‚Äù is the initial population?\nHow are mutations implemented in the code? Can you think of other ways?\nCan the parent be replaced by its own offspring? Why/why not?\nTry to decreasing/increase value of \\(\\mu\\) (mutation rate). Which values makes evolution go faster? Which values make evolution more precise?\n\n\n\nAnswer  a. Looking at the output or at the code, we see that the initial population has a ‚Äúfitness‚Äù of 0.05.  b. Mutations are implemented by adding a normally distributed random number (mean 0, std 0.1) to the parent‚Äôs fitness value with a probability of mu. This comes down to sampling from a normal distribution with the mean set to the parent‚Äôs value. Other ways could include using a uniform distribution for mutations, making every jump equally likely.  c.¬†Yes, the parent can be replaced by its own offspring if the random individual chosen to die happens to be the parent. This is possible because the death selection is random and independent of fitness.  d.¬†Increasing mutation rates makes it easier to find fitter mutants when the population is not (yet) adapted, so evolution goes faster. However, it is also less ‚Äúprecise‚Äù, as the average of the population will fluctuate well below the optimal fitness of 1.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Introduction to evolution</span>"
    ]
  },
  {
    "objectID": "evo_intro_text.html#what-this-part-of-the-course-is-about",
    "href": "evo_intro_text.html#what-this-part-of-the-course-is-about",
    "title": "12¬† Introduction to evolution",
    "section": "12.5 What this part of the course is about",
    "text": "12.5 What this part of the course is about\nThe above simulation is fun, but not really‚Ä¶ biologically relevant. While some simplifications are necessary to make models feasible, we will investigate a few evolutionary models that are somewhat more interesting. We will discuss how to model spatial structure and local competition, how genotypes (where mutations happen) get translated into phenotypes (where selection happens), and how the environment can change over time and lead to niche construction and interactions.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Introduction to evolution</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html",
    "href": "evo_practical_1.html",
    "title": "13¬† Sticking together",
    "section": "",
    "text": "Sticking together\nIn this practical, you will practice building your own model of collective behaviour, based on the one you saw at the end of the lecture:\nThe example above is a implemented in Javascript, a programming language that is widely used for web development. It is easy to share with others, interactive, and surprisingly fast. But, it‚Äôs not the most ‚Äúprofessional‚Äù programming language. Plus, at this stage of the course there is no point in learning yet another programming language, as you are here to learn about modelling biology. So we will stick to Python.\nFirst, let‚Äôs discuss how we can let individuals walk around in space.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-steering",
    "href": "evo_practical_1.html#sec-steering",
    "title": "13¬† Sticking together",
    "section": "13.1 Steering",
    "text": "13.1 Steering\nWe can represent a moving individual in space as a point with a position and a velocity. The position is represented by two coordinates, \\(x\\) and \\(y\\), and the velocity is represented by two components, \\(v_x\\) and \\(v_y\\). All movement that this individual can do, will be a matter of repeatedly updating its position based on their velocity:\n\n\n\n  \n  \n  Vector Visualisation\n  \n\n\n  \n\n  \n    ‚Üê \n    ‚Üí \n    ‚Üë \n    ‚Üì \n    ‚ü≤ \n    ‚ü≥ \n  \n\n  \n\n\n\n\n To model such a vector in python, we can simply define a base point with an x- and y-coordinate, and a velocity vector with an x- and y-component. The position of the individual can then be updated by adding the velocity to the position. Combining that with a function that draws an arrow in Python, we get the following code:\n\n\n\n\n\n\nNoteCODE FOR ‚Äúmoving vector in Python‚Äù\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Enable interactive mode for matplotlib\nplt.ion()\n\n# Setup figure and axis for plotting the arrow\nfig, ax = plt.subplots(figsize=(8, 4))\nax.set_xlim(0, 600)  # x-axis limits\nax.set_ylim(0, 250)  # y-axis limits\nax.set_aspect('equal')  # Keep aspect ratio square\nax.set_facecolor('#f0f0f0')  # Background color\nax.set_title(\"A moving vector with an arrowhead\")  # Title\n\n# Initial position and velocity\nx, y = 250.0, 180.0      # Position coordinates\nvx, vy = 5.0, 10.5        # Velocity components\n\n\ndef draw_arrow(x, y, vx, vy):\n    \"\"\"\n    Draws an arrow at position (x, y) with velocity (vx, vy).\n    \"\"\"\n    ax.clear()\n    ax.set_xlim(0, 600)\n    ax.set_ylim(0, 250)\n    ax.set_aspect('equal')\n    ax.set_facecolor('#f0f0f0')\n    ax.set_title(\"A moving vector with an arrowhead\")\n\n    # Normalize velocity for drawing the arrow\n    \n    dx = vx*5\n    dy = vy*5\n\n    # Arrow shaft\n    end_x = x + dx\n    end_y = y + dy\n\n    # Arrowhead calculation\n    angle = np.arctan2(dy, dx)\n    angle_offset = np.pi / 7\n    hx1_x = end_x - np.cos(angle - angle_offset)\n    hx1_y = end_y - np.sin(angle - angle_offset)\n    hx2_x = end_x - np.cos(angle + angle_offset)\n    hx2_y = end_y - np.sin(angle + angle_offset)\n\n    # Draw shaft\n    ax.quiver(x, y, dx, dy, angles='xy', scale_units='xy', scale=1, color='#007acc', width=0.005)\n    # Draw base point\n    ax.plot(x, y, 'o', color='#333')\n\n    # Labels\n    ax.text(x+10, y+10, f\"x = {x:.2f}\")\n    ax.text(x+10, y-5, f\"y = {y:.2f}\")\n    ax.text(end_x + 10, end_y - 20, f\"v‚Çì = {vx:.2f}\")\n    ax.text(end_x + 10, end_y, f\"v·µß = {vy:.2f}\")\n\n    plt.draw()\n    plt.pause(0.03)\n\n# Animation loop: update position by velocity\nfor i in range(500):\n    x += vx*0.1  # Update x position\n    y += vy*0.1  # Update y position\n\n    # Wrap around edges\n    x %= 600\n    y %= 250\n    \n    draw_arrow(x, y, vx, vy)\n\nplt.ioff()\n\n\n\n\nExercise 13.1 (Playing with steering arrows - Mathematical thinking) \nCopy-paste the code above, study it for a few minutes, and run it.\n\nWhat can you do to make the arrow accelerate?\n\nTo rotate a vector, we can use the following trigonometrical equations, where \\(\\theta\\) is the angle of rotation:\n\\[\n\\begin{aligned}\nx_{new} = x \\cdot cos(\\theta) - y \\cdot sin(\\theta) \\newline\ny_{new} = x \\cdot sin(\\theta) + y \\cdot cos(\\theta)\n\\end{aligned}\n\\] b. Use the equation above to rotate the velocity vector in the code by a small angle every timestep. What happens? c.¬†Modelling 1 individual is not very exciting. Think about what the code above would look like if you had more than 1 individual. Discuss this with other students and/or Bram.\n\n\nAnswers a. For this, you can simply multiply the velocity components every timestep by a value &gt; 1. For example:\n# Accellerate the velocity\nvx *= 1.01\nvy *= 1.01\nNote that this gets out of hand quite quickly. b. To do this, we need to store the new values in a temporary variable, and then assign them to the original variables. This is necessary because while we first modify vx, we still want to use the ‚Äòold‚Äô value of vx to calculate the new vy. The following code rotates the velocity vector by 0.05 radians:\n# Rotate the velocity vector by a small angle \nvxnew = vx * np.cos(0.05) - vy * np.sin(0.05)\nvynew = vx * np.sin(0.05) + vy * np.cos(0.05)\nvx, vy = vxnew, vynew\n\n\nIf you run this code, you will see that the dot will go in circles.\n\n\n\nWhile the code above stores x, y, vx, and vy as a ‚Äòglobal variables‚Äô, this is not good if we have many individuals. Instead, we can store the state of each individual in a list, dictionary, or a class. For example, we make a class ‚Äòcell‚Äô and store 100 of these ‚Äòcells‚Äô in a large list:\n\nclass Cell:\n    def __init__(self, x, y, vx, vy):\n        self.x = x\n        self.y = y\n        self.vx = vx\n        self.vy = vy\n# Create 100 cells\ncells = [Cell(np.random.rand(), np.random.rand(), 0.0, 0.0) for _ in range(100)]",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#moving-cells",
    "href": "evo_practical_1.html#moving-cells",
    "title": "13¬† Sticking together",
    "section": "Moving ‚Äúcells‚Äù",
    "text": "Moving ‚Äúcells‚Äù\nIn this practical, you will practice with modelling individuals in space by modifying a Python code based on the foraging cells shown at the beginning. To accommodate for many cells, we will define a new Cell class, embedded in a Simulation class.1\nFirst, read the code yourself (you can ignore the Visualisation class), and see if you can get it running on your own laptop.\n\n\n\n\n\n\nNoteSTARTING CODE FOR ‚Äúmoving cells‚Äù\n\n\n\n\n\n###\n# PRACTICAL 1 | \"Every cell for themselves?\"\n# This is the starting code. Follow the instructions in the practical to complete the code. \n# If you get stuck, you can look at the final code in `foraging_for_resources_final.py`, or ask\n# Bram. \n#\n# The structure of this code is as follows:\n# 1. Imports and parameters\n# 2. Simulation class\n# 3. Cell class\n# 4. Visualisation class (you do not need to change this)\n#\n###\n\n# 1. IMPORTS AND PARAMETERS\n# Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider\n\n# Parameters for simulation\nWORLD_SIZE = 200    # Width / height of the world (size of grid and possible coordinates for cells)\nMAX_VELOCITY = 0.3  # Maximum velocity magnitude\nMAX_FORCE = 0.3     # Maximum force magnitude\nRANDOM_MOVEMENT  = 0.01 # Random movement factor to add some noise to the cell's movement\n\n# Parameters for display\nDRAW_ARROW = True  # Draw the arrows showing the velocity direction of the cells\nINIT_CELLS = 20 # Initial number of cells in the simulation\nDISPLAY_INTERVAL = 1 # Frequency with which the plot is updated (e.g., every 10 timesteps can speed things up)\n\n# 1. MAIN LOOP (using functions and classes defined below)\ndef main():\n    \"\"\"Main function to set up and run the simulation.\"\"\"\n    # NOTE: The `Visualisation` class is responsible for managing the visualization \n    # of the simulation, including creating plots, updating them, and handling \n    # user interactions like the slider. As this has nothing to do with modeling\n    # per se, understanding this code is not necessary, but it can be fun to look\n    # at if you are interested. \n    \n    num_cells = INIT_CELLS\n    sim = Simulation(num_cells) \n\n    plt.ion()\n    vis = Visualisation(sim)\n\n    def update_cells(val):\n        sim.initialise_cells(int(vis.slider.val))\n        vis.redraw_plot(sim)\n        \n    # Connect the slider to the update function\n    vis.slider.on_changed(update_cells)\n\n    # Run simulation\n    for t in range(1, 10000):\n        \n        sim.simulate_step()\n        \n        if(t % DISPLAY_INTERVAL == 0):\n            # As long as only cells move, update only positions and timestamp\n            vis.update_plot(sim) \n            vis.ax.set_title(f\"Timestep: {t}\")\n            vis.fig.canvas.draw_idle()\n            plt.pause(10e-20)        \n        if(sim.redraw):\n            # When more has changes (e.g. number of cells or target position), redraw the plot\n            vis.redraw_plot(sim) \n            sim.redraw = False # Make sure it doesn't keep redrawing if not necessary\n        \n\n    # Keep the final plot open\n    plt.ioff()\n    # plt.show()\n\n\n\n# 2. SIMULATION CLASS\nclass Simulation:\n    \"\"\"Manages the grid, cells, target, and simulation logic.\"\"\"\n    def __init__(self, num_cells):\n        # Initialise a grid for the simulation\n        self.grid = np.zeros((WORLD_SIZE, WORLD_SIZE))  # Initialise an empty grid\n        self.fill_grid(self.grid, 0, 0, 0, 0)           # Fill grid with values (currently just 1s)\n        # Initialise a population of cells\n        self.cells = []\n        self.initialise_cells(num_cells)\n        # Place a 'target' in the middle\n        self.target_position = [WORLD_SIZE/2, WORLD_SIZE/2] \n        # A flag to only rebuild the plot when necessary (e.g. when the number of cells changes)\n        self.redraw = False\n\n    def simulate_step(self):\n        \"\"\"Simulate one timestep of the simulation.\"\"\"\n        for cell in self.cells:\n            # Actions taken by each cell. Most of them are still undefined, so you can implement them yourself.\n            self.move_towards_dot(cell)  \n            if self.check_target_reached(cell):\n                print(f\"Target reached!\")\n                self.reproduce_cell(cell)\n                self.redraw = True\n            \n            #self.avoid_collision(cell)\n            #self.stick_to_close(cell)\n            #self.find_peak(cell)\n\n            # Apply forces and update position\n            cell.apply_forces()\n            cell.update_position()\n\n            # Limit velocity to the maximum allowed\n            cell.vx = np.clip(cell.vx, -MAX_VELOCITY, MAX_VELOCITY)\n            cell.vy = np.clip(cell.vy, -MAX_VELOCITY, MAX_VELOCITY)\n\n    def initialise_cells(self, num_cells):\n        \"\"\"Initialise the cells with random positions and velocities.\"\"\"\n        self.cells = []\n        for _ in range(num_cells):\n            x = np.random.uniform(0, WORLD_SIZE)\n            y = np.random.uniform(0, WORLD_SIZE)\n            vx = np.random.uniform(-1, 1)\n            vy = np.random.uniform(-1, 1)\n            self.cells.append(Cell(x, y, vx, vy))\n\n    def fill_grid(self, grid, mean_x, mean_y, std_dev, noise=0):\n        \"\"\"\n        Write a function that takes the 2D grid and fills it with values representing \n        a Gaussian (normal) distribution centered at (mean_x, mean_y). See\n        if you can use the 'noise' argument to randomise the gaussian distribution a bit.\n        \n        Hint: e^{-x^2} yields a bell curve centered around 0. \n        \n        \"\"\"\n        for i in range(WORLD_SIZE):\n            for j in range(WORLD_SIZE):\n                x = i / (WORLD_SIZE - 1)\n                y = j / (WORLD_SIZE - 1)\n                grid[i, j] = 1 # This is 1 in the example, but should be a Gaussian distribution\n\n        # Normalize the grid to keep the total resource concentration the same\n        self.grid = grid\n    \n    def find_peak(self, cell):\n        \"\"\"Make the cell move towards the peak of the resource gradient with a random walk.\"\"\"\n        # Convert cell position to grid indices, as well as the previous position\n        grid_x = int(cell.x) % WORLD_SIZE\n        grid_y = int(cell.y) % WORLD_SIZE\n        next_x = (int(cell.x + 30*cell.vx) + WORLD_SIZE) % WORLD_SIZE \n        next_y = (int(cell.y + 30*cell.vy) + WORLD_SIZE) % WORLD_SIZE \n         \n    \n    def avoid_collision(self, cell):\n        \"\"\"Implement a simple collision avoidance mechanism. You can do so by\n        checking if this individual overlaps with another individual, and if so,\n        applying a repulsion force to the individual apposing the overlapping\n        direction.\"\"\"\n        for other_cell in self.cells:\n            if other_cell is not cell:\n                # Calculate the distance between the two cells\n                dx = cell.x - other_cell.x\n                dy = cell.y - other_cell.y\n                distance = np.sqrt(dx**2 + dy**2)\n                \n                    \n    def stick_to_close(self, cell):\n        \"\"\"Implement an attraction to cells that are nearby (but not overlapping)\"\"\"\n        for other_cell in self.cells:\n            if other_cell is not cell:\n                # Calculate the distance between the two cells\n                dx = cell.x - other_cell.x\n                dy = cell.y - other_cell.y\n                distance = np.sqrt(dx**2 + dy**2)\n\n    \n    def move_towards_dot(self, cell):\n        \"\"\"\n        Write your own function that applies forces in the direction of the dot.\n        Try to think of a way to apply the same force to every cell irrespective\n        of the distance to the dot, such that the cells move towards the dot at \n        the same speed. \n        \n        To get you started, the function already calculates dx and dy, which are\n        the distances to the target position in the x and y direction, respectively.\n        \"\"\"\n        # Calculate dx and dy\n        dx = self.target_position[0] - cell.x\n        dy = self.target_position[1] - cell.y\n        \n    \n    def check_target_reached(self, cell):\n        \"\"\"\n        Write your own function that checks if this cell has reached the target position.\n        You can do this by calculating the distance between the cell and the target.\n        If the distance is smaller than a certain threshold (e.g., 3 units), return True.\n        Otherwise, return False.\n        \"\"\"\n        \n        return(False)  # Dummy 'return' value. \n    \n    def reproduce_cell(self, cell):\n        \"\"\"\n        Write your own function that reproduces this cell. Think\n        about what it should inherit, and what it should *not* inherit. \n        \n        To keep the number of cell constant, you can first throw away a random cell.\n        \"\"\"\n        # Reproduce: Create a new cell with the same properties as the current cell\n        return(False) # Dummy 'return' value.\n\n        \n        \n# 3. CELL CLASS\nclass Cell:\n    \"\"\"Represents an individual cell in the simulation.\"\"\"\n    def __init__(self, x, y, vx, vy):\n        self.x = x\n        self.y = y\n        self.vx = vx\n        self.vy = vy\n        self.ax = 0\n        self.ay = 0\n        self.stickiness = 0.01 # Initial stickiness, can be adjusted later\n        \n    def update_position(self):\n        \"\"\"Update the cell's position based on its velocity.\"\"\"\n        self.x = (self.x + self.vx ) % WORLD_SIZE  # Wrap around the world\n        self.y = (self.y + self.vy ) % WORLD_SIZE  # Wrap around the world\n\n    def apply_forces(self):\n        \"\"\"Apply a force to the cell, updating its velocity.\"\"\"\n        self.ax = np.clip(self.ax, -MAX_FORCE, MAX_FORCE)\n        self.ay = np.clip(self.ay, -MAX_FORCE, MAX_FORCE)\n        self.vx += self.ax + RANDOM_MOVEMENT * np.random.uniform(-1, 1)\n        self.vy += self.ay + RANDOM_MOVEMENT * np.random.uniform(-1, 1)\n        # Apply drag to slow down the cell naturally\n        self.ax = 0\n        self.ay = 0\n        \n\n\n# Visualisation class for showing the individuals and the grid. For the practical, you do not need to change this. \nclass Visualisation:    \n    def __init__(self, sim):\n        fig, ax = plt.subplots(figsize=(6, 6))\n        self.cell_x = [cell.x for cell in sim.cells]\n        self.cell_y = [cell.y for cell in sim.cells]\n        self.cell_vx = np.array([cell.vx for cell in sim.cells])\n        self.cell_vy = np.array([cell.vy for cell in sim.cells])\n        self.cell_stickiness = np.array([cell.stickiness for cell in sim.cells])\n        # Colour cells by stickiness using inferno colormap\n        self.cell_scatter = ax.scatter(self.cell_x, self.cell_y, c=self.cell_stickiness, cmap='inferno', s=50, edgecolor='white', vmin=0, vmax=1)\n        if(DRAW_ARROW): self.cell_quiver = ax.quiver(self.cell_x, self.cell_y, self.cell_vx * 0.5, self.cell_vy * 0.5, angles='xy', scale_units='xy', scale=0.02, color='white')\n        plt.subplots_adjust(bottom=0.2)\n\n        ax.set_xlim(0, WORLD_SIZE)\n        ax.set_ylim(0, WORLD_SIZE)\n        ax.set_aspect('equal', adjustable='box')\n        ax.set_title(f\"Timestep: 0\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n\n        target_point=ax.scatter(sim.target_position[0], sim.target_position[1], c='purple', s=100, edgecolor='red')\n        grid_im=ax.imshow(sim.grid.T, extent=(0, WORLD_SIZE, 0, WORLD_SIZE), origin='lower', cmap='viridis', alpha=1.0)\n\n        self.fig = fig\n        self.ax = ax\n        self.target_point = target_point\n        self.grid_im = grid_im\n\n        # Add a slider for selecting the number of cells\n        ax_slider = plt.axes([0.2, 0.05, 0.6, 0.03])\n        self.slider = Slider(ax_slider, 'Cells', 1, 1000, valinit=len(sim.cells), valstep=1)\n\n    def update_cell_positions(self, sim):\n        \"\"\"Update the positions of the cells in the visualisation.\"\"\"\n        self.cell_x = [cell.x for cell in sim.cells]\n        self.cell_y = [cell.y for cell in sim.cells]\n        self.cell_vx = np.array([cell.vx for cell in sim.cells])\n        self.cell_vy = np.array([cell.vy for cell in sim.cells])\n        self.cell_stickiness = np.array([cell.stickiness for cell in sim.cells])\n    \n    def update_plot(self, sim):\n        self.update_cell_positions(sim)\n        self.cell_scatter.set_offsets(np.c_[self.cell_x,self.cell_y])\n        self.cell_scatter.set_array(self.cell_stickiness)\n        if(DRAW_ARROW): \n            self.cell_quiver.set_offsets(np.c_[self.cell_x, self.cell_y])\n            self.cell_quiver.set_UVC(self.cell_vx * 0.5, self.cell_vy * 0.5)        \n\n    def redraw_plot(self, sim):\n        self.update_cell_positions(sim)\n        cell_scatter_new = self.ax.scatter(self.cell_x, self.cell_y, c=self.cell_stickiness, cmap='inferno', s=50, edgecolor='white', vmin=0, vmax=1)\n        if(DRAW_ARROW): \n            cell_quiver_new = self.ax.quiver(self.cell_x, self.cell_y, self.cell_vx * 0.15, self.cell_vy * 0.15, angles='xy', scale_units='xy', scale=0.02, color='white')\n            self.cell_quiver.remove()\n            self.cell_quiver = cell_quiver_new\n        self.cell_scatter.remove()\n        self.fig.canvas.draw_idle()\n        self.cell_scatter = cell_scatter_new\n        self.grid_im.remove()\n        self.grid_im = self.ax.imshow(sim.grid.T, extent=(0, WORLD_SIZE, 0, WORLD_SIZE), origin='lower', cmap='viridis', alpha=1.0)\n        self.target_point.remove()\n        self.target_point=self.ax.scatter(sim.target_position[0], sim.target_position[1], c='purple', s=100, edgecolor='red')\n        plt.pause(10e-20)\n            \n            \n# 4. Execute the main loop\nif __name__ == \"__main__\":\n    # with cProfile.Profile() as pr:\n        main()\n        # pr.print_stats()\n\n\n\n\n\nMake sure you inspect the code. What features does the Simulation class have? What features does a Cell have?\nAs you can see if you inspected the code properly, many functions are left empty (or at least do not do anything yet). You will start filling these with your own code.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-movingtarget",
    "href": "evo_practical_1.html#sec-movingtarget",
    "title": "13¬† Sticking together",
    "section": "13.2 Moving the target",
    "text": "13.2 Moving the target\nIf you run the code, you will see a purple dot (with a red outline). This may represent a ‚Äútarget‚Äù. It could represent a resource patch for bacteria, but it could also be a piece of fruit for a monkey (at this point, the model is still very abstract, so both could be true). Let‚Äôs make the target change position around after an individual touches it.\n\nExercise 13.2 (Playing with steering arrows - Algorithmic/mathematical thinking) To help you on your way, first answer the following questions for yourself:\n\nHow can you calculate the distance between an individual and the orange dot?\nWhen is an individual close enough to the orange dot?\nHow can we assign a random position to the dot?\n\n\nA working code to steer the individuals towards the target is shown below. Note that the acceleration that is applied is only small, otherwise the individuals will move in a straight line towards the target very rapidly.\nFirst try it yourself. If you get stuck, ask Bram for help.\n\nAnswer\ndef move_towards_dot(self, cell):\n    \"\"\"Apply forces in the direction of the dot.\"\"\"\n    # Calculate dx and dy\n    dx = self.target_position[0] - cell.x\n    dy = self.target_position[1] - cell.y\n    # Calculate the distance to the target (Pythagorean theorem)\n    distance = np.sqrt(dx**2 + dy**2)\n    \n    # Normalize dx and dy \n    dx /= distance\n    dy /= distance\n    # Apply a small force towards the target\n    cell.ax += dx * 0.01\n    cell.ay += dy * 0.01",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-reproduction",
    "href": "evo_practical_1.html#sec-reproduction",
    "title": "13¬† Sticking together",
    "section": "13.3 Reproduction",
    "text": "13.3 Reproduction\nLet‚Äôs reward the individual that found the target. To do this, we can call the ‚ÄòCell‚Äô constructor to make a new cell, and add it to the list of cells:\nnew_cell = Cell(new_x, new_y, new_vx, new_vy, new_speed)\nself.cells.append(new_cell)\n\nExercise 13.3 (The birth of an arrow - Biological / algorithmic thinking) Consider which properties of the parent cell get inherited to the child:\n\nShould the exact position be inherited to offspring? (yes/no)\nShould the offspring be placed nearby its parent? (yes/no)\nDoes the velocity get inherited?\n\n\n\nAnswer A working code to reproduce a cell is shown below. Note that I decided to first throw an individual away, and then add the newborn individual (instead of the other way around). This ensures the newborn cannot be immediately thrown away, which would be a rather pointless event.\ndef reproduce_cell(self, cell):\n       # Reproduce: Create a new cell with the same properties as the current cell\n       angle = np.random.uniform(0, 2 * np.pi)\n       radius = np.random.uniform(0.05, 1.5)\n       new_x = cell.x + radius * np.cos(angle)\n       new_y = cell.y + radius * np.sin(angle)\n       new_cell = Cell(new_x, new_y, cell.vx, cell.vy)\n       random_cell = np.random.choice(self.cells)   \n       self.cells.remove(random_cell)\n       self.cells.append(new_cell)\n\nNote that depending on the scenario, the above questions may change. When a planktonic algea reproduces the daughter cells may inherit the velocity of the mother cell, but if a monkey gives birth, it does not make a lot of sense to talk about the ‚Äòvelocity‚Äô of the mother. If we consider plants, we should not even consider velocity of the individuals at any stage of their life.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-collision",
    "href": "evo_practical_1.html#sec-collision",
    "title": "13¬† Sticking together",
    "section": "13.4 Collision detection",
    "text": "13.4 Collision detection\nAs the cells move towards the dot, you may notice that cells start overlapping quite a bit. Let‚Äôs implement a simple form of collision detection, where overlap is resolved by pushing cells away from each other. Answer the following questions to get on your way:\n\nExercise 13.4 (Overlapping circles - Algorithmic/mathematical thinking) ¬†\n\nHow can you calculate the distance between two cells?\nWhen are two points overlapping?\nWhat can we do when two points overlap?\n\n\n\nAnswer A working code for cell-cell collision is shown below. Note that we apply a large force as we want this force to be able to overpower other forces that make the cells overlap.\ndef avoid_collision(self, cell):\n    \"\"\"Avoidance forces to prevent cells from colliding.\"\"\"\n    for other_cell in self.cells:\n        if other_cell is not cell:\n            # Calculate the distance between the two cells\n            dx = cell.x - other_cell.x\n            dy = cell.y - other_cell.y\n            distance = np.sqrt(dx**2 + dy**2)\n\n            # If the cells are too close, apply a repulsion force\n            if distance &lt; 5.0 and distance &gt; 0:  # Threshold for \"too close\"\n                # repulsion force proportional to the inverse of the distance\n                force_magnitude = (5.0 - distance) / distance\n                cell.ax += force_magnitude * dx * 100\n                cell.ay += force_magnitude * dy * 100\n\nNow that we have implemented both target-finding, reproduction, and collision, we can study these individual mechanisms by commenting one or the other out. This is an important process in understanding a model. Try it for yourself!",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-resourcepeak",
    "href": "evo_practical_1.html#sec-resourcepeak",
    "title": "13¬† Sticking together",
    "section": "13.5 Implementing a resource peak",
    "text": "13.5 Implementing a resource peak\nAs you may have noticed when reading the code, it also includes a grid. However, you don‚Äôt see this grid yet, as the function fill_grid currently sets every point to 1.\nWe can loop over the grid coordinates by using a double for-loop:\nfor i in range(WORLD_SIZE):\n  for j in range(WORLD_SIZE):\n    grid[i,j] = 1\nThe above function loops over all the grid points, and set the value of each grid point to 1. Let‚Äôs use the function np.exp to calculate a Gaussian that we will place in the center of the grid. The fill_grid already takes as arguments the grid, a relative x-coordinate (0-1), a relative y-coordinate (0-1), and a standard deviation (0-1). Get started by answering the question below.\n\nExercise 13.5 (Bell curves in space - Mathematical thinking) ¬†\n\nThe function \\(e^{-x^2}\\) gives a bell-curve centered around zero. How can you make it centered around a different value?\nCombined with the numpy function np.exp, how can we use the equation in question a to create a Gaussian that is at the center of the grid?\n\n\n\nAnswer A working code to implement a resource peak (with optional noise set to 0 by default) is shown below:\ndef fill_grid(self, grid, mean_x, mean_y, std_dev, noise=0):\n       \"\"\"Creates a Gaussian distribution with noise on the grid.\"\"\"\n       for i in range(WORLD_SIZE):\n           for j in range(WORLD_SIZE):\n               x = i / (WORLD_SIZE - 1)\n               y = j / (WORLD_SIZE - 1)\n               distance_squared = (x - mean_x)**2 + (y - mean_y)**2\n               grid[i, j] = np.exp(-distance_squared / (2 * std_dev**2)) * np.random.uniform(0.0, 1.0)**noise\n\n       # Normalize the grid to keep the total resource concentration the same\n       grid /= np.sum(grid)\n      self.grid = grid",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-runandtumble",
    "href": "evo_practical_1.html#sec-runandtumble",
    "title": "13¬† Sticking together",
    "section": "13.6 Run and tumble",
    "text": "13.6 Run and tumble\n\n\n\n‚ÄúRun and tumble‚Äù\n\n\nBacterial cells are so small, that they cannot detect a gradient directly (in other words, they don‚Äôt know in which direction resources are higher!). Instead, bacteria often use a ‚Äúrun and tumble‚Äù strategy. When they are currently not detecting an increase in the concentration (over time), they tumble. If they do detect an increase, they keep moving in the same direction. This is a very simple strategy, but it can be very effective mechanism for chemotaxis.\nIn our earlier code of a single, moving vector, we rotated the arrow by changing the ‚Äòangle‚Äô variable. However, these cells do not have an angle parameters, but only a velocity vector with components \\(v_x\\) and \\(v_y\\). If we want to rotate the velocity vector, we can use the following equation (rooted in basic trigonometry):\n\\[\nv_x' = v_x \\cdot cos(\\theta) - v_y \\cdot sin(\\theta) \\\\\nv_y' = v_x \\cdot sin(\\theta) + v_y \\cdot cos(\\theta)\n\\]\nWhere \\(\\theta\\) is the angle we want the vector to rotate (in radians, not degrees!), and \\(v_x\\) and \\(v_y\\) are the components of the vector. With this in mind, let‚Äôs try and model chemotaxis.\n\nExercise 13.6 (Run and tumble - Biology / algorithmic thinking) ¬†\n\nDetermine the concentration at the position of the cell, AND the predicted position of the cell after a small timestep (hint: use \\(v_x\\) and \\(v_y\\) to predict the future position! ask Bram if you get stuck)\nMake sure the future position is not outside of the grid! (hint: use Google, ChatGPT, or Copilot and figure out how the ‚Äúmodulo‚Äù operator works)\nIf the future position has a higher concentration than the current position, keep moving in more or less the same direction, with a very small change.\nIf the future position is a lower concentration, rotate the velocity vector a lot.\n\n\n\nAnswer A working code to implement a run-and-tumble mechanism shown below:\ndef find_peak(self, cell):\n       # Convert cell position to grid indices, as well as the previous position\n       grid_x = int(cell.x) % WORLD_SIZE\n       grid_y = int(cell.y) % WORLD_SIZE\n       next_x = (int(cell.x + 10*cell.vx) + WORLD_SIZE) % WORLD_SIZE \n       next_y = (int(cell.y + 10*cell.vy) + WORLD_SIZE) % WORLD_SIZE \n       # Get the resource value at cell's position, as well as the next position\n       resource_value = self.grid[grid_x, grid_y]\n       resource_next = self.grid[next_x, next_y]\n       \n       # Check if the cell is moving in the right direction\n       if resource_next &gt; resource_value:\n           # Moving in the right direction: small random adjustment\n           angle = np.random.uniform(-0.1, 0.1)  # Small angle change\n       else:\n           # Moving in the wrong direction: large random adjustment\n           angle = np.random.uniform(-np.pi*1.0, np.pi*1.0)  # Large angle change\n       \n       # Rotate the velocity vector by the angle calculated\n       new_vx = cell.vx * np.cos(angle) - cell.vy * np.sin(angle)\n       new_vy = cell.vx * np.sin(angle) + cell.vy * np.cos(angle)\n   \n       # Update the acceleration with the new velocity vector\n       cell.vx = new_vx\n       cell.vy = new_vy\n       cell.ax += cell.vx\n       cell.ay += cell.vy\n\nStudy if your individuals can find the resource peak. Notice that depending on your implementation, it may or may not work. Make sure to carefully investigate why it does or does not work.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#sec-sticking",
    "href": "evo_practical_1.html#sec-sticking",
    "title": "13¬† Sticking together",
    "section": "13.7 Sticking together",
    "text": "13.7 Sticking together\nCells sticking together can be implemented in multiple ways. Cells could be connected by a Newtonian spring, or we could simple make sure that cells that are close to each other are attracted to one another. In this case, we will use the latter method. Note that this is not very different from collision avoidance, but it is the other way around. In fact, we now have two opposing forces: cells are attracted to one another but do not want to overlap. This can be a bit finicky to get right, so feel free to explore. Ask for help if you get stuck.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#cells",
    "href": "evo_practical_1.html#cells",
    "title": "13¬† Sticking together",
    "section": "13.8 1000 cells?",
    "text": "13.8 1000 cells?\nTry to run the model with 1000 cells. Also go back to the starting code again (without all the additions), and run this code with 1000 or cells too.\n\nExercise 13.7 (Algorithmic / computational thinking) ¬†\n\nWhat happens? Can you explain this?\n\n\n\nAnswer It becomes quite slow, and in the microbial world 1000 cells is not all that much‚Ä¶ The earlier code without all the collision and stickiness did not have such a noticable slowing down. This is because the collision and stickiness functions both loop over all cells for each cell. For 1000 cells, this results in 1,000,000 interactions to check per timestep, which can be quite slow in Python. In other words, because we have to compare everyone with everyone, a 10-fold increase in the number of cells results in a 100-fold increase in the computation time. Not ideal.\n\nThis is far as this introduction to IBMs in python goes. Simple IBMs can be efficiently implemented in basic Python, but for more complex models, it is better to i) use numpy operations to speed up your Python code, or ii) use a faster programming language like C, Rust, or Javascript. For the last part of this pratical, we will study the Javascript version of this model. But before that, here is the final code that I ended up with:",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#final-python-code-for-interested-students",
    "href": "evo_practical_1.html#final-python-code-for-interested-students",
    "title": "13¬† Sticking together",
    "section": "13.9 Final Python code (for interested students)",
    "text": "13.9 Final Python code (for interested students)\nThis combination of individuals moving in continuous space, combined with a grid (e.g.¬†with resources, or other environmental states) is a very useful way to make a spatially structured model.\n\n\n\n\n\n\nNoteFinal code\n\n\n\n\n\n\n###\n# PRACTICAL 1 | \"Every cell for themselves?\"\n# Things in this model that you have tried to implement yourself:\n# 1. Implement collision avoidance\n# 2. Implement reproduction\n# 3. Implement a Gaussian grid\n# 4. Implement \"run and tumble\"\n# 5. Add noise to Gaussian, what happens?\n# 5. Modify collision into STICKING (a little finicky)\n# 6. Try it out with 500 cells... \n###\n\n###\n# PRACTICAL 1 | PLENARY DISCUSSION\n# What else was discussed in the plenary?\n# 1. Why are grids so popular in modelling?\n# 2. Tessellation of space\n# 3. Automatic tessellation of space: quad tree\n# 4. In the full model (javascript/Cacatoo), a quad tree is present, impacting performance\n###\n\n# 1. IMPORTS AND PARAMETERS\n# Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider\n\n# Parameters for simulation\nWORLD_SIZE = 200  # Width / height of the world (size of grid and possible coordinates for cells)\nMAX_VELOCITY = 0.3  # Maximum velocity magnitude\nMAX_FORCE = 0.3  # Maximum force magnitude\nDRAG_COEFFICIENT = 0.01  # Friction to slow down the cell naturally\nRANDOM_MOVEMENT  = 0.01 # Random movement factor to add some noise to the cell's movement\nCELL_STICKINESS_LOW = 0.0 # Minimal stickiness of cells in population\nCELL_STICKINESS_HIGH = 0.10 # Maximal stickiness of cells in population\n# Parameters for display\nDRAW_ARROW = False  # Draw the arrows showing the velocity direction of the cells\nNOISE = 2 # Noise factor for the Gaussian grid (noise amount is raised to the power of this value)\nINIT_CELLS = 64 # Initial number of cells in the simulation\nSEASON_DURATION = 1000 # Duration of a season, after which the Gaussian grid is regenerated\nDISPLAY_INTERVAL = 5\n\n# 1. MAIN LOOP (using functions and classes defined below)\ndef main():\n    \"\"\"Main function to set up and run the simulation.\"\"\"\n    # Initialise simulation and its # The `visualis` variable in the code snippet provided is actually\n    # a misspelling of the correct variable name `vis`, which stands\n    # for the `Visualisation` class instance. The `Visualisation`\n    # class is responsible for managing the visualization of the\n    # simulation, including creating plots, updating them, and\n    # handling user interactions like the slider.\n    \n    num_cells = INIT_CELLS\n    sim = Simulation(num_cells) \n\n    plt.ion()\n    vis = Visualisation(sim)\n\n\n    def update_cells(val):\n        sim.initialise_cells(int(vis.slider.val))\n        vis.redraw_plot(sim)\n        \n    # Connect the slider to the update function\n    vis.slider.on_changed(update_cells)\n\n    # Run simulation\n    for t in range(1, 10000):\n        sim.simulate_step()\n        if(t % DISPLAY_INTERVAL == 0):\n            vis.update_plot(sim)\n            vis.ax.set_title(f\"Timestep: {t}\")\n            vis.fig.canvas.draw_idle()\n            plt.pause(10e-20)\n        if(t % SEASON_DURATION==0):\n            sim.fill_grid(sim.grid, 0.2+np.random.uniform(0,0.6), 0.2+np.random.uniform(0,0.6), 0.1, NOISE)\n            vis.redraw_plot(sim)# Create Gaussian grid\n        # Update title and redraw the plot\n\n    # Keep the final plot open\n    plt.ioff()\n    # plt.show()\n\n\n\n# 2. SIMULATION CLASS\nclass Simulation:\n    \"\"\"Manages the grid, cells, target, and simulation logic.\"\"\"\n    def __init__(self, num_cells):\n        self.grid = np.zeros((WORLD_SIZE, WORLD_SIZE))  # Initialise an empty grid\n        self.cells = []\n        self.target_position = [WORLD_SIZE/3, WORLD_SIZE/3]  # Initial target position at the center\n        self.target_position = [-1,-1]\n        self.fill_grid(self.grid, 0.5, 0.5, 0.1, NOISE)  # Create Gaussian grid\n        self.initialise_cells(num_cells)\n\n    def simulate_step(self):\n        \"\"\"Simulate one timestep of the simulation.\"\"\"\n        for cell in self.cells:\n            # Actions taken by each cell. Most of them are still undefined, so you can implement them yourself.\n            #self.move_towards_dot(cell)  \n            #if self.check_target_reached(cell):\n            #    print(f\"Target reached! New target position: {self.target_position}\")\n            #    self.reproduce_cell(cell) \n            \n            self.avoid_collision(cell)\n            self.stick_to_close(cell)\n            self.find_peak(cell)\n            \n            # Apply drag force to acceleration\n            cell.ax += -DRAG_COEFFICIENT * cell.vx\n            cell.ay += -DRAG_COEFFICIENT * cell.vy\n\n            # Apply forces and update position\n            \n            cell.apply_forces()\n            cell.update_position()\n\n            # Limit velocity to the maximum allowed\n            cell.vx = np.clip(cell.vx, -MAX_VELOCITY, MAX_VELOCITY)\n            cell.vy = np.clip(cell.vy, -MAX_VELOCITY, MAX_VELOCITY)\n\n    def initialise_cells(self, num_cells):\n        \"\"\"Initialise the cells with random positions and velocities.\"\"\"\n        self.cells = []\n        for _ in range(num_cells):\n            x = np.random.uniform(0, WORLD_SIZE)\n            y = np.random.uniform(0, WORLD_SIZE)\n            vx = np.random.uniform(-1, 1)\n            vy = np.random.uniform(-1, 1)\n            self.cells.append(Cell(x, y, vx, vy))\n\n    def fill_grid(self, grid, mean_x, mean_y, std_dev, noise=0):\n        \"\"\"Creates a Gaussian distribution with noise on the grid.\"\"\"\n        for i in range(WORLD_SIZE):\n            for j in range(WORLD_SIZE):\n                x = i / (WORLD_SIZE - 1)\n                y = j / (WORLD_SIZE - 1)\n                distance_squared = (x - mean_x)**2 + (y - mean_y)**2\n                grid[i, j] = np.exp(-distance_squared / (2 * std_dev**2)) * np.random.uniform(0.0, 1.0)**noise\n\n        # Normalize the grid to keep the total resource concentration the same\n        grid /= np.sum(grid)\n        self.grid = grid\n    \n    def find_peak(self, cell):\n        \"\"\"Make the cell move towards the peak of the resource gradient with a random walk.\"\"\"\n        # Convert cell position to grid indices, as well as the previous position\n        grid_x = int(cell.x) % WORLD_SIZE\n        grid_y = int(cell.y) % WORLD_SIZE\n        next_x = (int(cell.x + 10*cell.vx) + WORLD_SIZE) % WORLD_SIZE \n        next_y = (int(cell.y + 10*cell.vy) + WORLD_SIZE) % WORLD_SIZE \n        # Get the resource value at the cell's position, as well as the previous position\n        resource_value = self.grid[grid_x, grid_y]\n        resource_next = self.grid[next_x, next_y]\n        \n        # Check if the cell is moving in the right direction\n        if resource_next &gt; resource_value:\n            # Moving in the right direction: small random adjustment\n            angle = np.random.uniform(-0.1, 0.1)  # Small angle change\n        else:\n            # Moving in the wrong direction: large random adjustment\n            angle = np.random.uniform(-np.pi*1.0, np.pi*1.0)  # Large angle change\n        \n        # Rotate the velocity vector by the random angle according to trigonometric rotation formulas\n        new_vx = cell.vx * np.cos(angle) - cell.vy * np.sin(angle)\n        new_vy = cell.vx * np.sin(angle) + cell.vy * np.cos(angle)\n\n        # Update the acceleration with the new velocity vector, such that the cell moves towards the peak\n        cell.vx = new_vx\n        cell.vy = new_vy\n        cell.ax += cell.vx\n        cell.ay += cell.vy\n         \n    \n    def avoid_collision(self, cell):\n        \"\"\"Avoidance forces to prevent cells from colliding.\"\"\"\n        for other_cell in self.cells:\n            if other_cell is not cell:\n                # Calculate the distance between the two cells\n                dx = cell.x - other_cell.x\n                dy = cell.y - other_cell.y\n                distance = np.sqrt(dx**2 + dy**2)\n\n                # If the cells are too close, apply a repulsion force\n                if distance &lt; 5.0 and distance &gt; 0:  # Threshold for \"too close\"\n                    # Calculate the repulsion force proportional to the inverse of the distance\n                    force_magnitude = (5.0 - distance) / distance\n                    cell.ax += force_magnitude * dx  * 100\n                    cell.ay += force_magnitude * dy * 100\n                    \n    def stick_to_close(self, cell):\n        \"\"\"Stick to closeby cells.\"\"\"\n        for other_cell in self.cells:\n            if other_cell is not cell:\n                # Calculate the distance between the two cells\n                dx = cell.x - other_cell.x\n                dy = cell.y - other_cell.y\n                distance = np.sqrt(dx**2 + dy**2)\n\n                # If the cells are too close, apply a repulsion force\n                if distance &lt; 12 and distance &gt; 5:  # Threshold for \"close\"\n                    # Calculate the repulsion force proportional to the inverse of the distance\n                    cell.ax -= cell.stickiness * dx *10\n                    cell.ay -= cell.stickiness * dy *10\n    \n    def move_towards_dot(self, cell):\n        \"\"\"Apply forces in the direction of the dot.\"\"\"\n        # Calculate dx and dy\n        dx = self.target_position[0] - cell.x\n        dy = self.target_position[1] - cell.y\n        # Calculate the distance to the target (pythagorean theorem)\n        distance = np.sqrt(dx**2 + dy**2)\n        \n        # Normalize dx and dy \n        dx /= distance\n        dy /= distance\n        # Apply a small force towards the target\n        cell.ax += dx * 0.01\n        cell.ay += dy * 0.01\n    \n    def check_target_reached(self, cell):\n        distance_to_target = np.sqrt((cell.x - self.target_position[0])**2 +\n                                         (cell.y - self.target_position[1])**2)\n        if distance_to_target &lt; 3:\n            # Set a new target position\n            self.target_position = [np.random.uniform(0, WORLD_SIZE), np.random.uniform(0, WORLD_SIZE)]\n            return(True)\n        return(False)\n    \n    def reproduce_cell(self, cell):\n        # Reproduce: Create a new cell with the same properties as the current cell\n        angle = np.random.uniform(0, 2 * np.pi)\n        radius = np.random.uniform(0.05, 1.5)\n        new_x = cell.x + radius * np.cos(angle)\n        new_y = cell.y + radius * np.sin(angle)\n        new_cell = Cell(new_x, new_y, cell.vx, cell.vy)\n        random_cell = np.random.choice(self.cells)   \n        self.cells.remove(random_cell)\n        self.cells.append(new_cell)\n\n\n        \n        \n        \n# 3. CELL CLASS\nclass Cell:\n    \"\"\"Represents an individual cell in the simulation.\"\"\"\n    def __init__(self, x, y, vx, vy):\n        self.x = x\n        self.y = y\n        self.vx = vx\n        self.vy = vy\n        self.ax = 0\n        self.ay = 0\n        if(np.random.uniform(0,1) &lt; 0.5): \n            self.stickiness = CELL_STICKINESS_LOW\n        else:\n            self.stickiness = CELL_STICKINESS_HIGH\n        \n    def update_position(self):\n        \"\"\"Update the cell's position based on its velocity.\"\"\"\n        self.x = (self.x + self.vx ) % WORLD_SIZE  # Wrap around the world\n        self.y = (self.y + self.vy ) % WORLD_SIZE  # Wrap around the world\n\n    def apply_forces(self):\n        \"\"\"Apply a force to the cell, updating its velocity.\"\"\"\n        self.ax = np.clip(self.ax, -MAX_FORCE, MAX_FORCE)\n        self.ay = np.clip(self.ay, -MAX_FORCE, MAX_FORCE)\n        self.vx += self.ax + RANDOM_MOVEMENT * np.random.uniform(-1, 1)\n        self.vy += self.ay + RANDOM_MOVEMENT * np.random.uniform(-1, 1)\n        self.ax = 0\n        self.ay = 0\n        \n\n\n# Visualisation class for showing the individuals and the grid. For the practical, you do not need to change this. \nclass Visualisation:    \n    def __init__(self, sim):\n        fig, ax = plt.subplots(figsize=(6, 6))\n        self.cell_x = [cell.x for cell in sim.cells]\n        self.cell_y = [cell.y for cell in sim.cells]\n        self.cell_vx = np.array([cell.vx for cell in sim.cells])\n        self.cell_vy = np.array([cell.vy for cell in sim.cells])\n        self.cell_stickiness = np.array([cell.stickiness for cell in sim.cells])\n        # Colour cells by stickiness using inferno colormap\n        self.cell_scatter = ax.scatter(self.cell_x, self.cell_y, c=self.cell_stickiness, cmap='inferno', s=50, edgecolor='white', vmin=0, vmax=CELL_STICKINESS_HIGH*1.2)\n        if(DRAW_ARROW): self.cell_quiver = ax.quiver(self.cell_x, self.cell_y, self.cell_vx * 0.15, self.cell_vy * 0.15, angles='xy', scale_units='xy', scale=0.02, color='darkblue')\n        plt.subplots_adjust(bottom=0.2)\n\n        ax.set_xlim(0, WORLD_SIZE)\n        ax.set_ylim(0, WORLD_SIZE)\n        ax.set_aspect('equal', adjustable='box')\n        ax.set_title(f\"Timestep: 0\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n\n        target_point=ax.scatter(sim.target_position[0], sim.target_position[1], c='orange', s=50, edgecolor='white')\n        grid_im=ax.imshow(sim.grid.T, extent=(0, WORLD_SIZE, 0, WORLD_SIZE), origin='lower', cmap='viridis', alpha=1.0)\n\n        self.fig = fig\n        self.ax = ax\n        self.target_point = target_point\n        self.grid_im = grid_im\n        # Add a slider for selecting the number of cells\n        ax_slider = plt.axes([0.2, 0.05, 0.6, 0.03])\n        self.slider = Slider(ax_slider, 'Cells', 1, 1000, valinit=len(sim.cells), valstep=1)\n\n    def update_cell_positions(self, sim):\n        \"\"\"Update the positions of the cells in the visualisation.\"\"\"\n        self.cell_x = [cell.x for cell in sim.cells]\n        self.cell_y = [cell.y for cell in sim.cells]\n        self.cell_vx = np.array([cell.vx for cell in sim.cells])\n        self.cell_vy = np.array([cell.vy for cell in sim.cells])\n        self.cell_stickiness = np.array([cell.stickiness for cell in sim.cells])\n    \n    def update_plot(self, sim):\n        self.update_cell_positions(sim)\n        self.cell_scatter.set_offsets(np.c_[self.cell_x,self.cell_y])\n        self.cell_scatter.set_array(self.cell_stickiness)\n        if(DRAW_ARROW): \n            self.cell_quiver.set_offsets(np.c_[self.cell_x, self.cell_y])\n            self.cell_quiver.set_UVC(self.cell_vx * 0.15, self.cell_vy * 0.15)        \n\n    def redraw_plot(self, sim):\n        self.update_cell_positions(sim)\n        cell_scatter_new = self.ax.scatter(self.cell_x, self.cell_y, c=self.cell_stickiness, cmap='inferno', s=50, edgecolor='white', vmin=0, vmax=CELL_STICKINESS_HIGH*1.2)\n        if(DRAW_ARROW): \n            cell_quiver_new = self.ax.quiver(self.cell_x, self.cell_y, self.cell_vx * 0.15, self.cell_vy * 0.15, angles='xy', scale_units='xy', scale=0.02, color='darkblue')\n            self.cell_quiver.remove()\n            self.cell_quiver = cell_quiver_new\n        self.cell_scatter.remove()\n        self.fig.canvas.draw_idle()\n        self.cell_scatter = cell_scatter_new\n        self.grid_im.remove()\n        self.grid_im = self.ax.imshow(sim.grid.T, extent=(0, WORLD_SIZE, 0, WORLD_SIZE), origin='lower', cmap='viridis', alpha=1.0)\n        \n        plt.pause(0.01)\n            \n            \n# 4. Execute the main loop\nif __name__ == \"__main__\":\n    # with cProfile.Profile() as pr:\n        main()\n        # pr.print_stats()",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#exploring-the-full-javascript-model-with-evolution",
    "href": "evo_practical_1.html#exploring-the-full-javascript-model-with-evolution",
    "title": "13¬† Sticking together",
    "section": "13.10 Exploring the full Javascript model with evolution",
    "text": "13.10 Exploring the full Javascript model with evolution\nThe full model is implemented in Javascript, and can be found here.\nIn this full model, cells also reproduce every once in a while (when a ‚Äúseason‚Äù ends). Their reproductive success is shaped by the amount of resources at that position. Every time a cell reproduces there it inherits the parents stickiness, but it can also change this value a bit. This way, stickiness is an ‚Äúevolvable‚Äù property on which natural selection will act. How much each cell is attracted to nearby cells depends on an internal ‚Äústickiness‚Äù parameter. Let the simulation run for some time.\n\nExercise 13.8 (The evolution of stickiness - Biology) ¬†\n\nWhat happens to the evolution of stickiness?\nIdentify multiple advantages and disadvantages of stickiness.\nGiven your answer in b., can you name an important parameter that may determine the balance bewteen the advantages and disadvantages of stickiness? See if you can test it using the options provided.\n\n\n\nAnswer a. In the full model, stickiness evolves to be higher. Because we select for individuals that find the resource peaks, we can state that natural selection favours stickiness. There is however a limit to this, as stickiness does not go beyond ~0.25.  b. The advantages of stickiness are that larger groups of cells are better at steering towards the resources. Even if a single cell tries to randomly go in the wrong direction (due to its tumbling mechanic), it will be pulled back. In other words, together these cells are far less sensitive to noise. Another distinct advantage you may have noticed is that the stickiest cells are sorted to be in the center of the group (as you have already seen earlier in this course!). That means that even after the peak was found, the stickiest cells have an advantage over the other cells as they can occupy the peak of the resource distribution. The disadvantages of stickiness are that larger groups clearly move more slowly, and that the sticky clusters tend to stay together even if there are many resource peaks (and therewith miss out on resources).  c.¬†The duration of the seasonal cycle is very important. If it is very long, it does not matter that the large clusters move slowly, as they will be better at finding and staying on the peak by being sticky. If the cycle is however very short, it may be more important to be able to move quickly and change direction rapidly. In that case, being too sticky may be a problem. As you modify this parameter in the online model, you will indeed see that shorter seasons mostly cause stickiness to drift close to 0.0, while longer seasons cause stickiness to evolve towards 0.25 or even higher.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_1.html#footnotes",
    "href": "evo_practical_1.html#footnotes",
    "title": "13¬† Sticking together",
    "section": "",
    "text": "The code also contains a Visualisation class that uses the matplotlib library to draw the cells and their movement, which we have tuned to speed things up a bit. You do not need to understand this part of the code, but if you are interested feel free to check it out.‚Ü©Ô∏é",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Sticking together</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html",
    "href": "evo_practical_2.html",
    "title": "14¬† What gets selected?",
    "section": "",
    "text": "14.1 Genotypes, phenotypes, and evolutionary algorithms\nNear the end of the lecture, we discussed the differences between the genotype (that which mutates) and the phenotype (that which is selected). Although you have likely already heard about these concepts, how can we study them in evolutionary models? During this practical, you will write your own evolutionary algorithms of increasing complexity, in order to learn about these topics. By the end of this practical, you should understand why the translation from genotype to phenotype (often referred to as the genotype-phenotype map) is such an important concept in evolutionary biology.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#simple-model-where-fitness-as-a-number",
    "href": "evo_practical_2.html#simple-model-where-fitness-as-a-number",
    "title": "14¬† What gets selected?",
    "section": "14.2 Simple model where fitness as a number",
    "text": "14.2 Simple model where fitness as a number\nIn the introduction of this course part on evolution, we have already looked at as simple ‚ÄúMoran process‚Äù:\n\nStart with a population of individuals, each with a fitness value.\nSelect individuals based on their fitness to reproduce.\nReplace a random individual with this newly generate offspring.\nWith a small probability, modify the ‚Äòfitness‚Äô value of the newborn.\n\nAnd so on.\nWith a Moran process, competition between individuals is modelled in a very indirect (‚Äúimplicit‚Äù) way. By always selecting fit individuals, and removing a random other, any individual in the populations could be replaced by another individual, which statistically is a fitter individual. One could thus say that ‚Äúeveryone is competing with everyone‚Äù. A different method is often applied in spatially structured models, as in this case only nearby individuals are competing. Then, we could sample who wins from an imaginary roulette wheel:\n\nAs can be seen, not all individual have the same size on the roulette wheel. That depicts differences in their growth rates, biomass, or other approximations of ‚Äúfitness‚Äù. Also, the roulette wheel contains an area (shown in black) that shapes the chance that nobody reproduces. We will try and implement this rule to let individuals reproduce based on their fitness, but with only 10 competitors at a time. Let‚Äôs start with a code where individuals have a ‚Äúfitness‚Äù value, but it not yet used for selection (see below). Read/test this code thoroughly before you move on to the next section.\n\n\n\n\n\n\nNoteCODE FOR ‚Äúfitness without fitness‚Äù\n\n\n\n\n\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\n# Set the random number seed for reproducibility\nrandom.seed(0)\n\nplt.ion()  # Enable interactive plotting\n\n# --- PARAMETERS ---\ninitial_fitness = 0.1            # Starting fitness for all individuals\npopulation_size = 500             # Number of individuals (should be a square number for grid mode)\ngenerations = 20000               # Number of generations to simulate\nmutation_rate = 0.005            # Probability of mutation per reproduction event\nsample_interval = 5               # How often to sample and plot data\n\n# --- INITIALIZATION ---\n# Create initial population: all individuals start with the same fitness\npopulation = [initial_fitness for _ in range(population_size)]\n\n# Lists to track average fitness and diversity over time\navg_fitness = []\ndiversity_over_time = []\n\n# --- CORE FUNCTIONS ---\n\ndef mutate(fitness, rate=mutation_rate):\n    \"\"\"Mutate the fitness value with a given probability.\"\"\"\n    if random.random() &lt; rate:\n        # Fitness changes by a random value in [-0.1, 0.1], clipped to [0, 1]\n        return min(1.0, max(0.0, fitness + random.uniform(-0.1, 0.1)))\n    return fitness\n\ndef calculate_diversity(population):\n    \"\"\"NOT YET IMPLEMENTED! Calculate diversity as the standard deviation of fitness values.\"\"\"\n    return 0 \n\n# --- PLOTTING SETUP ---\nfig, ax1 = plt.subplots(figsize=(12, 8))\nax1.set_xlabel(\"Generation\")\nax1.set_ylabel(\"Average Fitness\", color='tab:blue')\nax1.set_ylim(0, 1)\nline1, = ax1.plot([], [], color='tab:blue', linewidth=2, label='Fitness')\nax1.tick_params(axis='y', labelcolor='tab:blue')\n\n# Second y-axis for diversity\nax2 = ax1.twinx()\nax2.set_ylabel(\"Diversity\", color='tab:green')\nline2, = ax2.plot([], [], color='tab:green', linestyle=':', linewidth=2, label='Diversity')\nax2.tick_params(axis='y', labelcolor='tab:green')\n\nfig.suptitle(\"Evolution Toward Fitness 1\")\nfig.tight_layout()\nfig.legend(loc='upper right')\nplt.grid(True)\nplt.draw()\n\n# --- EVOLUTION LOOP ---\nbest_fitness = -1\nfound = False\n\nfor gen in range(generations):\n    total_fit = sum(population)\n    best = max(population)\n    # Print when a perfect solution is found\n    if best == 1 and not found:\n        found = True\n        print(\"Found perfect solution at generation\", gen)\n        \n    # Sample and plot data at intervals\n    if gen % sample_interval == 0:\n        avg_fitness.append(total_fit / population_size)\n        diversity_over_time.append(calculate_diversity(population))\n        x_vals = [i * sample_interval for i in range(len(avg_fitness))]\n        line1.set_data(x_vals, avg_fitness)\n        line2.set_data(x_vals, diversity_over_time)\n        ax1.relim(); ax1.autoscale_view()\n        ax2.relim(); ax2.autoscale_view()\n        fig.suptitle(f\"Best Fitness: {best:.2f}\", fontsize=14)\n        plt.pause(0.01)\n        \n\n    # --- MORAN PROCESS ---\n    # For each individual, perform a reproduction event\n    for _ in range(100):  # 100 competition events per generation\n        # Select 1 random individual for replication\n        probs = [1 for fit in population] # All probability weights are equal (1.0)\n        parent_idx = random.choices(range(len(population)), weights=probs)[0] # Grab one random individual based on an unweighted list...\n        # Select individual to be replaced (uniform random)\n        dead_idx = random.randrange(len(population))\n        # Copy population for next generation\n        new_pop = population.copy()\n        # Offspring replaces the dead individual (with possible mutation)\n        new_pop[dead_idx] = mutate(population[parent_idx])\n        population = new_pop\n\ninput(\"\\nSimulation complete. Press Enter to exit plot window...\")",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#making-a-roulette-wheel-with-everyone-in-it",
    "href": "evo_practical_2.html#making-a-roulette-wheel-with-everyone-in-it",
    "title": "14¬† What gets selected?",
    "section": "14.3 Making a roulette wheel with everyone in it",
    "text": "14.3 Making a roulette wheel with everyone in it\nIf you have read the code, you will see that we can pass a list of weights to the function random.choices, to determine who is most likely to be sampled. Currently, all the weights are set to 1:\nprobs = [1 for fit in population] # All probability weights are equal (1.0)\n\nRun the code with the current (all equal) weights. What happens?\nModify this line of code to take the fitness values as the weight, rather than 1. (hint: this is a VERY small change in the code).",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#a-roulette-wheel-of-a-subset-of-individuals",
    "href": "evo_practical_2.html#a-roulette-wheel-of-a-subset-of-individuals",
    "title": "14¬† What gets selected?",
    "section": "14.4 A roulette wheel of a subset of individuals",
    "text": "14.4 A roulette wheel of a subset of individuals\nInstead of letting everyone reproduce, let us modify the code to only sample from a smaller list of ‚Äòcompetitors‚Äô, and spin a virtual roulette wheel to determine who wins. There are many ways to implement this, but here‚Äôs how we will do it. We will sample N individuals from the population, and implement the following algorithm:\n\nSelect a random subset of N individuals from the population.\nTake/compute the fitness of each selected individual.\nAdd a reproduction-skip option with a fixed weight.\nChoose one individual or the skip option using weighted random selection.\nIf an individual was chosen, mutate it and replace a random individual in the population.\n\nBelow, there‚Äôs a small snippet of code doing what is explained above1. The variable no_reproduction_chance is the fixed weight that nobody gets to reproduce:\n\n\n\n\n\n\nNoteRoulette wheel algorithm\n\n\n\n\n\n  tournament_size = 10  \n  no_reproduction_chance = 1\n  \n  competitors = random.sample(population, tournament_size)\n  # Make a list of their fitness values\n  fitness_values = [fit for fit in competitors]\n  total = sum(fitness_values)\n  # Add a \"no reproduction\" dummy competitor with fitness = 0\n  competitors_with_dummy = competitors + [None]\n  probs = [f / total for f in fitness_values] + [no_reproduction_chance]\n  winner = random.choices(competitors_with_dummy, weights=probs, k=1)[0]\n  if winner is not None:\n      # Mutate winner to produce offspring\n      offspring = mutate(winner)\n      remove_idx = random.randrange(len(population))\n      population[remove_idx] = offspring  \n        \n\n\n\nAfter you understand the roulette wheel algorithm, do the following exercise:\n\nExercise 14.1 (Questions about the roulette wheel - Algorithmic thinking) \n\nLet‚Äôs imagine a moment where the roulette wheel contains only 10 highly unfit individuals (e.g.¬†all fitness values are 0.01). What is the chance that someone will reproduce? (you don‚Äôt have to calculate it, but give your reasoning)\nAnswer the same question as in a., but now imagine that all 10 individuals have a fitness value of 1.\nAnswer question b. and c.¬†again, but now assume that no_reproduction_chance is equal to 0.\nDescribe what the no_reproduction_chance parameter does in biological terms.\nSpatially structured populations are often placed on a grid. Describe how you could implement a roulette wheel to resolve local competition, e.g. when an empty grid point is competed for by the neighbours.\n\n\n\nAnswer a. If we sample 10 unfit individuals, the weight of the no_reproduction_event is proportionally high (the black slice of the roulette wheel is big). Hence, there is only a small chance that anyone will reproduce to begin with. b. If we sample 10 fit individuals (fitness 1), the chances are much higher that someone will reproduce. c.¬†If the dummy value is 0, the chances that someone will reproduce are the same in both scenarios, as even with very unfit individuals there is no chance that nobody reproduces. d.¬†In nature, if no individual is sufficiently fit, reproduction may not occur at all. For example, if the competing individuals are bacteria with very low glucose uptake rates, they may not yet be physiologically ready to reproduce. In such cases, population size should remain stable or even decline if death is also occurring. The no_reproduction_event captures this by ensuring that fitness is not judged solely in relative terms against other individuals, but also in absolute terms against environmental demands. e. If a grid point is empty (contains no individual), make a list of (up to) 8 individuals around that grid point. Apply the roulette wheel for those individuals, and place the ‚Äòwinner‚Äô inside the empty grid point.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#diversity-patterns",
    "href": "evo_practical_2.html#diversity-patterns",
    "title": "14¬† What gets selected?",
    "section": "14.5 Diversity patterns",
    "text": "14.5 Diversity patterns\nModify the function calculate_diversity to calculate the diversity of the population as the standard deviation of the fitness values.\n\nExercise 14.2 (The dynamics of diversity - Biology) \n\nUse a low mutation rate and study the dynamics of diversity. Describe the pattern verbally.\n\n\n\nAnswer A snippet to calculate the standard deviation of a population is shown below. Note that this value is already being plotted, so if you modify this function you ought to be able to see what happens immediately.\ndef calculate_diversity(population):\n   \"\"\"Calculate diversity as the standard deviation of fitness values.\"\"\"\n   mean = sum(population) / len(population)\n   variance = sum((x - mean) ** 2 for x in population) / (len(population) - 1)\n   return math.sqrt(variance)\n\nThe green dotted line is the standard deviation of the values in the population (‚Äòdiversity‚Äô). From this we can see that the population is only briefly diverse whenever a new, fit individual appears. In between these phases, diversity is 0. This makes intuitive (biological) sense, as with a low mutation rate the only moments where there is more than 1 species is during the invasion of a new mutant. During all other phases, there is just a single (fittest) species.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#evolving-a-dna-sequence",
    "href": "evo_practical_2.html#evolving-a-dna-sequence",
    "title": "14¬† What gets selected?",
    "section": "14.6 Evolving a DNA sequence",
    "text": "14.6 Evolving a DNA sequence\nA big problem with the previous model is there is no true distinction between genotype (that which mutates) and phenotype (that which is selected). Let us try and adapt the model to be more biologically meaningful, by making each individual represented by a DNA sequence. Copy the following code:\n\n\n\n\n\n\nNoteStarting code for ‚Äúevolving a DNA sequence‚Äù\n\n\n\n\n\nimport random\nimport math\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# set the random number seed\nrandom.seed(0)\n\nplt.ion()  # Enable interactive mode\n\n# Parameters\nalphabet = \"ATCG\"\ntarget_sequence = \"GATGCGCGCTGGATTAAC\"  # Example target sequence\ndna_length = len(target_sequence)\ntarget_length = len(target_sequence)\n\n# Simulation settings\npopulation_size = 500  # must be a square number for grid mode\ngenerations = 20000\nmutation_rate = 0.001\nsample_interval = 5\nsample_size = population_size\nno_reproduction_chance = 0.1\n\n# Core functions\ndef fitness(dna):\n    return 1 - sum(a != b for a, b in zip(dna, target_sequence)) / target_length\n\ndef mutate(dna, rate=mutation_rate):\n    return ''.join(\n        random.choice([b for b in alphabet if b != base]) if random.random() &lt; rate else base\n        for base in dna\n    )\n\ndef count_beneficial_mutations(dna):\n    f0 = fitness(dna)\n    count = 0\n    for i in range(len(dna)):\n        for b in alphabet:\n            if b != dna[i]:\n                mutant = dna[:i] + b + dna[i+1:]\n                if fitness(mutant) &gt; f0:\n                    count += 1\n    return count\n\ndef diversity(pop):\n    counts = {}\n    for ind in pop:\n        counts[ind] = counts.get(ind, 0) + 1\n    total = len(pop)\n    return -sum((c/total) * math.log(c/total + 1e-9) for c in counts.values()) if total &gt; 0 else 0\n\n# Initialize population\ninitial_sequence = \"GATAGCGAAGTTTAGCCG\" # far from target (only first 3 are correct)\npopulation = [initial_sequence for _ in range(population_size)]\n\navg_fitness = []\navg_beneficial = []\ndiversity_over_time = []\nbest_individuals = []\n\ndef get_neighbors(i, j):\n    return [(x % side, y % side)\n            for x in range(i-1, i+2)\n            for y in range(j-1, j+2)]\n\n# Initialize interactive plot\nfig, ax1 = plt.subplots(figsize=(12, 8))\nax1.set_xlabel(\"Generation\")\nax1.set_ylabel(\"Average Fitness\", color='tab:blue')\nax1.set_ylim(0, 1)\nline1, = ax1.plot([], [], color='tab:blue', linewidth=2, label='Fitness')\nax1.tick_params(axis='y', labelcolor='tab:blue')\n\nax2 = ax1.twinx()\nax2.set_ylabel(\"Beneficial Mutations / Diversity\", color='tab:purple')\nline2, = ax2.plot([], [], color='tab:purple', linestyle='--', linewidth=2, label='Beneficial Mutations')\nline3, = ax2.plot([], [], color='tab:green', linestyle=':', linewidth=2, label='Diversity')\nax2.tick_params(axis='y', labelcolor='tab:purple')\nfig.suptitle(\"Evolution Toward Target Sequence\")\nfig.tight_layout()\nax2.set_ylim(0, 20)\nfig.legend(loc='upper right')\nplt.grid(True)\nplt.draw()\n\nbest_seq = \"\"\nbest_score = -1\nfound = False\n\n# Evolution loop\nfor gen in range(generations):\n    fitnesses = [fitness(ind) for ind in population]\n    total_fit = sum(fitnesses)\n    best = max(fitnesses)\n    if(best == 1 and not found):\n        found = True\n        print(\"Found perfect solution at generation\", gen)\n        \n    if gen % sample_interval == 0:\n        sample = random.sample(population, sample_size)\n        avg_beneficial.append(sum(count_beneficial_mutations(ind) for ind in sample) / sample_size)\n        diversity_over_time.append(diversity(population))\n\n        # Update plot data\n        line1.set_data(range(len(avg_fitness)+1), avg_fitness + [sum(fitnesses)/population_size])\n        line2.set_data(range(len(avg_beneficial)), avg_beneficial)\n        line3.set_data(range(len(diversity_over_time)), diversity_over_time)\n        ax1.relim(); ax1.autoscale_view()\n        ax2.relim(); ax2.autoscale_view()\n        best = max(population, key=fitness)\n        fig.suptitle(f\"Best: {best} (target: {target_sequence})\", fontsize=14)\n        plt.pause(0.01)\n\n    else:\n        avg_beneficial.append(avg_beneficial[-1])\n        diversity_over_time.append(diversity_over_time[-1])\n\n    # Tournament selection (as in evolving_fitness_final.py)\n    new_pop = []\n    tournament_size = 10  # can be adjusted\n\n    for _ in range(population_size):\n        # Select tournament_size individuals randomly\n        competitors = random.sample(population, tournament_size)\n        # Pick the one with highest fitness\n        fitness_values = [fitness(ind) for ind in competitors]\n        total = sum(fitness_values)\n        \n        probs = [f / total for f in fitness_values]\n        winner = random.choices(competitors, weights=probs, k=1)[0]\n        # Mutate winner to produce offspring\n        offspring = mutate(winner)\n        new_pop.append(offspring)\n\n    population = new_pop\n\n    avg_fitness.append(sum(fitness(ind) for ind in population) / population_size)\n    if gen % 250 == 0:\n        best = max(population, key=fitness)\n        best_individuals.append((gen, best))\n\ninput(\"\\nSimulation complete. Press Enter to exit plot window...\")\n\n\n\nAnswer the following questions using the options available in the model:\n\nExercise 14.3 (Evolving DNA - Biology / abstract thinking) ¬†\n\nRun the code. What does the new (dashed blue) line represent? Do you understand how is changes over time?\n\nThe program reports after how many generations it manages to find the target sequence. With default settings this can take a long time‚Ä¶ (default: 429 generations)\n\nModify the mutation rate to see how it affects the time to find the target sequence. Try different mutation rates between 0.0001 and 0.1. Keep track of both how long (number of generations) it takes to find the target, and how fit the population is once the target is found. What do you observe?\nDiversity is no longer calculated as the standard deviation in fitness, but as the Shannon diversity of all present sequences (although it is not super complex, you do not need to fully understand this function). Because of this, the exact number (quantities) cannot be compared to our ealrier model. Do you see a qualitative differences?\nStudy how fitness is calculated in this model. Is there a distinction between genotype en phenotype? Why/why not?\n\n\n\nAnswer a. The new blue dotted line represents how many mutations are beneficial (towards the target). As the population gets closer to the target, the number of beneficial mutations decreases. As such, this line is a mirror image of the fitness in the population. We will look a bit deeper into this line in the next model.  b. Generally speaking, a higher mutation rate helps to find the target faster. However, with high mutation rates (0.01 or higher), the fitness after the target is found starts to decrease, as individual produce many (unfit) mutants. In fact, if mutation rate is too high (approximately 0.04 or higher), the population fails to find the target at all, as reproduction is too inaccurate! This concept is known as the ‚ÄòError Threshold‚Äô or the ‚ÄòError Catastrophe‚Äô in evolutionary biology.  c.¬†Qualitatively, there is no clear difference to what we saw before: diversity only peaks at moments when there is a new mutant coming in, but otherwise diversity is still 0. d.¬†There is no distinction between genotype and phenotype. Fitness is directly calculated from the DNA sequence, so there is no ‚Äògenotype-to-phenotype mapping‚Äô.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#evolving-a-protein-sequence",
    "href": "evo_practical_2.html#evolving-a-protein-sequence",
    "title": "14¬† What gets selected?",
    "section": "14.7 Evolving a protein sequence",
    "text": "14.7 Evolving a protein sequence\nNext we will extend the simulation a little more. The individual genotypes will still be represented as a DNA sequence, but before evaluating fitness this will be translated into a protein sequence. To do so, the code first defines the codon table (which we of course all know by heart =)), and then translates the DNA sequence into a protein sequence. The protein sequence is then used to calculate the fitness of the individual, which is based on how well the protein sequence matches a target protein sequence. The code is as follows:\n\n\n\n\n\n\nNoteStarting code for evolving a protein sequence\n\n\n\n\n\nimport random\nimport math\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# set the random number seed\nrandom.seed(0)\n\nplt.ion()  # Enable interactive mode\n\n# Codon table\ncodon_table = {\n    'TTT': 'F', 'TTC': 'F', 'TTA': 'L', 'TTG': 'L', 'CTT': 'L', 'CTC': 'L', 'CTA': 'L', 'CTG': 'L',\n    'ATT': 'I', 'ATC': 'I', 'ATA': 'I', 'ATG': 'M',\n    'GTT': 'V', 'GTC': 'V', 'GTA': 'V', 'GTG': 'V',\n    'TCT': 'S', 'TCC': 'S', 'TCA': 'S', 'TCG': 'S', 'AGT': 'S', 'AGC': 'S',\n    'CCT': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P',\n    'ACT': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n    'GCT': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A',\n    'TAT': 'Y', 'TAC': 'Y', 'CAT': 'H', 'CAC': 'H',\n    'CAA': 'Q', 'CAG': 'Q', 'AAT': 'N', 'AAC': 'N',\n    'AAA': 'K', 'AAG': 'K', 'GAT': 'D', 'GAC': 'D',\n    'GAA': 'E', 'GAG': 'E', 'TGT': 'C', 'TGC': 'C',\n    'TGG': 'W', 'CGT': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R', 'AGA': 'R', 'AGG': 'R',\n    'GGT': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G',\n    'TAA': '*', 'TAG': '*', 'TGA': '*'\n}\n\n# Parameters\nalphabet = \"ATCG\"\ntarget_protein = \"DARWIN\"\ndna_length = len(target_protein)*3\ntarget_length = len(target_protein)\n\n# Simulation settings\npopulation_size = 625  # must be a square number for grid mode\ngenerations = 20000\nmutation_rate = 0.0005   \nsample_interval = 5\nsample_size = population_size\nno_reproduction_chance = 0.01\n\n# Core functions\ndef translate(dna):\n    return ''.join(codon_table.get(dna[i:i+3], '?') for i in range(0, len(dna), 3))\n\ndef fitness(dna):\n    protein = translate(dna)\n    return 1 - sum(a != b for a, b in zip(protein, target_protein)) / target_length\n\ndef mutate(dna, rate=mutation_rate):\n    return ''.join(\n        random.choice([b for b in alphabet if b != base]) if random.random() &lt; rate else base\n        for base in dna\n    )\n\ndef count_beneficial_mutations(dna):\n    f0 = fitness(dna)\n    count = 0\n    for i in range(len(dna)):\n        for b in alphabet:\n            if b != dna[i]:\n                mutant = dna[:i] + b + dna[i+1:]\n                if fitness(mutant) &gt; f0:\n                    count += 1\n    return count\n\ndef diversity(pop):\n    counts = Counter(pop)\n    total = len(pop)\n    return -sum((c/total) * math.log(c/total + 1e-9) for c in counts.values()) if total &gt; 0 else 0\n\n# Initialize population\ninitial_sequence = ''.join(random.choice(alphabet) for _ in range(dna_length))\npopulation = [initial_sequence for _ in range(population_size)]\n\navg_fitness = []\navg_beneficial = []\ndiversity_over_time = []\nbest_individuals = []\n\n# Grid setup\nside = int(math.sqrt(population_size))\nassert side * side == population_size, \"Population size must be a square number for grid mode\"\n\ndef get_neighbors(i, j):\n    return [(x % side, y % side)\n            for x in range(i-1, i+2)\n            for y in range(j-1, j+2)]\n\n# Initialize interactive plot\nfig, ax1 = plt.subplots(figsize=(12, 8))\nax1.set_xlabel(\"Generation\")\nax1.set_ylabel(\"Average Fitness\", color='tab:blue')\nax1.set_ylim(0, 1)\nline0, = ax1.plot([], [], color='black', linewidth=1, label='Max fitness')\nline1, = ax1.plot([], [], color='tab:blue', linewidth=2, label='Fitness')\nax1.tick_params(axis='y', labelcolor='tab:blue')\n\nax2 = ax1.twinx()\nax2.set_ylabel(\"Beneficial Mutations / Diversity\", color='tab:purple')\nline2, = ax2.plot([], [], color='tab:purple', linestyle='--', linewidth=2, label='Beneficial Mutations')\nline3, = ax2.plot([], [], color='tab:green', linestyle=':', linewidth=2, label='Diversity')\nax2.tick_params(axis='y', labelcolor='tab:purple')\nfig.suptitle(\"Evolution Toward \" + str(target_protein))\nfig.tight_layout()\nax2.set_ylim(0, 5)\nfig.legend(loc='upper right')\nplt.grid(True)\nplt.draw()\n\nbest_seq = \"\"\nbest_score = -1\nbest_fitnesses = []\nfound = False\n\n# Evolution loop\nfor gen in range(generations):\n    fitnesses = [fitness(ind) for ind in population]\n    total_fit = sum(fitnesses)\n    best = max(fitnesses)\n    best_fitnesses.append(best)\n    if(best == 1 and not found):\n        found = True\n        print(\"Found perfect solution at generation\", gen)\n        \n    if gen % sample_interval == 0:\n        sample = random.sample(population, sample_size)\n        avg_beneficial.append(sum(count_beneficial_mutations(ind) for ind in sample) / sample_size)\n        diversity_over_time.append(diversity(population))\n\n        # Update plot data\n        line0.set_data(range(len(avg_fitness)+1), best_fitnesses)\n        line1.set_data(range(len(avg_fitness)+1), avg_fitness + [sum(fitnesses)/population_size])\n        line2.set_data(range(len(avg_beneficial)), avg_beneficial)\n        line3.set_data(range(len(diversity_over_time)), diversity_over_time)\n        ax1.relim(); ax1.autoscale_view()\n        ax2.relim(); ax2.autoscale_view()\n        best = max(population, key=fitness)\n        fig.suptitle(f\"Best: {translate(best)} (target: {target_protein})\", fontsize=14)\n        plt.pause(0.01)\n\n    else:\n        avg_beneficial.append(avg_beneficial[-1])\n        diversity_over_time.append(diversity_over_time[-1])\n\n    new_pop = []\n    tournament_size = 10  # can be adjusted\n    \n    for _ in range(population_size):\n        # Select tournament_size individuals randomly\n        competitors = random.sample(population, tournament_size)\n        # Pick the one with highest fitness\n        fitness_values = [fitness(ind) for ind in competitors]\n        total = sum(fitness_values)\n        \n        probs = [f / total for f in fitness_values]\n        winner = random.choices(competitors, weights=probs, k=1)[0]\n        # Mutate winner to produce offspring\n        offspring = mutate(winner)\n        new_pop.append(offspring)\n\n    population = new_pop\n\n\n    avg_fitness.append(sum(fitness(ind) for ind in population) / population_size)\n    if gen % 250 == 0:\n        best = max(population, key=fitness)\n        best_individuals.append((gen, translate(best)))\n\ninput(\"\\nSimulation complete. Press Enter to exit plot window...\")\n\n\n\nAnswer the following question about the model:\n\nExercise 14.4 (Evolving protein sequences - Biology / abstract thinking) ¬†\n\nAnother line was added to the model. What new information can you obtain from analysing this line?\nStudy carefully how the other lines (also present in previous models) change over time. What do you observe? Try and capture what you see into words.\nIn biology, multiple genotypes can translate to the same phenotype (this is called a many-to-one genotype-phenotype map), or alternatively, one genotype can produce multiple phenotype (this is called phenotypic plasticity, or a one-to-many genotype-phenotype map). Which genotype-phenotype (GP) mapping applies to this model? Why?\nBonus question for motivated students modify the code to include a second target protein sequence, and alternate between the two targets. If you see something interesting, please share it with the class!\n\n\n\nAnswer a. The new black line is the maximum fitness in the population. Thanks to this line we can see that, sometimes, an individual is present that is fitter but it does not manage to take over the population. We are dealing with a stochastic process, so this is very natural.  b. The fitness line once again goes in distinct steps. The diversity line (green dotted) has a distinct behaviour compared to earlier models. Instead of only going up during the discovery of a new mutant, it constantly creeps up during periods where fitness does not change. This is because the codon-table is partially redundant: many different DNA sequences can code for the same amino acid sequence, so diveristy increases. However, when a new ‚Äúfitter‚Äù individual comes in, diversity goes down as that individual clonally takes over the population. Then, diversity slowly increases again. A similar pattern is reflected in the line that represents the ‚Äúbeneficial mutations‚Äù (blue dotted line). TLDR; even when fitness is not changing there is still a lot going on in this population! c.¬†This model represents a many-to-one mapping between genotype and phenotype. The DNA sequence (genotype) is translated into an amino acid sequence (phenotype), which is then used to calculate fitness. This means that many different genotypes can lead to the same phenotype, and thus the same fitness. Earlier in this course you have learned about development, and those processes often lead to effects where the same genotype can produce many different shapes (phenotypes).  d.¬†BONUS: I have not personally done this, and I do not know the answer to this question yet. However, it is sometimes observed that with many-to-one mapping, populations can because better and better at switching between two alternating targets. This is because the alternating selection pressures make populations move towards genotypes that are ‚Äúclose‚Äù to both targets, and because there is some neutrality coding this can be acchieved without losing fitness in either environment. For cool paper on this principle, see Crombach and Hogeweg (2008). I suspect however that our current model will not be able to do the same.\n\n\n\n\n\nCrombach, Anton, and Paulien Hogeweg. 2008. ‚ÄúEvolution of Evolvability in Gene Regulatory Networks.‚Äù PLoS Computational Biology 4 (7): e1000112.",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_2.html#footnotes",
    "href": "evo_practical_2.html#footnotes",
    "title": "14¬† What gets selected?",
    "section": "",
    "text": "Note that this is from the full code, so this code does not work stand-alone‚Ü©Ô∏é",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>What gets selected?</span>"
    ]
  },
  {
    "objectID": "evo_practical_3.html",
    "href": "evo_practical_3.html",
    "title": "15¬† Public goods",
    "section": "",
    "text": "15.1 Building a model from scratch\nOver the last weeks you have been given many model of biology, and you have modified or extended upon them. For this practical, I will give you a only a description. Your challenge will be to see how far you get in trying to get this model working yourself. I advice you use AI-assisted programming only to solve small steps, otherwise you have no clue what you are doing. But if you try and do everything yourself, it may take a little long.\nAt the end of the pratical, we will compare different implementations by students, as well as my implementation. Hopefully, we will see some generic patterns, because the model description should be good enough to give ‚Äúsimilar models‚Äù. The description should be ‚Äúvague enough‚Äù to lead to some differences, but ‚Äúprecise enough‚Äù to yield similar results. This is an experiment in and of itself. So let‚Äôs see :‚Äô)\nNote that I also do not yet know exactly what will happen in this simulation (although I have tried it out), so I‚Äôm hoping we will learn some cool stuff together!",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Public goods</span>"
    ]
  },
  {
    "objectID": "evo_practical_3.html#simulating-a-simple-microbial-ecosystem-with-public-goods",
    "href": "evo_practical_3.html#simulating-a-simple-microbial-ecosystem-with-public-goods",
    "title": "15¬† Public goods",
    "section": "15.2 Simulating a simple microbial ecosystem with ‚Äúpublic goods‚Äù",
    "text": "15.2 Simulating a simple microbial ecosystem with ‚Äúpublic goods‚Äù\n\nModel description\nMicrobes often produce public goods, from which surrounding microbes can benefit. This can lead to interesting dynamics, such as cooperation and competition. Most models however consider on 1 public good at a time, which leads to limited diversity (a producer, and a non-producer may or may not coexist). Here, we will an ecosystem with many public goods, and simulate them on a grid.\nAn individual microbe will carry a ‚Äúgenome‚Äù that is represented by a binary string (101010010011). Each position in the string represents a public good, and whether the individual can produce it (1) or not (0). The individual can rely on other individuals to produce public goods.\nWe will simulate individuals (microbial cells) reproducing and dying on a grid. A grid point either contains an individual, or it is empty. Every empty point, will be competed for by individuals that are in that neighbourhood. The neighbourhood is defined as the 8 surrounding grid points (this is called the ‚ÄúMoore‚Äù neighbourhood). The cells can only replicate if they have all the public goods they need, which means that they can rely on other individuals in their neighbourhood to produce them. If they do not have all public goods available, they cannot replicate. The ‚Äúwinner‚Äù from these (max) 8 viable competitors will be determined by a roulette wheel selection, where the relative probability is determined by their fitness:\n\\[\nF_i = 1 - c \\cdot \\sum({bitstring})\n\\]\nIn other words, fitness goes down as the number of public goods produced increases, and there is a cost \\(c\\) associated with producing each public good. Make sure this roulette wheel contains a probability that nobody wins, such that highly unfit individuals are less likely to replicate than highly fit individuals (also see earlier practicals).\nThe individual that replicates, can undergo mutations in the bitstring (gene loss and gene gain). Assume gene loss is more likely than gene gain (initial parameters to explore are summarised below)\nFinally, implement a function that allows you to mix the grid (all individuals are placed in a random position).\n\n\nModel output\nThe model will have the following output: a grid that is coloured by the number of public goods produced (for consistency, let‚Äôs all use a ‚Äòviridis‚Äô scale), and a line graph that plots the total population sizes, as well as the population sizes of species producing 0 public goods, 1 public good, 2 public goods, etc. (see Figure¬†15.1)\n\n\nParameters to start out with\n\nGrid size: 50 x 50\nInitial population: produces all public goods (1111‚Ä¶1)\nDeath rate: 0.1\nCost (c): 0.05\nBitstring 1 to 0 mutation (losing a gene): 0.01\nBitstring 0 to 1 mutation (gaining a gene): 0.001\nNumber of public goods (i.e.¬†bitstring length): 10\n‚ÄúNo-event‚Äù size of roulette wheel: 1\n\n\n\n\n\n\n\nFigure¬†15.1: Example of what the simulation could look like\n\n\n\n\n\nProposed experiments\nTry investigating how the model behaves with different values of \\(c\\) (the cost of producing public goods). Can you explain what happens at \\(c=0.0\\)?\nTry studying the effect of mixing the whole grid every timestep, such that neighbourhoods are constantly ‚Äúrandomised‚Äù. Look at the population size, as well as the distribution of different types. Can you explain the observations in biological terms?\nTry studying what happens at different mutation rates.\n\nAnswer This practical is quite open-ended, so instead of answers it is more useful to have a scientific discussion. The image below was first run on a spatially structured grid, but from the dashed line onwards the grid was mixed every timestep. From this we can see that it matters ‚Äúwho you compete with‚Äù. On the spatial grid, individuals compete mostly with other individuals of their own species. That means that, on average, they cannot complement each other by providing public goods (they produce the same set!). Because of this, the ‚Äúomni-producers‚Äù (yellow line) dominates, surrounded by a cloud of mutants that rely on other types. The population size fluctuates strongly, as local populations can collapse due to the loss of public goods, typically followed by the ‚Äúomni-producers‚Äù once again invading the available niche space (empty space on the grid).  When mixing the grid every time step, who you interact with is randomised, and individuals can now rely on other genotypes in the population. Although there is some luck involved in this, you can see that the total population (the black line) increases from this point onward. Thus, although the omni-producers are less dominant, statistically individuals are much better off in this mixed world. That said, there is a risk to this mixed population: if the cost of producing public goods is too high, the population can collapse as there is a strong incentive to lose production of public goods and start relying on others. Depending on your implementation, you may find that mixed systems therefor perform better or worse than spatially structured systems. If they behave identically though, let me (Bram) know, as that is something I almost never observe in models like this ;)\n\n\n\n\n\nDynamics of the producer-types in a system with 10 ‚Äòpublic goods‚Äô",
    "crumbs": [
      "V) Evolution",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Public goods</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ben-Zvi, Danny, and Naama Barkai. 2010. ‚ÄúScaling of Morphogen\nGradients by an Expansion-Repression Integral Feedback Control.‚Äù\nProceedings of the National Academy of Sciences 107 (15):\n6924‚Äì29.\n\n\nBulusu, Vinay, Nicole Prior, Marteinn T Snaebjornsson, Andreas Kuehne,\nKatharina F Sonnen, Jana Kress, Frank Stein, Carsten Schultz, Uwe Sauer,\nand Alexander Aulehla. 2017. ‚ÄúSpatiotemporal Analysis of a\nGlycolytic Activity Gradient Linked to Mouse Embryo Mesoderm\nDevelopment.‚Äù Developmental Cell 40 (4): 331‚Äì41.\n\n\nCarraco, Gil, Ana P Martins-Jesus, and Raquel P Andrade. 2022.\n‚ÄúThe Vertebrate Embryo Clock: Common Players Dancing to a\nDifferent Beat.‚Äù Frontiers in Cell and Developmental\nBiology 10: 944016.\n\n\nCheung, David, Cecelia Miles, Martin Kreitman, and Jun Ma. 2011.\n‚ÄúScaling of the Bicoid Morphogen Gradient by a Volume-Dependent\nProduction Rate.‚Äù Development 138 (13): 2741‚Äì49.\n\n\nCollinet, Claudio, and Thomas Lecuit. 2021. ‚ÄúProgrammed and\nSelf-Organized Flow of Information During Morphogenesis.‚Äù\nNature Reviews Molecular Cell Biology 22 (4): 245‚Äì65.\n\n\nCrombach, Anton, and Paulien Hogeweg. 2008. ‚ÄúEvolution of\nEvolvability in Gene Regulatory Networks.‚Äù PLoS Computational\nBiology 4 (7): e1000112.\n\n\nDing, Zhaojun, and Ji≈ôƒ±ÃÅ Friml. 2010. ‚ÄúAuxin Regulates Distal Stem\nCell Differentiation in Arabidopsis Roots.‚Äù Proceedings of\nthe National Academy of Sciences 107 (26): 12046‚Äì51.\n\n\nDornbusch, Tino, S√©verine Lorrain, Dmitry Kuznetsov, Arnaud Fortier,\nRobin Liechti, Ioannis Xenarios, and Christian Fankhauser. 2012.\n‚ÄúMeasuring the Diurnal Pattern of Leaf Hyponasty and Growth in\nArabidopsis‚Äìa Novel Phenotyping Approach Using Laser Scanning.‚Äù\nFunctional Plant Biology 39 (11): 860‚Äì69.\n\n\nDriever, Wolfgang, and Christiane N√ºsslein-Volhard. 1988. ‚ÄúThe\nBicoid Protein Determines Position in the Drosophila Embryo in a\nConcentration-Dependent Manner.‚Äù Cell 54 (1): 95‚Äì104.\n\n\nFried, Patrick, and Dagmar Iber. 2014. ‚ÄúDynamic Scaling of\nMorphogen Gradients on Growing Domains.‚Äù Nature\nCommunications 5 (1): 5077.\n\n\nGarcƒ±ÃÅa-G√≥mez, M√≥nica L, Diego Ornelas-Ayala, Adriana Garay-Arroyo,\nBerenice Garcƒ±ÃÅa-Ponce, Marƒ±ÃÅa de la Paz S√°nchez, and Elena R\nAlvarez-Buylla. 2020. ‚ÄúA System-Level Mechanistic Explanation for\nAsymmetric Stem Cell Fates: Arabidopsis Thaliana Root Niche as a Study\nSystem.‚Äù Scientific Reports 10 (1): 3525.\n\n\nGilbert, SCOTT F, and MJF Barresi. 2016. ‚ÄúDevelopmental Biology.\n11th Edn, 372.‚Äù Sinauer Associates.\n\n\nGilmour, Darren, Martina Rembold, and Maria Leptin. 2017. ‚ÄúFrom\nMorphogen to Morphogenesis and Back.‚Äù Nature 541 (7637):\n311‚Äì20.\n\n\nGraner, Fran√ßois, and James A Glazier. 1992. ‚ÄúSimulation of\nBiological Cell Sorting Using a Two-Dimensional Extended Potts\nModel.‚Äù Physical Review Letters 69 (13): 2013.\n\n\nHe, Feng, Chuanxian Wei, Honggang Wu, David Cheung, Renjie Jiao, and Jun\nMa. 2015. ‚ÄúFundamental Origins and Limits for Scaling a Maternal\nMorphogen Gradient.‚Äù Nature Communications 6 (1): 6679.\n\n\nHerrgen, Leah, Sa√∫l Ares, Luis G Morelli, Christian Schr√∂ter, Frank\nJ√ºlicher, and Andrew C Oates. 2010. ‚ÄúIntercellular Coupling\nRegulates the Period of the Segmentation Clock.‚Äù Current\nBiology 20 (14): 1244‚Äì53.\n\n\nHester, Susan D. 2012. ‚ÄúMulti-Scale Cell-Based Computational\nModels of Vertebrate Segmentation and Somitogenesis Illuminate\nCoordination of Developmental Mechanisms Across Scales.‚Äù PhD\nthesis, Indiana University.\n\n\nInomata, Hidehiko. 2017. ‚ÄúScaling of Pattern Formations and\nMorphogen Gradients.‚Äù Development, Growth &\nDifferentiation 59 (1): 41‚Äì51.\n\n\nIshimatsu, Kana, Tom W Hiscock, Zach M Collins, Dini Wahyu Kartika Sari,\nKenny Lischer, David L Richmond, Yasumasa Bessho, Takaaki Matsui, and\nSean G Megason. 2018. ‚ÄúSize-Reduced Embryos Reveal a Gradient\nScaling-Based Mechanism for Zebrafish Somite Formation.‚Äù\nDevelopment 145 (11): dev161257.\n\n\nLewis, Julian. 2003. ‚ÄúAutoinhibition with Transcriptional Delay: A\nSimple Mechanism for the Zebrafish Somitogenesis Oscillator.‚Äù\nCurrent Biology 13 (16): 1398‚Äì408.\n\n\nM√§h√∂nen, Ari Pekka, Kirsten ten Tusscher, Riccardo Siligato, Ond≈ôej\nSmetana, Sara Dƒ±ÃÅaz-Trivi√±o, Jarkko Saloj√§rvi, Guy Wachsman, Kalika\nPrasad, Renze Heidstra, and Ben Scheres. 2014. ‚ÄúPLETHORA Gradient\nFormation Mechanism Separates Auxin Responses.‚Äù Nature\n515 (7525): 125‚Äì29.\n\n\nMichaud, Olivier, Anne-Sophie Fiorucci, Ioannis Xenarios, and Christian\nFankhauser. 2017. ‚ÄúLocal Auxin Production Underlies a Spatially\nRestricted Neighbor-Detection Response in Arabidopsis.‚Äù\nProceedings of the National Academy of Sciences 114 (28):\n7444‚Äì49.\n\n\nOostrom, Marek J van, Yuting I Li, Wilke HM Meijer, Tomas EJC Noordzij,\nCharis Fountas, Erika Timmers, Jeroen Korving, Wouter M Thomas, Benjamin\nD Simons, and Katharina F Sonnen. 2025. ‚ÄúScaling of Mouse\nSomitogenesis by Coupling of Cell Cycle to Segmentation Clock\nOscillations.‚Äù bioRxiv, 2025‚Äì01.\n\n\nOskam, Lisa, Basten L Snoek, Chrysoula K Pantazopoulou, Hans van Veen,\nSanne EA Matton, Rens Dijkhuizen, and Ronald Pierik. 2023. ‚ÄúA\nLow-Cost and Open-Source Imaging Platform Reveals Spatiotemporal Insight\ninto Arabidopsis Leaf Elongation and Movement.‚Äù BioRxiv,\n2023‚Äì08.\n\n\nPraat, Myrthe, Zhang Jiang, Joe Earle, Sjef Smeekens, and Martijn van\nZanten. 2024. ‚ÄúUsing a Thermal Gradient Table to Study Plant\nTemperature Signalling and Response Across a Temperature\nSpectrum.‚Äù Plant Methods 20 (1): 114.\n\n\nSaga, Yumiko. 2012. ‚ÄúThe Mechanism of Somite Formation in\nMice.‚Äù Current Opinion in Genetics & Development 22\n(4): 331‚Äì38.\n\n\nSeki, Motohide, Takayuki Ohara, Timothy J Hearn, Alexander Frank,\nViviane CH Da Silva, Camila Caldana, Alex AR Webb, and Akiko Satake.\n2017. ‚ÄúAdjustment of the Arabidopsis Circadian Oscillator by Sugar\nSignalling Dictates the Regulation of Starch Metabolism.‚Äù\nScientific Reports 7 (1): 8305.\n\n\nSonnen, Katharina F, Volker M Lauschke, Julia Uraji, Henning J Falk,\nYvonne Petersen, Maja C Funk, Mathias Beaupeux, Paul Fran√ßois, Christoph\nA Merten, and Alexander Aulehla. 2018. ‚ÄúModulation of Phase Shift\nBetween Wnt and Notch Signaling Oscillations Controls Mesoderm\nSegmentation.‚Äù Cell 172 (5): 1079‚Äì90.\n\n\nSoroldoni, Daniele, David J J√∂rg, Luis G Morelli, David L Richmond,\nJohannes Schindelin, Frank J√ºlicher, and Andrew C Oates. 2014. ‚ÄúA\nDoppler Effect in Embryonic Pattern Formation.‚Äù Science\n345 (6193): 222‚Äì25.\n\n\nStickney, Heather L, Michael JF Barresi, and Stephen H Devoto. 2000.\n‚ÄúSomite Development in Zebrafish.‚Äù Developmental\nDynamics: An Official Publication of the American Association of\nAnatomists 219 (3): 287‚Äì303.\n\n\nTam, PPL. 1981. ‚ÄúThe Control of Somitogenesis in Mouse\nEmbryos.‚Äù Development 65 (Supplement): 103‚Äì28.\n\n\nTomka, Tomas, Dagmar Iber, and Marcelo Boareto. 2018. ‚ÄúTravelling\nWaves in Somitogenesis: Collective Cellular Properties Emerge from\nTime-Delayed Juxtacrine Oscillation Coupling.‚Äù Progress in\nBiophysics and Molecular Biology 137: 76‚Äì87.\n\n\nWolpert, Lewis. 1969. ‚ÄúPositional Information and the Spatial\nPattern of Cellular Differentiation.‚Äù Journal of Theoretical\nBiology 25 (1): 1‚Äì47.",
    "crumbs": [
      "References"
    ]
  }
]